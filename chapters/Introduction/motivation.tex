\section{اهمیت داده‌ها و ضرورت حفظ حریم خصوصی}
\label{sec:intro:importance}

در دهه‌های اخیر، جهان شاهد رشد انفجاری در تولید و جمع‌آوری داده‌ها بوده است.
پیشرفت‌های چشم‌گیر در فناوری‌های ذخیره‌سازی، محاسبات ابری و اینترنت اشیاء، منجر به انباشت حجم عظیمی از داده‌ها شده است که اغلب تحت عنوان کلان‌داده\LTRfootnote{Big Data} شناخته می‌شوند.
این داده‌ها سوخت اصلی موتورهای تصمیم‌گیری مدرن و سیستم‌های هوشمند هستند.
امروزه، الگوریتم‌های یادگیری ماشین\LTRfootnote{Machine Learning} و تحلیل داده\LTRfootnote{Data Analytics} با بهره‌گیری از این مخازن عظیم اطلاعاتی، قادرند الگوهای پیچیده‌ای را شناسایی کنند که در حوزه‌هایی نظیر تشخیص پزشکی، بهینه‌سازی ترافیک شهری، توصیه‌گرهای تجاری و سیاست‌گذاری‌های کلان اقتصادی کاربرد حیاتی دارند.

با این حال، این استفاده‌ی گسترده از داده‌ها، نگرانی‌های جدی و فزاینده‌ای را در خصوص حریم خصوصی\LTRfootnote{Privacy} افراد به وجود آورده است.
داده‌های خامی که برای آموزش مدل‌های هوشمند یا استخراج آماره‌ها استفاده می‌شوند، اغلب حاوی اطلاعات حساس\LTRfootnote{Sensitive Information} و شخصی هستند.
تاریخچه‌ی تراکنش‌های مالی، سوابق پزشکی، موقعیت‌های مکانی و حتی الگوهای جستجو در وب، همگی می‌توانند جزئیات دقیقی از زندگی خصوصی افراد را فاش کنند.
بنابراین، یک چالش اساسی شکل می‌گیرد: چگونه می‌توان از سودمندی\LTRfootnote{Utility} آماری داده‌ها بهره برد، بدون آنکه حریم خصوصی مشارکت‌کنندگان در داده‌ها نقض شود؟

در سال‌های ابتدایی عصر اطلاعات، تصور عمومی بر این بود که حذف شناسه‌های صریح\LTRfootnote{Explicit Identifiers} (مانند نام، کد ملی و شماره تلفن) برای محافظت از هویت افراد کافی است.
این فرایند که گمنام‌سازی \LTRfootnote{Anonymization} نامیده می‌شود، با این فرض انجام می‌شد که داده‌های باقی‌مانده قابلیت ردیابی به فرد خاصی را ندارند.
اما پژوهش‌های متعددی نشان داده‌اند که این روش‌های سنتی در برابر حملات بازشناسایی \LTRfootnote{Re-identification Attacks} به شدت آسیب‌پذیر هستند.
در این نوع حملات، مهاجم با استفاده از اطلاعات جانبی\LTRfootnote{Auxiliary Information} یا اتصال پایگاه‌داده‌های مختلف به یکدیگر، موفق به کشف هویت افراد در داده‌های به ظاهر گمنام می‌شود.

چندین رخداد مشهور در دو دهه‌ی گذشته، ناکارآمدی روش‌های سنتی گمنام‌سازی را اثبات کرده‌اند:

\begin{itemize}
  \item \textbf{داده‌های پزشکی ماساچوست:}
  در یکی از اولین و مشهورترین موارد، لاتانیا سوئینی نشان داد که می‌توان با ترکیب داده‌های پزشکی گمنام‌سازی شده (که نام بیماران از آن حذف شده بود) با فهرست عمومی رأی‌دهندگان، هویت افراد را بازشناسایی کرد.
  او با استفاده از ترکیب تاریخ تولد، جنسیت و کد پستی (که به آن‌ها شبه‌شناسه\LTRfootnote{Quasi-identifier} می‌گویند)، موفق شد پرونده پزشکی فرماندار وقت ایالت ماساچوست را شناسایی کند \cite{sweeney2002k}.

  \item \textbf{مجموعه داده‌ی نتفلیکس\LTRfootnote{Netflix Prize Data}:}
  شرکت نتفلیکس مجموعه‌ای از امتیازهای کاربران به فیلم‌ها را منتشر کرد که در آن شناسه‌های کاربری با اعداد تصادفی جایگزین شده بودند.
  پژوهشگران نشان دادند که با استفاده از اطلاعات عمومی موجود در وب‌سایت \lr{IMDb} و تطبیق الگوهای امتیازدهی، می‌توان هویت بسیاری از کاربران را با دقت بالا کشف کرد \cite{narayanan2008robust}.

  \item \textbf{داده‌های جستجوی \lr{AOL}:}
  در سال ۲۰۰۶، شرکت \lr{AOL} تاریخچه‌ی جستجوی هزاران کاربر خود را منتشر کرد.
  اگرچه نام کاربران حذف شده بود، اما تحلیل محتوای جستجوها منجر به شناسایی هویت افراد شد (از جمله پرونده مشهور تلما آرنولد) که نشان داد حتی خودِ داده‌ها نیز می‌توانند به عنوان شناسه عمل کنند \cite{barbaro2006face}.
\end{itemize}

این شواهد تجربی و نظری نشان می‌دهند که تعاریف هیوریستیک و روش‌های موردی (مانند حذف ستون‌ها یا مخدوش‌سازی ساده) نمی‌توانند تضمین امنیتی پایداری ارائه دهند.
مهاجمان همواره می‌توانند دانش پس‌زمینه‌ی پیش‌بینی‌نشده‌ای داشته باشند که مکانیزم‌های سنتی را دور بزند.
در نتیجه، نیاز مبرمی به یک چارچوب ریاضی دقیق احساس شد که بتواند حریم خصوصی را به صورت کمی تعریف کرده و تضمین دهد که ریسک افشای اطلاعات، مستقل از توان محاسباتی یا دانش جانبی مهاجم، همواره محدود باقی می‌ماند.
این نیاز، زمینه را برای ظهور مفهوم محرمانگی تفاضلی فراهم کرد که در بخش‌های آتی به تفصیل به آن خواهیم پرداخت.




\subsection{محرمانگی تفاضلی (\lr{DP})}
\label{sec:intro:cdp}

در پاسخ به چالش‌های امنیتی و ناکارآمدی روش‌های سنتی گمنام‌سازی، دوُرک و همکاران در سال ۲۰۰۶ مفهوم محرمانگی تفاضلی\LTRfootnote{Differential Privacy} را معرفی کردند \cite{dwork2006differential}.
این چارچوب ریاضی دقیق، به جای تمرکز بر ویژگی‌های ظاهری داده‌ها (مانند حذف نام‌ها)، بر فرایند تولید خروجی تمرکز دارد و تضمین می‌کند که حضور یا عدم حضور یک فرد خاص در پایگاه‌داده، تأثیر ناچیزی بر خروجی نهایی الگوریتم داشته باشد.

پیش از آنکه به تعاریف صوری و ریاضی بپردازیم، ضروری است که درک عمیقی از چیستی محرمانگی و تمایز بنیادین آن با مفاهیم امنیتی کلاسیک پیدا کنیم. بسیاری از سوتفاهم‌ها در این حوزه ناشی از تمایز ندادن دو مفهوم امنیت داده (که قلمرو رمزنگاری\LTRfootnote{Cryptography} است) و محرمانگی داده (که هدف ماست) می‌باشد. رمزنگاری اساساً سازوکاری برای کنترل دسترسی\LTRfootnote{Access Control} است و تضمین می‌کند که تنها افراد مجاز می‌توانند داده‌ها را ببینند؛ اما در برابر نشت اطلاعات از خروجی‌های مجاز سکوت می‌کند. تصور کنید یک پایگاه‌داده‌ی حساس پزشکی کاملاً رمزنگاری شده باشد و پژوهشگری مجاز، نتیجه‌ی یک تحلیل آماری ساده (مانند میانگین حقوق یا نرخ یک بیماری) را منتشر کند. رمزنگاری هیچ محافظتی در برابر استنتاج‌های ثانویه ارائه نمی‌دهد و مهاجم می‌تواند با ترکیب این خروجی مجاز با دانش پیشین\LTRfootnote{Auxiliary Knowledge} خود، اطلاعات خصوصی افراد را بازسازی کند. بنابراین، رمزنگاری شرط لازم است، اما برای حفظ محرمانگی کافی نیست؛ چرا که خودِ نتیجه‌ی تحلیل، حامل اطلاعات است.

در پاسخ به این چالش، دوُرک مفهوم محرمانگی را با ایده‌ی امنیت معنایی\LTRfootnote{Semantic Security} پیوند می‌زند. در این دیدگاه، هدف محرمانگی جلوگیری از یادگیری حقایق کلی درباره‌ی جامعه نیست، بلکه هدف محافظت از حقایق خاص مربوط به یک فرد مشخص است. فلسفه‌ی مرکزی این است که نتیجه‌ی هر تحلیلی باید تقریباً یکسان باشد، چه یک فرد خاص در آن مطالعه مشارکت کند و چه نکند. این تعریف، محرمانگی را به مفهوم ریسک گره می‌زند؛ به این معنا که مشارکت در یک پایگاه‌داده نباید باعث شود ریسک افشای رازهای یک فرد به طور چشم‌گیری افزایش یابد.

برای مدل‌سازی این مفهوم، می‌توانیم از استعاره‌ی جهان‌های موازی استفاده کنیم. دو پایگاه‌داده‌ی همسایه\LTRfootnote{Neighboring Database} (مانند \Dset و $\Dset'$) را به عنوان دو جهان موازی در نظر بگیرید که در یکی، داده‌های کاربر $x$ وجود دارد و در دیگری، این داده‌ها حذف یا تغییر یافته‌اند. هدف نهایی این است که از دیدگاه یک ناظر بیرونی (مهاجم)، این دو جهان غیرقابل تفکیک\LTRfootnote{Indistinguishable} باشند. اگر مکانیزم محرمانگی بتواند کاری کند که مهاجم با مشاهده‌ی خروجی، نتواند تشخیص دهد که این خروجی از کدام جهان آمده است، آنگاه حریم خصوصی کاربر $x$ حفظ شده است. 

برای رسیدن به این هدف، ما از الگوریتم‌های تصادفی\LTRfootnote{Randomized Algorithms} بهره می‌بریم. این الگوریتم‌ها با تزریق نویز کنترل‌شده، توزیع خروجی‌ها را بین دو مجموعه‌داده‌ی همسایه چنان به هم نزدیک می‌کنند که تمایز قائل شدن میان آن‌ها از نظر آماری ناممکن می‌شود.


\subsubsection{چرا روش‌های قطعی شکست می‌خورند؟}
دستیابی به هدف فوق با روش‌های قطعی\LTRfootnote{Deterministic} ممکن نیست. برای درک بهتر، یک حمله‌ی تفاضلی\LTRfootnote{Differencing Attack} کلاسیک را در نظر بگیرید. فرض کنید $f$ تابعی قطعی است که میانگین درآمد $n$ فرد در پایگاه‌داده را برمی‌گرداند ($f(\Dset) = \frac{1}{n}\sum_{i=1}^n x_i$). مهاجم می‌تواند دو پرس‌وجو انجام دهد:
\begin{enumerate}
 \item میانگین درآمد $n$ نفر حاضر در پایگاه‌داده $\left(M_1 = f(\Dset)\right)$.
 \item میانگین درآمد همان افراد، به جز فرد هدف $k$ $\left(M_2 = f(\Dset \setminus \{x_k\})\right)$.
\end{enumerate}
از آن‌جا که خروجی بدون نویز است، مهاجم با یک محاسبه‌ی ساده‌ی جبری 
$\left(x_k = n \cdot M_1 - (n-1) \cdot M_2\right)$،
مقدار دقیق درآمد فرد $k$ را به دست می‌آورد. این مثال نشان می‌دهد که هر تغییر کوچکی در ورودی یک تابع قطعی، به تغییری مشخص و قابل‌ردیابی در خروجی منجر می‌شود که بلافاصله دو جهان موازی را از هم متمایز می‌کند.

در واقع مثال میانگین را می‌توان به هر تابع قطعی
$f: \Xset^n \to \Zset$ تعمیم داد. فرض کنید مهاجم به دنبال بازیابی داده‌ی فرد $k$-ام ($x_k$) است. اگر سایر داده‌های موجود در پایگاه‌داده، یعنی $\Dset_{-k} = \{x_1, \dots, x_{k-1}, x_{k+1}, \dots, x_n\}$ برای مهاجم شناخته شده باشند (فرضی که در تحلیل‌های بدبینانه‌ی محرمانگی تفاضلی استاندارد است)، تابع خروجی را می‌توان تنها بر حسب متغیر مجهول $x_k$ به صورت $g(x) = f(x, \Dset_{-k})$ بازنویسی کرد.

اگر تابع $g$ روی دامنه‌ی $\Xset$ یک‌به‌یک\LTRfootnote{Injective} (یا حتی در بازه‌ای مشخص وارون‌پذیر) باشد، محرمانگی به طور کامل از بین می‌رود؛ زیرا مهاجم با مشاهده‌ی خروجی $z$، می‌تواند ورودی را به صورت $x_k = g^{-1}(z)$ بازیابی کند. حتی اگر $g$ کاملاً وارون‌پذیر نباشد، مشاهده‌ی $z$ فضای جستجوی مقادیر ممکن برای $x_k$ را به شدت کاهش می‌دهد:
\[
    x_k \in \{x \in \Xset \mid g(x) = z\}
\]

از دیدگاه نظریه اطلاعات، مشکل مکانیزم‌های قطعی این است که توزیع احتمال خروجی آن‌ها به ازای یک ورودی مشخص، یک جرم احتمالی\LTRfootnote{Probability Mass} تک‌نقطه‌ای (تابع دلتای دیراک) است. اگر دو پایگاه‌داده‌ی همسایه‌ی $\Dset$ و $\Dset'$ چنان باشند که $f(\Dset) \neq f(\Dset')$، آن‌گاه تکیه‌گاه\LTRfootnote{Support} توزیع‌های خروجی کاملاً مجزا خواهد بود. در نتیجه، واگرایی کولبک-لایبلر\LTRfootnote{Kullback-Leibler Divergence} بین آن‌ها بی‌نهایت می‌شود:
\[
    D_{KL}(\mech(\Dset) || \mech(\Dset')) = \infty
\]
این رابطه اثبات می‌کند که هیچ سطح محدودی از محرمانگی ($\eps < \infty$) با توابع قطعیِ غیرثابت قابل دستیابی نیست. بنابراین، همان‌طور که در ادبیات موضوع تأکید شده است \cite{dwork2014roth}، برای شکستن این وابستگی قطعی و ایجاد ابهام آماری، تصادفی‌سازی\LTRfootnote{Randomization} در فرآیند مکانیزم الزامی است.





\subsubsection{کاربردهای محرمانگی تفاضلی}
این چارچوب ریاضی امروزه به استاندارد طلایی در تحلیل داده‌های حساس تبدیل شده و کاربردهای آن فراتر از آمارهای ساده رفته است. برخی از مهم‌ترین کاربردهای آن عبارتند از:

\begin{itemize}
 \item \textbf{تخمین میانگین و مجموع\LTRfootnote{Mean and Sum Estimation}:} 
  اساسی‌ترین کاربرد \lr{DP} در محاسبه‌ی آماره‌های توصیفی است. سازمان‌های آماری (مانند اداره سرشماری آمریکا) از این روش برای انتشار میانگین درآمد، سن یا جمعیت مناطق استفاده می‌کنند، بدون آنکه داده‌های فردی شهروندان به خطر بیفتد.
  
  \item \textbf{انتشار هیستوگرام\LTRfootnote{Histogram Release}:} 
  بسیاری از تحلیل‌ها نیازمند دانستن توزیع داده‌ها هستند. \lr{DP} اجازه می‌دهد تا تعداد افراد در هر بازه (مثلاً گروه‌های سنی یا درآمدی) با دقت بالا منتشر شود، در حالی که نویز اضافه شده مانع از شناسایی افراد در گروه‌های کم‌جمعیت می‌شود.
  
  \item \textbf{یادگیری ماشین خصوصی\LTRfootnote{Private Machine Learning}:} 
  در آموزش مدل‌های عمیق، خطر به‌خاطرسپاری\LTRfootnote{Memorization} داده‌های آموزشی وجود دارد. با استفاده از الگوریتم‌هایی نظیر \lr{DP-SGD}، می‌توان مدل‌هایی آموزش داد که الگوهای کلی را یاد می‌گیرند اما قادر به بازتولید داده‌های آموزشی حساس (مانند تصاویر چهره یا متون خصوصی) نیستند.
  
  \item \textbf{سیستم‌های توصیه‌گر و داده‌های مکانی:} 
  شرکت‌های فناوری از \lr{DP} برای جمع‌آوری آمارهای رفتاری (مانند پربازدیدترین وب‌سایت‌ها یا مکان‌های پرتردد) استفاده می‌کنند تا بدون ردیابی لحظه‌ای کاربران، کیفیت خدمات خود را بهبود بخشند (مانند مکانیزم \lr{RAPPOR} در گوگل کروم).
\end{itemize}

این توضیحات، زیربنای اصلی تعاریف ریاضی دقیقی است که در فصل بعد به آن‌ها خواهیم پرداخت.




\subsection{محرمانگی تفاضلی موضعی (\lr{LDP})}
\label{sec:intro:ldp}


اگرچه محرمانگی تفاضلی متمرکز (\lr{CDP}) استاندارد طلایی حفاظت از داده‌ها محسوب می‌شود، اما پاشنه‌ی آشیل آن در فرضیه‌ی وجود یک متصدی مورد اعتماد\LTRfootnote{Trusted Curator} نهفته است که به تمام داده‌های خام دسترسی دارد. این مدل در دنیای واقعی با چالش‌های امنیتی و حقوقی جدی روبروست؛ چرا که تجربه نشان داده است اعتماد کامل به سرورهای مرکزی، حتی در صورت مدیریت توسط نهادهای بزرگ فناوری، همواره در معرض تهدید است. یکی از این خطرات، نفوذهای خارجی و سرقت انبوه داده‌هاست؛ به طوری که حتی پیشرفته‌ترین دیوارهای آتش\LTRfootnote{Firewalls} نیز در برابر حملات پیچیده آسیب‌پذیرند. در چنین شرایطی، اگر داده‌ها به صورت خام ذخیره شده باشند، نشت اطلاعاتی مانند آنچه در واقعه‌ی \lr{Equifax} رخ داد، مکانیزم‌های محرمانگی تفاضلی مرکزی را عملاً بی‌فایده می‌کند؛ زیرا مهاجم با دور زدن مکانیزم، مستقیماً به مخزن داده‌های حساس دست می‌یابد\cite{house2018equifax}. 

علاوه بر تهدیدهای خارجی، خطر سوءاستفاده‌های داخلی توسط کارمندان یا مدیران سیستم با دسترسی‌های سطح بالا نیز وجود دارد که امنیت داده‌ها را نه به ریاضیات، بلکه به اخلاق انسانی گره می‌زند. از سوی دیگر، محدودیت‌های حقوقی و احضاریه‌های قضایی نیز متصدی را ملزم به افشای اطلاعات می‌کند. در تمام این سناریوها، مدل متمرکز با یک نقطه شکست مرکزی\LTRfootnote{Single Point of Failure} روبروست. 

\subsubsection{گذار به مدل موضعی، حذف نیاز به اعتماد}

بنابراین، تنها راه تضمین قطعی حریم خصوصی، اتخاذ رویکردی است که در آن متصدی اساساً به داده‌های اصلی دسترسی نداشته باشد. این ضرورت، نقطه‌ی عزیمت ما از مدل متمرکز به سمت چارچوب محرمانگی تفاضلی موضعی (\lr{LDP}) است که در آن فرآیند خصوصی‌سازی پیش از خروج داده از دستگاه کاربر انجام می‌شود.

در این معماری، مرز اعتماد از سرور مرکزی به دستگاه شخصی کاربر (موبایل یا لپ‌تاپ) منتقل می‌شود. پروتکل به گونه‌ای طراحی می‌شود که هیچ‌کس، نه نفوذگران، نه کارمندان کنجکاو و نه حتی دولت‌ها، هرگز داده‌ی واقعی کاربر را مشاهده نکنند. سرور تنها نسخه‌هایی مخدوش و نویزدار از داده‌ها را دریافت می‌کند که به تنهایی بی‌معنی هستند، اما در تجمیع با تعداد زیادی داده‌ی دیگر، الگوهای آماری دقیق را آشکار می‌سازند. این رویکرد، خطر نقض حریم خصوصی را بسیار کنترل می‌کند.

