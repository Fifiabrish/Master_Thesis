
\section{بیان مسئله و اهداف پژوهش}
\label{sec:intro:problem-statement}

همان‌طور که در مرور ادبیات بیان شد، چارچوب مینیماکس ارائه شده توسط دوچی و همکاران \cite{Duchi2013}، نشان داد که محرمانگی تفاضلی موضعی (\LDP) منجر به کاهش چشمگیر نرخ همگرایی در تخمین‌های آماری می‌شود. با این حال، تحلیل‌های موجود در ادبیات اغلب به صورت موردی و با استفاده از ابزارهای پراکنده صورت گرفته است. برای مثال، کران‌های پایین معمولاً با استفاده از واگرایی کولبک-لایبلر (\lr{KL}) برای برخی مسائل و فاصله‌ی تغییرات کل (\lr{TV}) برای برخی دیگر اثبات می‌شوند.

این رویکرد چندگانه، دو چالش اصلی ایجاد می‌کند:
\begin{enumerate}
    \item \textbf{فقدان دیدگاه هندسی یکپارچه:} مشخص نیست که دقیقاً کدام ویژگی هندسیِ مکانیزم‌های \LDP \ مسئول کاهش اطلاعات است.
    \item \textbf{دشواری در تعمیم:} اثبات کران‌ها برای توابع زیان جدید یا توزیع‌های پیچیده، نیازمند ابداع تکنیک‌های اثباتی جدید است.
\end{enumerate}

مسئله‌ی اصلی این پژوهش، پر کردن این خلأ نظری از طریق توسعه‌ی یک چارچوب یکپارچه مبتنی بر نظریه اطلاعات است. ما به دنبال پاسخی جامع برای این پرسش هستیم: «چگونه می‌توان اتلاف اطلاعات در مکانیزم‌های \LDP \ را با استفاده از یک معیار عمومی سنجید و آن را مستقیماً به خطای تخمین آماری مرتبط کرد؟»

\subsection{رویکرد پژوهش: $f$-واگرایی‌ها به عنوان ابزار تحلیل}
\label{sec:intro:f-div-approach}

برای حل مسئله‌ی فوق، این پایان‌نامه پیشنهاد می‌کند که به جای تمرکز بر معیارهای خاص، از خانواده‌ی عمومی \textbf{$f$-واگرایی‌ها}\LTRfootnote{$f$-divergences} استفاده شود.
$f$-واگرایی‌ها (که توسط سیسار\LTRfootnote{Csiszár} معرفی شدند)، کلاسی از معیارهای فاصله بین دو توزیع احتمال $P$ و $Q$ هستند که به صورت زیر تعریف می‌شوند:
\begin{equation}
D_f(P \| Q) = \Eset_Q \left[ f\left( \frac{dP}{dQ} \right) \right]
\end{equation}
که در آن $f$ یک تابع محدب با ویژگی $f(1)=0$ است. اهمیت این کلاس در آن است که معیارهای مشهوری نظیر واگرایی \lr{KL}، فاصله‌ی هلینجر ($H^2$)، فاصله‌ی کای-دو ($\chi^2$) و فاصله‌ی تغییرات کل ($TV$) همگی حالت‌های خاصی از $f$-واگرایی هستند.

استفاده از این ابزار قدرتمند به ما اجازه می‌دهد تا مفهوم «محرمانگی» را به صورت یک محدودیت هندسی تفسیر کنیم. در این دیدگاه، مکانیزم \LDP \ به مثابه‌ی یک کانال انقباضی\LTRfootnote{Contraction Channel} عمل می‌کند که فاصله‌ی بین توزیع‌های ورودی را کاهش می‌دهد. هدف ما محاسبه‌ی دقیق این «ضریب انقباض» برای $f$-واگرایی‌های مختلف و استفاده از آن برای استخراج کران‌های مینیماکس است.

\subsection{نوآوری‌ها و مشارکت‌های پایان‌نامه}
\label{sec:intro:contributions}

این پژوهش با بهره‌گیری از ابزارهای فوق، مشارکت‌های زیر را در ادبیات موضوع ارائه می‌دهد:

\begin{itemize}
    \item \textbf{ارائه‌ی چارچوب یکپارچه برای تحلیل انقباض:}
    ما نشان می‌دهیم که چگونه محدودیت \LDP[\al] باعث محدود شدن طیف وسیعی از $f$-واگرایی‌ها می‌شود. به طور خاص، «نابرابری‌های قوی پردازش داده»\LTRfootnote{Strong Data Processing Inequalities (SDPI)} را برای مدل موضعی توسعه داده و کران‌های دقیقی برای ضریب انقباض در واگرایی‌های $\chi^2$ و \lr{KL} استخراج می‌کنیم.

    \item \textbf{تعمیم متدهای کران پایین (لوکام و فانو):}
    با جایگزینی معیارهای سنتی با $f$-واگرایی‌های عمومی، نسخه‌های تعمیم‌یافته‌ای از «متد دو نقطه‌ای لوکام»\LTRfootnote{Le Cam's Method} و «نامساوی فانو»\LTRfootnote{Fano's Inequality} را ارائه می‌دهیم. این ابزارها به ما امکان می‌دهند تا کران‌های پایین مینیماکس را برای دسته‌ی گسترده‌تری از مسائل تخمین با سهولت بیشتری اثبات کنیم.

    \item \textbf{تحلیل بهینگی مکانیزم‌ها:}
    با استفاده از هندسه‌ی $f$-واگرایی‌ها، نشان می‌دهیم که چرا مکانیزم‌های استاندارد (مانند لاپلاس) در ابعاد بالا ناکارآمد هستند و چرا خانواده‌ی مکانیزم‌های «پاسخ تصادفی تعمیم‌یافته» رفتار بهینه‌تری از خود نشان می‌دهند.
\end{itemize}