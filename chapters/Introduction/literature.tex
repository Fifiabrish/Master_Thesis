\section{کارهای پیشین و مرور ادبیات}
\label{sec:intro:lit-review}

\subsection{مکانیزم‌های بنیادین و مثال‌های عملی}

برای درک عمیق‌تر چگونگی عملکرد \LDP \ در عمل، ابتدا به بررسی مکانیزم بنیادین این حوزه می‌پردازیم و سپس نحوه‌ی تکامل آن را در پروتکل‌های صنعتی مدرن بررسی می‌کنیم.
\begin{itemize}
    \item 
\textbf{پاسخ تصادفی: سنگ بنای \LDP}

    پایه‌ای‌ترین مکانیزم در ادبیات \LDP، «پاسخ تصادفی»\LTRfootnote{Randomized Response (RR)} است که نخستین بار در سال ۱۹۶۵ توسط وارنر برای انجام نظرسنجی‌های حساس (نظیر مصرف مواد مخدر یا عقاید سیاسی خاص) معرفی شد \cite{warner1965randomized}. هدف این مکانیزم، ایجاد یک «انکارپذیری باورپذیر»\LTRfootnote{Plausible Deniability} برای پاسخ‌دهنده است.
    
    سازوکار کلاسیک این روش برای یک پرسش با پاسخ «بله/خیر» به صورت زیر است:
    فرض کنید از کاربر $i$ خواسته می‌شود که ویژگی حساس $X_i \in \{0, 1\}$ را گزارش کند. کاربر به جای پاسخ مستقیم، طبق دستورالعمل زیر عمل می‌کند:
    \begin{enumerate}
        \item یک سکه‌ را پرتاب می‌کند.  (می‌تواند سکه غیرمنصفانه\LTRfootnote{Unfair} باشد)
        \item اگر سکه «شیر» آمد، پاسخ واقعی ($X_i$) را گزارش می‌کند.
        \item اگر سکه «خط» آمد، یک پاسخ تصادفی (با پرتاب سکه‌ی دوم) تولید و گزارش می‌کند.
    \end{enumerate}
    در این سناریو، حتی اگر سرور پاسخ «بله» را دریافت کند، با قطعیت نمی‌داند که آیا کاربر واقعاً دارای ویژگی $X$ بوده است (شیر آمده) یا صرفاً به دلیل تصادف (خط آمدن سکه‌ی اول و شیر آمدن سکه‌ی دوم) این پاسخ را ارسال کرده است. با این حال، از آن‌جایی که احتمالات سکه‌ها مشخص است، سرور می‌تواند با جمع‌آوری تعداد زیادی از پاسخ‌ها ($n$ بسیار بزرگ)، اثر نویز را به صورت آماری حذف کرده و توزیع واقعی جامعه را با خطا تخمین بزند.
    
\item
\textbf{گذار به مقیاس صنعتی: چالش‌ها و راه‌حل‌ها}

اما پاسخ تصادفی تنها راهکاری برای ایجاد محرمانگی در داده‌های دودویی است، و در کاربردهای مدرن با چالش دامنه‌ی بسیار بزرگ\LTRfootnote{High-Dimensional Domain} روبروست. شرکت‌های بزرگ فناوری نیاز دارند داده‌هایی نظیر «آدرس‌های اینترنتی بازدید شده» یا «کلمات جدید تایپ‌شده» را جمع‌آوری کنند که دامنه‌ی آن‌ها(\Xset) می‌تواند شامل میلیون‌ها حالت باشد. اعمال مستقیم \lr{RR} در این حالات منجر به نویز بسیار زیاد و کاهش شدید سودمندی می‌شود. در ادامه، راهکارهای اتخاذ شده توسط بزرگ‌ترین شرکت‌های فناوری را مرور می‌کنیم:

\begin{itemize}
    \item \textbf{گوگل و پروتکل \lr{RAPPOR}:}
    در سال ۲۰۱۴، گوگل برای جمع‌آوری آمار تنظیمات مرورگر کروم و شناسایی بدافزارها، پروتکل \lr{RAPPOR}\LTRfootnote{Randomized Aggregatable Privacy-Preserving Ordinal Response} را معرفی کرد \cite{Wang2020}. چالش اصلی گوگل، جمع‌آوری رشته‌های متنی\LTRfootnote{String} بود. راه‌حل آن‌ها ترکیب پاسخ تصادفی با فیلترهای بلوم\LTRfootnote{Bloom Filters} بود.
    در این روش، داده‌ی ورودی ابتدا به یک بردار بیتی (با استفاده از توابع درهم‌ساز) نگاشت می‌شود و سپس پاسخ تصادفی روی تک‌تک بیت‌های این فیلتر اعمال می‌گردد. این معماری به گوگل اجازه داد تا بدون دانستن ورودی دقیق هر کاربر، الگوهای پرتکرار و ناهنجاری‌ها را در مقیاس میلیونی شناسایی کند.

    \item \textbf{اپل و جمع‌آوری داده‌های دایره‌لغات:}
    شرکت اپل از \lr{LDP} برای بهبود کیبورد \lr{QuickType}، شناسایی ایموجی‌های پرطرفدار و داده‌های سلامت در سیستم‌عامل‌های \lr{iOS} و \lr{macOS} استفاده می‌کند. مسئله‌ی اپل، مخابره‌ی کارآمد داده‌ها با حفظ حریم خصوصی بود.
    راه‌حل اپل استفاده از تکنیک‌های مبتنی بر طرح‌ریزی\LTRfootnote{Sketching} و تبدیل‌های ریاضی مانند تبدیل هادامارد\LTRfootnote{Hadamard Transform} است. این تبدیل‌ها به مکانیزم اجازه می‌دهند که اطلاعات را در ابعاد پایین‌تر فشرده کند تا هم بار ارتباطی کاهش یابد و هم واریانس تخمین‌گر در دامنه‌های بزرگ کنترل شود \cite{Wang2020}.

    \item \textbf{مایکروسافت و داده‌های تله‌متری:}
    مایکروسافت برای جمع‌آوری داده‌های تله‌متری ویندوز (مانند مدت زمان استفاده از برنامه‌ها) با چالش تخمین هیستوگرام‌های پیوسته روبرو بود. آن‌ها از مکانیزم‌هایی نظیر نمونه‌برداری هیستوگرام و روش‌های تکرار‌کننده برای بازسازی توزیع داده‌ها استفاده کردند. تمرکز اصلی در این‌جا، ایجاد تعادل بین دقت آماری در جمع‌آوری داده‌های سیستمی و عدم امکان بازشناسایی رفتار یک کاربر خاص در طول زمان است.
\end{itemize}



\end{itemize}
این نمونه‌ها نشان می‌دهند که محرمانگی تفاضلی موضعی (\lr{LDP}) تنها یک مفهوم نظری نیست، بلکه یک ابزار حیاتی مهندسی است که با استفاده از تکنیک‌های پیشرفته‌ی آماری برای حل مسائل دنیای واقعی مقیاس‌دهی شده است.





\subsection{چالش بزرگ‌نمایی نویز و هزینه سودمندی}
یکی از مهم‌ترین مباحث در ادبیات \LDP که انگیزه‌ی اصلی پژوهش حاضر نیز می‌باشد، مسئله‌ی هزینه‌ی سودمندی است. بیایید تفاوت ریاضی دو مدل را بررسی کنیم.
در مدل متمرکز، متصدی ابتدا آمار واقعی را محاسبه می‌کند و سپس یک بار نویز به آن اضافه می‌کند. از آن‌جا که نویز تنها یک بار اضافه می‌شود، خطا با افزایش تعداد کاربران ($n$) به سرعت و با نرخ $O(1/n)$ کاهش می‌یابد.
اما در مدل موضعی، هر کاربر باید به صورت مستقل نویز اضافه کند. وقتی میانگین این داده‌های نویزدار محاسبه می‌شود، طبق قانون اعداد بزرگ، واریانس نویزها با هم جمع می‌شود. ریاضیات نشان می‌دهد که خطای نهایی در این حالت با نرخ بسیار کندتر یعنی $O(1/\sqrt{n})$ کاهش می‌یابد \cite{Duchi2013}.

این تفاوت چشم‌گیر به این معناست که اگر در مدل متمرکز با هزار داده به دقت مطلوب می‌رسیدیم، در مدل موضعی ممکن است به یک میلیون داده نیاز داشته باشیم. 


\subsection{انواع پروتکل‌های موضعی}
پروتکل‌های \LDP \ را می‌توان بر اساس نحوه‌ی ارتباط کاربران با سرور به دو دسته‌ی اصلی تقسیم کرد:

\begin{enumerate}
    \item \textbf{پروتکل‌های غیرتعاملی\LTRfootnote{Non-interactive}:}
    در این حالت، هر کاربر $i$ مکانیزم \mech \ را مستقل اجرا می‌کند و پیام خود را یک‌بار برای همیشه به سرور می‌فرستد. هیچ ارتباطی بین کاربران یا بازخوردی از سرور وجود ندارد. مزیت این روش سادگی پیاده‌سازی و مقیاس‌پذیری بالا است.

    \item \textbf{پروتکل‌های تعاملی\LTRfootnote{Interactive}:}
    در این روش، کاربران به نوبت با سرور ارتباط برقرار می‌کنند. کاربر بعدی می‌تواند قبل از ارسال داده‌ی خود، خلاصه‌ای از داده‌های کاربران قبلی را مشاهده کند و نویز خود را هوشمندانه‌تر تنظیم نماید. اگرچه این روش از نظر تئوری می‌تواند دقت را در مسائل پیچیده بهبود بخشد، اما به دلیل تأخیر شبکه و پیچیدگی هماهنگی، در سیستم‌های واقعی کم‌تر استفاده می‌شود \cite{Kairouz2016}.
\end{enumerate}

در فصل‌های آینده، ما عمدتاً بر تحلیل حدود پایین و نرخ‌های مینیماکس در این پروتکل‌ها تمرکز خواهیم کرد و نشان خواهیم داد که چگونه ابزارهای مدرن نظریه اطلاعات می‌توانند درک ما را از محدودیت‌های ذاتی این سیستم‌ها دگرگون کنند.