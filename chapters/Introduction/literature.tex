\section{کارهای پیشین و مرور ادبیات}
\label{sec:intro:lit-review}
\subsection{آغازگرها: از پیمایش‌های آماری تا تعریف مدرن محرمانگی}
\label{sec:lit:foundations}

اگرچه نگرانی پیرامون محرمانگی داده‌ها قدمتی به اندازه خودِ آمار دارد، اما فرمول‌بندی ریاضی دقیق آن دستاورد قرن بیست و یکم است. ادبیات کلاسیک این حوزه با تلاش برای {کنترل افشای آماری}\LTRfootnote{Statistical Disclosure Control} آغاز شد، اما ناکارآمدی روش‌های مبتنی بر گمنام‌سازی در برابر دانش پس‌زمینه مهاجم، نیاز به یک تعریف معنایی قوی‌تر را ایجاب کرد.

نقطه عطف این تحول، معرفی مفهوم \textbf{محرمانگی تفاضلی}\LTRfootnote{Differential Privacy} توسط دِوُرک و همکاران بود \cite{dwork2006differential}. این تعریف، برخلاف روش‌های پیشین که بر ویژگی‌های داده تمرکز داشتند، بر ویژگی‌های مکانیزم پردازش داده تمرکز دارد. در مدل استاندارد (متمرکز)، یک مکانیزم تصادفی \mech \ دارای شرایط \DP \ است اگر برای هر دو پایگاه‌داده همسایه \Dset \ و $\Dset'$ (که تنها در داده یک فرد متفاوت‌اند) و برای هر زیرمجموعه از خروجی‌ها $\mathcal{S} \subseteq \mathrm{Range}(\mech)$، رابطه زیر برقرار باشد:

\begin{equation}
\label{eq:dp-def-lit}
\Pr[\mech(\Dset) \in \mathcal{S}] \le e^{\eps} \cdot \Pr[\mech(\Dset') \in \mathcal{S}] + \del
\end{equation}

که در آن \eps \ پارامتری کلیدی به نام \textbf{بودجه محرمانگی}\LTRfootnote{Privacy Budget} است و \del \ احتمال شکست ناچیز مکانیزم را نشان می‌دهد \cite{dwork2014roth}.
برای درک شهودی این مفهوم، می‌توان \eps \ را به عنوان یک «پیچ تنظیم» برای کنترل توازن میان امنیت و مطلوبیت داده‌ها در نظر گرفت. این پارامتر تعیین می‌کند که خروجی مکانیزم تا چه حد اجازه دارد بین دو جهان موازی (جهانی با حضور داده‌ی شما و جهانی بدون آن) تمایز قائل شود:
\begin{itemize}
    \item \textbf{مقادیر کوچک \eps \ (محرمانگی قوی):} زمانی که $\eps \to 0$، توزیع‌های خروجی برای دو پایگاه‌داده همسایه تقریباً بر هم منطبق می‌شوند. در این حالت، مکانیزم مجبور است نویز بسیار زیادی به پاسخ اضافه کند تا تفاوت‌ها را بپوشاند. در نتیجه، مهاجم تقریباً هیچ توانی برای تشخیص حضور فرد ندارد، اما در مقابل، دقت آماری خروجی کاهش می‌یابد.
    \item \textbf{مقادیر بزرگ \eps \ (محرمانگی ضعیف):} با افزایش \eps، مکانیزم آزادی عمل بیش‌تری دارد تا خروجی‌های متمایزتری تولید کند (نویز کمتر). این امر دقت تحلیل را افزایش می‌دهد، اما هم‌زمان ریسک بازشناسایی فرد و نشت اطلاعات خصوصی نیز به صورت نمایی بالا می‌رود.
\end{itemize}

همان‌طور که در بخش قبل توضیح داده شد، پیاده‌سازی این تعریف نیازمند یک پیش‌فرض قوی است: وجود یک {متصدی مورد اعتماد} که تمام داده‌های خام را جمع‌آوری کرده و نویز را به صورت مرکزی اعمال کند. اما دیدیم که این مدل دارای نقطه ضعف‌هایی است. حذف این فرض و انتقال اعتماد از سرور به کاربر، منجر به شکل‌گیری مفهوم \textbf{محرمانگی تفاضلی موضعی\LTRfootnote{Local Differential Privacy}(\lr{LDP})} شد. اگرچه اصطلاح \lr{LDP} و صورت‌بندی مدرن آن در سال‌های اخیر توسط پژوهشگرانی نظیر کاسی‌یسواناتان و دیگران تدوین شد \cite{whatcanwelearnprivatly}، اما ریشه‌های عملی آن به دهه‌ها قبل باز می‌گردد.


\begin{تعریف}[محرمانگی تفاضلی موضعی \lr{(\al,\del)-LDP}]
\label{def:ldp-approx-formal}
یک مکانیزم تصادفی $\mech: \Xset \to \Zset$ (تصادفی‌ساز موضعی) شرط «محرمانگی تفاضلی موضعی تقریبی» یا \ \LDP[(\al, \del)] را برآورده می‌کند، اگر برای تمام جفت ورودی‌های ممکن $x, x' \in \Xset$ و هر زیرمجموعه‌ی خروجی \ $\Sset \subseteq \Zset$، رابطه زیر برقرار باشد:
\begin{equation}
\label{eq:ldp-delta-def}
\Pr[\mech(x) \in \Sset] \le e^{\alpha} \cdot \Pr[\mech(x') \in \Sset] + \delta
\end{equation}
در این تعریف:
\begin{itemize}
    \item \al، بودجه محرمانگی است که میزان شباهت توزیع‌های خروجی را کنترل می‌کند.
    \item \del، احتمال ناچیز شکست مکانیزم در برقراری شرط محرمانگی است.
\end{itemize}
اگر $\delta = 0$ باشد، تعریف به حالت استاندارد یا «محرمانگی تفاضلی موضعی خالص» (\LDP) باز می‌گردد.
\end{تعریف}

در واقع، ساده‌ترین و نخستین نمونه از یک مکانیزم \lr{LDP}، روش «پاسخ تصادفی»\LTRfootnote{Randomized Response (RR)} است که توسط وارنر در سال ۱۹۶۵ برای حذف سوگیری در نظرسنجی‌های حساس معرفی شد \cite{warner1965randomized}. وارنر این روش را نه برای حفاظت در برابر حملات سایبری، بلکه برای تشویق پاسخ‌دهندگان به صداقت در سوالات حساس (مانند مصرف مواد مخدر یا عقاید سیاسی خاص) طراحی کرد.
    
    سازوکار کلاسیک این روش برای یک پرسش با پاسخ «بله/خیر» به صورت زیر است:
    فرض کنید از کاربر $i$ خواسته می‌شود که ویژگی حساس $X_i \in \{0, 1\}$ را گزارش کند. کاربر به جای پاسخ مستقیم، طبق دستورالعمل زیر عمل می‌کند:
    \begin{enumerate}
        \item یک سکه‌ را پرتاب می‌کند.  (می‌تواند سکه غیرمنصفانه\LTRfootnote{Unfair} باشد)
        \item اگر سکه «شیر» آمد، پاسخ واقعی ($X_i$) را گزارش می‌کند.
        \item اگر سکه «خط» آمد، یک پاسخ تصادفی (با پرتاب سکه‌ی دوم) تولید و گزارش می‌کند.
    \end{enumerate}
    در این سناریو، حتی اگر سرور پاسخ «بله» را دریافت کند، با قطعیت نمی‌داند که آیا کاربر واقعاً دارای ویژگی $X$ بوده است (شیر آمده) یا صرفاً به دلیل تصادف (خط آمدن سکه‌ی اول و شیر آمدن سکه‌ی دوم) این پاسخ را ارسال کرده است. با این حال، از آن‌جایی که احتمالات سکه‌ها مشخص است، سرور می‌تواند با جمع‌آوری تعداد زیادی از پاسخ‌ها ($n$ بسیار بزرگ)، اثر نویز را به صورت آماری حذف کرده و توزیع واقعی جامعه را با خطا تخمین بزند.
     به زبان ریاضی مدرن، اگر احتمال گزارش پاسخ واقعی $p$ باشد، نسبت احتمال خروجی‌ها برای دو ورودی متفاوت $x$ و $x'$ به صورت زیر محدود می‌شود:

\begin{equation}
\frac{\Pr[\mech(x)=z]}{\Pr[\mech(x')=z]} \le \frac{p}{1-p}
\end{equation}

این رابطه دقیقاً منطبق بر تعریف \LDP \ است و نشان می‌دهد که بودجه محرمانگی \al \ چگونه مستقیماً از پارامترهای مکانیزم ($p$) مشتق می‌شود:
\begin{equation}
e^\al = \frac{p}{1-p} \quad \Rightarrow \quad \al = \ln\left(\frac{p}{1-p}\right)
\end{equation}
این فرمول، تفسیر شهودی \lr{LDP} را کامل می‌کند:
\begin{itemize}
    \item اگر $p \approx 0.5$ (سکه کاملاً تصادفی)، آنگاه $\al \approx 0$ می‌شود. یعنی خروجی هیچ اطلاعاتی از ورودی ندارد (محرمانگی کامل، اما بدون فایده آماری).
    \item اگر $p \to 1$ (پاسخ تقریباً همیشه راست)، آنگاه $\al \to \infty$ می‌شود. یعنی داده‌ها دقیق هستند اما هیچ محرمانگی وجود ندارد.
\end{itemize}
بنابراین، کار وارنر را می‌توان سنگ‌بنای تاریخی این حوزه دانست که نشان داد چگونه می‌توان بدون اعتماد به گیرنده پیام، و با تنظیم دقیق پارامتر $p$ (و در نتیجه \al)، اطلاعات آماری مفیدی را مخابره کرد.

اما پاسخ تصادفی تنها راهکاری برای ایجاد محرمانگی در داده‌های دودویی است، و در کاربردهای مدرن با چالش دامنه‌ی بسیار بزرگ\LTRfootnote{High-Dimensional Domain} روبروست. شرکت‌های بزرگ فناوری نیاز دارند داده‌هایی نظیر «آدرس‌های اینترنتی بازدید شده» یا «کلمات جدید تایپ‌شده» را جمع‌آوری کنند که دامنه‌ی آن‌ها(\Xset) می‌تواند شامل میلیون‌ها حالت باشد. اعمال مستقیم \lr{RR} در این حالات منجر به نویز بسیار زیاد و کاهش شدید سودمندی می‌شود. در ادامه، راهکارهای اتخاذ شده توسط بزرگ‌ترین شرکت‌های فناوری را مرور می‌کنیم:

\begin{itemize}
    \item \textbf{گوگل و پروتکل \lr{RAPPOR}:}
    در سال ۲۰۱۴، گوگل برای جمع‌آوری آمار تنظیمات مرورگر کروم و شناسایی بدافزارها، پروتکل \lr{RAPPOR}\LTRfootnote{Randomized Aggregatable Privacy-Preserving Ordinal Response} را معرفی کرد \cite{Wang2020}. چالش اصلی گوگل، جمع‌آوری رشته‌های متنی\LTRfootnote{String} بود. راه‌حل آن‌ها ترکیب پاسخ تصادفی با فیلترهای بلوم\LTRfootnote{Bloom Filters} بود.
    در این روش، داده‌ی ورودی ابتدا به یک بردار بیتی (با استفاده از توابع درهم‌ساز) نگاشت می‌شود و سپس پاسخ تصادفی روی تک‌تک بیت‌های این فیلتر اعمال می‌گردد. این معماری به گوگل اجازه داد تا بدون دانستن ورودی دقیق هر کاربر، الگوهای پرتکرار و ناهنجاری‌ها را در مقیاس میلیونی شناسایی کند.

    \item \textbf{اپل و جمع‌آوری داده‌های دایره‌لغات:}
    شرکت اپل از \lr{LDP} برای بهبود کیبورد \lr{QuickType}، شناسایی ایموجی‌های پرطرفدار و داده‌های سلامت در سیستم‌عامل‌های \lr{iOS} و \lr{macOS} استفاده می‌کند. مسئله‌ی اپل، مخابره‌ی کارآمد داده‌ها با حفظ حریم خصوصی بود.
    راه‌حل اپل استفاده از تکنیک‌های مبتنی بر طرح‌ریزی\LTRfootnote{Sketching} و تبدیل‌های ریاضی مانند تبدیل هادامارد\LTRfootnote{Hadamard Transform} است. این تبدیل‌ها به مکانیزم اجازه می‌دهند که اطلاعات را در ابعاد پایین‌تر فشرده کند تا هم بار ارتباطی کاهش یابد و هم واریانس تخمین‌گر در دامنه‌های بزرگ کنترل شود \cite{Wang2020}.

    \item \textbf{مایکروسافت و داده‌های تله‌متری:}
    مایکروسافت برای جمع‌آوری داده‌های تله‌متری ویندوز (مانند مدت زمان استفاده از برنامه‌ها) با چالش تخمین هیستوگرام‌های پیوسته روبرو بود. آن‌ها از مکانیزم‌هایی نظیر نمونه‌برداری هیستوگرام و روش‌های تکرار‌کننده برای بازسازی توزیع داده‌ها استفاده کردند. تمرکز اصلی در این‌جا، ایجاد تعادل بین دقت آماری در جمع‌آوری داده‌های سیستمی و عدم امکان بازشناسایی رفتار یک کاربر خاص در طول زمان است.
\end{itemize}


این نمونه‌ها نشان می‌دهند که محرمانگی تفاضلی موضعی (\lr{LDP}) تنها یک مفهوم نظری نیست، بلکه یک ابزار حیاتی مهندسی است که با استفاده از تکنیک‌های پیشرفته‌ی آماری برای حل مسائل دنیای واقعی مقیاس‌دهی شده است.




\subsection{چالش سودمندی و موازنه دقت-محرمانگی}
\label{sec:lit:utility-challenge}

اگرچه حذف متصدی مرکزی در مدل \lr{LDP}، تضمین‌های امنیتی بسیار قوی‌تری را فراهم می‌کند، اما این امنیت رایگان به دست نمی‌آید. چالش بنیادین در این رویکرد، کاهش چشم‌گیر \textbf{سودمندی}\LTRfootnote{Utility} داده‌ها یا همان دقت تحلیل‌های آماری است. این پدیده تحت عنوان \textbf{موازنه محرمانگی-دقت}\LTRfootnote{Privacy-Accuracy Trade-off} شناخته می‌شود.

برای درک شهودی این چالش، مقایسه نحوه اعمال نویز در دو مدل ضروری است:
\begin{itemize}
    \item \textbf{در مدل متمرکز (\lr{CDP}):} نویز تنها «یک‌بار» و پس از تجمیع داده‌ها به نتیجه نهایی اضافه می‌شود. از آن‌جا که مجموع (یا میانگین) داده‌ها حساسیت کمی دارد، مقدار نویز معمولاً مستقل از تعداد کاربران ($n$) و بسیار کوچک است.
    \item \textbf{در مدل موضعی (\lr{LDP}):} نویز باید به «تک‌تک» داده‌های ورودی اضافه شود (پیش از آنکه از دستگاه کاربر خارج شوند). وقتی تحلیل‌گر قصد دارد میانگین این داده‌ها را محاسبه کند، واریانس نویزهای $n$ کاربر با هم جمع می‌شود.
\end{itemize}

این انباشت نویز باعث می‌شود که \textbf{نسبت سیگنال به نویز}\LTRfootnote{Signal-to-Noise Ratio (SNR)} در مدل موضعی بسیار پایین‌تر از مدل متمرکز باشد. به بیان دیگر، برای دستیابی به همان سطح از دقت که در مدل متمرکز وجود دارد، در مدل \lr{LDP} نیازمند تعداد بسیار بیش‌تری نمونه داده هستیم.

این مسئله در کاربردهای عملی بسیار حائز اهمیت است. برای مثال، اگر هدف تخمین فراوانی یک بیماری نادر باشد، نویز اضافه شده توسط مکانیزم‌های \lr{LDP} ممکن است سیگنال اصلی را کاملاً بپوشاند. همین چالش بود که پژوهشگران را بر آن داشت تا به جای استفاده از روش‌های ساده (مثل وارنر)، به دنبال پاسخ این پرسش باشند که: «آیا می‌توان مکانیزم‌هایی طراحی کرد که با کم‌ترین میزان نویز، بیش‌ترین محرمانگی را فراهم کنند؟» و «حد نهایی این دقت کجاست؟»
این پرسش‌ها زمینه را برای ورود تئوری‌های پیشرفته‌تر نظیر «تخمین مینیماکس» فراهم کرد.







\subsection{عصر مدرن \lr{LDP}: چارچوب مینیماکس و حدود بنیادین}
\label{sec:lit:modern-minimax}

پاسخ به پرسش بالا، مسیر پژوهش‌های این حوزه را به سمت \textbf{نظریه مینیماکس آماری}\LTRfootnote{Statistical Minimax Theory} تغییر داد. نقطه عطف این تحول، سلسله مقالات جریان‌ساز دوچی، جردن و وین‌رایت\LTRfootnote{Duchi, Jordan, and Wainwright} بود \cite{Duchi2013, duchi2018}. آن‌ها با صورتی‌بندی مسئله در قالب نظریه اطلاعات، نشان دادند که هزینه محرمانگی در مدل موضعی بسیار سنگین و غیرقابل اجتناب است.

در تحلیل مینیماکس، هدف یافتن «ریسک مینیماکس» ($\mathfrak{M}_n$) است؛ یعنی کم‌ترین خطایی که «بهترین تخمین‌گر ممکن» در «بدترین توزیع داده‌ی ممکن» مرتکب می‌شود. دوچی و همکاران با استفاده از ابزارهایی نظیر \textbf{نامساوی فانو}\LTRfootnote{Fano's Inequality} و \textbf{لم اسود}\LTRfootnote{Assouad's Lemma} (که در فصل سوم به تفصیل بررسی خواهند شد)، ثابت کردند که برای مسائل پایه‌ای نظیر تخمین میانگین یا چگالی احتمال، نرخ همگرایی خطا در مدل \lr{LDP} رفتاری متفاوت با مدل متمرکز دارد.

به طور مشخص، برای $n$ کاربر و بودجه محرمانگی $\al$، کران پایین خطا ($\cal{E}$) به صورت مجانبی از رابطه‌ی زیر پیروی می‌کند:

% \begin{equation}
\[
\label{eq:minimax-comparison}
\mathcal{E}_{LDP} \asymp \frac{1}{\sqrt{n \al^2}} \quad 
\text{در حالی که}
\quad \mathcal{E}_{CDP} \asymp \frac{1}{n \eps}
\]
% \end{equation}

این نتیجه که به «قانون مقیاس‌دهی کانونی»\LTRfootnote{Canonical Scaling Law} معروف است، حاوی دو پیام مهم است:
\begin{enumerate}
    \item \textbf{کندی همگرایی:} در حالی که خطای مدل متمرکز با سرعت $1/n$ کاهش می‌یابد، خطای مدل موضعی با سرعت بسیار کندتر $1/\sqrt{n}$ کم می‌شود.
    \item \textbf{اندازه نمونه مؤثر:} ضریب $\al^2$ نشان می‌دهد که هر نمونه داده‌ی خصوصی‌سازی شده، عملاً حاوی اطلاعاتی معادل با $\al^2$ نمونه داده‌ی خام است (برای $\al < 1$). این یعنی برای جبران نویز \LDP، حجم داده‌ها باید با ضریب $1/\al^2$ افزایش یابد.
\end{enumerate}

پس از استقرار این چارچوب نظری، تمرکز جامعه علمی بر طراحی «مکانیزم‌های بهینه ترتیب‌مقدماتی»\LTRfootnote{Order-optimal Mechanisms} قرار گرفت که بتوانند به این کران‌های نظری دست یابند.
از جمله مهم‌ترین این تلاش‌ها می‌توان به معرفی «مکانیزم‌های پله‌ای»\LTRfootnote{Staircase Mechanisms} توسط کایروز و همکاران \cite{Kairouz2016} و توسعه پروتکل‌های پیشرفته‌ای نظیر \lr{UE} (کدگذاری یگانی)\LTRfootnote{Unary Encoding} و \lr{OLH} (هشینگ محلی بهینه)\LTRfootnote{Optimal Local Hashing} توسط وانگ و همکاران \cite{Wang2020} اشاره کرد.
این روش‌ها تلاش می‌کنند با بهینه‌سازی ساختار نویز و استفاده از تکنیک‌های فشرده‌سازی اطلاعات، فاصله بین عملکرد عملی و حدود نظری مینیماکس را به حداقل برسانند.


\subsection{دسته‌بندی پروتکل‌های موضعی: تعاملی و غیرتعاملی}
\label{sec:lit:protocols-types}

به عنوان آخرین مبحث در مرور ادبیات موضوع، لازم است به دسته‌بندی پروتکل‌های \LDP \ بر اساس «معماری ارتباطی» اشاره کنیم. پژوهش‌های انجام شده در این حوزه، مکانیزم‌ها را به دو دسته‌ی کلی تقسیم می‌کنند:

\begin{enumerate}
    \item \textbf{پروتکل‌های غیرتعاملی}\LTRfootnote{Non-interactive / Simultaneous}:
    در این حالت، تمام کاربران به صورت هم‌زمان و مستقل عمل می‌کنند. هر کاربر $i$ مکانیزم \mech \ را تنها بر اساس داده‌ی خودش $X_i$ اجرا کرده و پیام $Z_i$ را به سرور می‌فرستد. هیچ ارتباطی بین کاربران وجود ندارد و سرور نیز هیچ بازخوردی به کاربران نمی‌دهد. به دلیل سادگی پیاده‌سازی و مقیاس‌پذیری بالا، اکثر پروتکل‌های صنعتی (مانند \lr{RAPPOR} گوگل یا سیستم‌های اپل) در این دسته قرار می‌گیرند.

    \item \textbf{پروتکل‌های تعاملی}\LTRfootnote{Interactive / Sequential}:
    در این روش، کاربران به صورت متوالی با سرور ارتباط برقرار می‌کنند. کاربر $i$ می‌تواند قبل از ارسال داده‌ی خود، خلاصه‌ای از داده‌های کاربران قبلی ($Z_1, \dots, Z_{i-1}$) را از سرور دریافت کند و نویز خود را هوشمندانه‌تر تنظیم نماید. اگرچه به نظر می‌رسد این آزادی عمل باید دقت را افزایش دهد، اما دوچی و همکاران در نتایج حیرت‌انگیزی نشان دادند که برای دسته‌ی بزرگی از توابع محدب (مانند تخمین میانگین)، پروتکل‌های تعاملی هیچ مزیتی نسبت به روش‌های غیرتعاملی ندارند و نرخ مینیماکس را بهبود نمی‌بخشند \cite{Duchi2013, Kairouz2016}.
\end{enumerate}


در این پایان‌نامه، چارچوب نظری ارائه‌شده به گونه‌ای است که نتایج (به‌ویژه کران‌های پایین مینیماکس) برای هر دو کلاس پروتکل‌های تعاملی و غیرتعاملی صادق هستند. ما نشان خواهیم داد که محدودیت‌های ذاتیِ مدل \lr{LDP}، مستقل از معماری ارتباطی شبکه عمل می‌کنند و افزودن تعامل، لزوماً راه گریزی از این محدودیت‌های بنیادین فراهم نمی‌کند. ابزارهای ریاضیاتی قدرتمندی که امکان چنین تحلیل یک‌پارچه‌ای را فراهم می‌سازند، در بخش بعدی معرفی خواهند شد.

