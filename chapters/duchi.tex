\chapter{تحلیل مینی‌مکس و هندسه اطلاعاتی در \lr{LDP}}
\label{ch:minimax-theory}

\section{مقدمه}
\label{sec:minimax:intro}

در فصل پیشین (\ref{ch:ldp})، چارچوب محرمانگی تفاضلی موضعی (\LDP) را به عنوان جایگزینی برای مدل متمرکز معرفی کردیم. دیدیم که مکانیزم‌های استاندارد مانند «پاسخ تصادفی تعمیم‌یافته» (\lr{GRR}) و «کدگذاری‌های یکانی» (\lr{UE})، اگرچه امنیت داده‌ها را تضمین می‌کنند، اما بهای سنگینی را از نظر دقت آماری تحمیل می‌کنند. به طور خاص، دیدیم که در بسیاری از مسائل تخمین میانگین، واریانس خطا از مرتبه $\mathcal{O}(1/n)$ در مدل متمرکز به $\mathcal{O}(1/n\epsilon^2)$ در مدل موضعی افزایش می‌یابد (که در این‌جا $n$ تعداد کاربران است).

این مشاهده، یک پرسش بنیادین را ایجاد می‌کند: آیا این کاهش دقت، ناشی از ضعف در طراحی مکانیزم‌های موجود (مانند \lr{RR}) است، یا یک محدودیت ذاتی و غیرقابل اجتناب در طبیعت محرمانگی موضعی است؟ به بیان دیگر، آیا می‌توان مکانیزم هوشمندانه‌تری طراحی کرد که هم شرط \LDP\ را ارضا کند و هم به نرخ خطای مدل متمرکز نزدیک شود؟

هدف اصلی این فصل، پاسخ به این پرسش با استفاده از ابزارهای قدرتمند «نظریه مینی‌مکس» است. ما نشان خواهیم داد که هزینه پرداختی در مدل موضعی (معمولاً ضریبی از $\sqrt{n}$ در نرخ همگرایی)، یک مانع اطلاعاتی بنیادین است و هیچ الگوریتمی نمی‌تواند از آن عبور کند.

برای اثبات این ادعا، ما از زیرساخت‌های ریاضی که در فصل \ref{ch:preliminaries} بنا نهادیم (شامل $f$-واگرایی‌ها، لم لوکم، نامساوی فانو و لم اسود) بهره خواهیم برد. با این حال، استفاده مستقیم از این ابزارها در محیط \lr{LDP} ممکن نیست. نوآوری اصلی که در این فصل بررسی می‌کنیم، چارچوب «محرمانگی به عنوان انقباض»\LTRfootnote{Privacy as Contraction} است که عمدتاً توسط دوچی، جردن و وین‌رایت \cite{Duchi2013, duchi2018} توسعه یافته است.

ایده مرکزی این است که مکانیزم‌های \lr{LDP} مانند یک فیلتر اطلاعاتی عمل می‌کنند که فاصله آماری بین توزیع‌های ورودی را «منقبض» کرده و تشخیص آن‌ها را دشوار می‌سازند. ما در این فصل:
\begin{enumerate}
    \item ابتدا خاصیت انقباضی مکانیزم‌های \LDP\ را بر حسب واگرایی کولبک-لایبلر (\lr{KL}) و فاصله تغییرات کل (\lr{TV}) فرمول‌بندی می‌کنیم.
    \item سپس این خاصیت را با نامساوی‌های پردازش داده\LTRfootnote{Data Processing Inequalities} ترکیب کرده تا کران‌های پایین مینی‌مکس را برای مسائل کلاسیک آماری استخراج کنیم.
    \item در نهایت، نشان می‌دهیم که مکانیزم‌های معرفی شده در فصل \ref{ch:ldp}، در واقع «بهینه مینی‌مکس» هستند و به بهترین نرخ ممکن دست می‌یابند.
\end{enumerate}

\section{محرمانگی به عنوان انقباض}
\label{sec:privacy-contraction}

همان‌طور که در فصل پیش‌نیازها (\ref{sec:bg:f-divergence}) بحث شد، $f$-واگرایی‌ها ابزاری قدرتمند برای سنجش تمایز بین توزیع‌های احتمالی هستند. ایده مرکزی در تحلیل مینی‌مکس تحت \lr{LDP} این است که مکانیزم‌های محرمانگی به عنوان «عملگرهای انقباضی»\LTRfootnote{Contraction Operators} عمل می‌کنند. به عبارت دیگر، شرط \LDP\ باعث می‌شود که توزیع‌های خروجی مکانیزم برای هر دو ورودی دل‌خواه، به یک‌دیگر بسیار نزدیک شوند و در نتیجه واگرایی بین آن‌ها محدود شود.

در این بخش، ما لم اساسی دوچی و همکاران \cite{duchi2018} را بیان و اثبات می‌کنیم. این لم نشان می‌دهد که واگرایی \lr{KL} بین توزیع‌های خروجی، توسط فاصله \lr{TV} توزیع‌های ورودی و ضریبی از بودجه محرمانگی \al کران‌دار می‌شود.

\begin{قضیه}[کران انقباض قوی برای \lr{LDP}]
\label{thm:ldp-contraction}
فرض کنید $\mech: \Xset \to \Zset$ یک مکانیزم \LDP\ باشد. برای هر جفت توزیع احتمالی $P_1$ و $P_2$ روی فضای ورودی $\Xset$، اگر $M_1$ و $M_2$ توزیع‌های حاشیه‌ای القا شده روی خروجی $\Zset$ باشند (یعنی $M_i(S) = \int \mech(S|x) dP_i(x)$)، آنگاه نامساوی زیر برقرار است:
\begin{equation}
\label{eq:kl-contraction-bound}
D_{KL}(M_1 \| M_2) + D_{KL}(M_2 \| M_1) \le (e^\al - 1)^2 \norm{P_1 - P_2}_{TV}^2
\end{equation}
\end{قضیه}

\begin{اثبات}
برای اثبات این قضیه، از خاصیت تحدب مشترک $f$-واگرایی‌ها و تعریف \LDP\ استفاده می‌کنیم. اثبات در دو گام انجام می‌شود: ابتدا مسئله را به ورودی‌های نقطه‌ای تقلیل می‌دهیم و سپس کران را برای آن محاسبه می‌کنیم.

\textbf{گام اول: تقلیل به ورودی‌های نقطه‌ای.}
می‌دانیم که نگاشت $(P, Q) \mapsto D_{KL}(P \| Q)$ محدب مشترک است (بخش \ref{sec:bg:f-div-def}). هم‌چنین فاصله تغییرات کل $\norm{\cdot}_{TV}$ نیز محدب است. بیشینه واگرایی زمانی رخ می‌دهد که توزیع‌های $P_1$ و $P_2$ کم‌ترین هم‌پوشانی را داشته باشند (متعامد باشند).
با توجه به خطی بودن مکانیزم \mech روی مخلوط‌های احتمالی، می‌توان نشان داد که کران بالا با در نظر گرفتن توزیع‌های متمرکز روی تک‌نقطه‌ها (توزیع‌های دیراک) به دست می‌آید \cite{Duchi2013}. فرض کنید $x, x' \in \Xset$ دو نقطه دل‌خواه باشند و $Q(\cdot|x)$ و $Q(\cdot|x')$ توزیع‌های خروجی متناظر با آن‌ها باشند. اگر بتوانیم نشان دهیم که برای هر $x, x'$:
\begin{equation}
D_{KL}(Q(\cdot|x) \| Q(\cdot|x')) + D_{KL}(Q(\cdot|x') \| Q(\cdot|x)) \le (e^\al - 1)^2
\end{equation}
آنگاه با توجه به این‌که برای ورودی‌های نقطه‌ای $\norm{\delta_x - \delta_{x'}}_{TV} = 1$ است، حکم برای حالت کلی با استفاده از نامساوی ینسن و تحدب نتیجه می‌شود.

\textbf{گام دوم: محاسبه کران برای ورودی‌های ثابت.}
طبق تعریف \LDP، برای هر خروجی $z \in \Zset$ و هر جفت ورودی $x, x'$ داریم:
\begin{equation}
e^{-\al} \le \frac{q(z|x)}{q(z|x')} \le e^\al
\end{equation}
که در آن $q(\cdot|x)$ تابع چگالی (یا جرم) احتمال مکانیزم است. عبارت سمت چپ نامساوی \eqref{eq:kl-contraction-bound} را بسط می‌دهیم (که همان واگرایی متقارن یا واگرایی جفریز است):
\begin{align}
D_{KL}(Q_x \| Q_{x'}) + D_{KL}(Q_{x'} \| Q_x) &= \int_{\Zset} \left( q(z|x) - q(z|x') \right) \log \left( \frac{q(z|x)}{q(z|x')} \right) \nu(dz)
\end{align}
از شرط \LDP، می‌دانیم که نسبت درست‌نمایی‌ها کران‌دار است. به طور خاص، چون نسبت $q(z|x)/q(z|x')$ در بازه $[e^{-\al}, e^\al]$ قرار دارد، لگاریتم آن در بازه $[-\al, \al]$ است.
با این حال، برای به دست آوردن ضریب دقیق $(e^\al - 1)^2$، از یک نامساوی دقیق‌تر استفاده می‌کنیم. طبق لم ۱ در \cite{Duchi2013}، اگر نسبت دو چگالی $p/q$ در بازه $[e^{-\al}, e^\al]$ باشد، آنگاه:
\begin{equation}
D_{KL}(p \| q) \le \frac{1}{2} (e^\al - 1) \norm{p - q}_{1}
\end{equation}
با جمع دو طرف برای واگرایی متقارن و استفاده از این‌که $\norm{p-q}_1 = 2\norm{P-Q}_{TV}$، به رابطه دقیق‌تر می‌رسیم. اما یک روش مستقیم‌تر استفاده از کران زیر است که برای هر دو توزیع $P, Q$ که شرط نسبت محدود را دارند برقرار است:
\begin{equation}
\int (p(z) - q(z)) \log \frac{p(z)}{q(z)} dz \le (e^\al - 1) \int |p(z) - q(z)| dz = (e^\al - 1) \norm{Q_x - Q_{x'}}_1
\end{equation}
هم‌چنین می‌دانیم که تحت شرط \LDP، فاصله $L_1$ خروجی‌ها نیز محدود است:
\begin{equation}
\norm{Q_x - Q_{x'}}_1 \le e^\al - 1
\end{equation}
(این نامساوی از آن‌جا ناشی می‌شود که جرم احتمال نمی‌تواند سریع‌تر از ضریب $e^\al$ جابجا شود).
با ترکیب این دو نتیجه:
\begin{align}
D_{KL}(Q_x \| Q_{x'}) + D_{KL}(Q_{x'} \| Q_x) &\le (e^\al - 1) \norm{Q_x - Q_{x'}}_1 \nonumber \\
&\le (e^\al - 1)^2
\end{align}
این اثبات برای حالت ورودی‌های نقطه‌ای کامل می‌شود. تعمیم به توزیع‌های کلی $P_1, P_2$ از طریق نامساوی پردازش داده (\lr{DPI}) و تحدب مشترک حاصل می‌شود که در آن ضریب $\norm{P_1 - P_2}_{TV}^2$ ظاهر می‌گردد.
\end{اثبات}

\subsection{تفسیر رژیم‌های محرمانگی}
نامساوی \eqref{eq:kl-contraction-bound} پیامدهای مهمی برای نرخ‌های همگرایی آماری دارد. رفتار ضریب $(e^\al - 1)^2$ در دو رژیم حدی قابل توجه است:

\begin{itemize}
    \item \textbf{رژیم محرمانگی بالا ($\al \le 1$):}
    در این حالت، می‌توان از بسط تیلور استفاده کرد: $e^\al - 1 \approx \al$. بنابراین کران انقباض به صورت زیر در می‌آید:
    \begin{equation}
    D_{KL}(M_1 \| M_2) \lesssim \al^2 \norm{P_1 - P_2}_{TV}^2
    \end{equation}
    این نتیجه نشان می‌دهد که میزان اطلاعات فیشر و واگرایی‌ها با توان دوم \al کاهش می‌یابند. این پدیده دلیل اصلی نرخ همگرایی کندتر در مدل موضعی (نرخ $1/\al^2$) نسبت به مدل متمرکز است.
    
    \item \textbf{رژیم محرمانگی پایین ($\al \to \infty$):}
    در این حالت، $e^\al$ به سرعت رشد می‌کند و کران بی‌نهایت می‌شود. این منطقی است، زیرا وقتی \al بسیار بزرگ است، مکانیزم \LDP\ تقریباً محدودیتی اعمال نمی‌کند و اجازه می‌دهد توزیع‌های خروجی کاملاً متمایز باشند (عدم انقباض).
\end{itemize}

این خاصیت انقباضی، ابزار اصلی ما در فصل بعد برای اثبات کران‌های پایین مینی‌مکس خواهد بود، جایی که نشان می‌دهیم تمایز بین پارامترهای مدل حتی با وجود داده‌های زیاد، به دلیل این انقباض دشوار باقی می‌ماند.


\section{نامساوی‌های پردازش داده قوی (\lr{SDPI})}
\label{sec:sdpi}

در بخش قبل دیدیم که مکانیزم‌های \LDP\ باعث کاهش فاصله بین توزیع‌ها می‌شوند. این مفهوم را می‌توان به صورت صوری‌تر در قالب «نامساوی پردازش داده قوی»\LTRfootnote{Strong Data Processing Inequality (SDPI)} بیان کرد. در حالی که نامساوی پردازش داده استاندارد (DPI) تنها بیان می‌کند که واگرایی افزایش نمی‌یابد ($D_f(\mech(P) \| \mech(Q)) \le D_f(P \| Q)$)، نسخه قوی آن تضمین می‌کند که واگرایی اکیداً کاهش می‌یابد.

\begin{تعریف}[ضریب انقباض \lr{Dobrushin}]
برای یک کانال یا مکانیزم \mech و یک واگرایی $f$، ضریب انقباض $\eta_f(\mech)$ به صورت زیر تعریف می‌شود:
\begin{equation}
\label{eq:contraction-coeff}
\eta_f(\mech) = \sup_{P \neq Q} \frac{D_f(\mech(P) \| \mech(Q))}{D_f(P \| Q)}
\end{equation}
اگر $\eta_f(\mech) < 1$ باشد، می‌گوییم مکانیزم دارای خاصیت SDPI است.
\end{تعریف}

برای مکانیزم‌های \LDP، می‌توان نشان داد که این ضریب همواره کمتر از یک است، که نشان‌دهنده اتلاف حتمی اطلاعات در فرآیند خصوصی‌سازی است. با این حال، در تحلیل‌های مینی‌مکس، ما اغلب به کران‌هایی نیاز داریم که واگرایی خروجی را مستقیماً به فاصله تغییرات کل (\lr{TV}) ورودی مرتبط کنند، زیرا فاصله \lr{TV} متر طبیعی روی فضای پارامترها در بسیاری از مسائل است.

\subsection{کران انقباض برای واگرایی کای-دو ($\chi^2$)}
اگرچه واگرایی \lr{KL} (بخش قبل) ابزار استانداردی در نظریه اطلاعات است، اما کار با واگرایی $\chi^2$ در مسائل \LDP\ اغلب ساده‌تر است. دلیل این امر رفتار چندجمله‌ای $\chi^2$ است که برخلاف لگاریتم در \lr{KL}، با عملیات جمع و انتگرال‌گیری سازگاری بیشتری دارد.

قضیه زیر که برگرفته از تحلیل‌های دوچی و همکاران \cite{Duchi2013} است، کران انقباض را برای واگرایی $\chi^2$ بیان می‌کند.

\begin{قضیه}[انقباض $\chi^2$ در \lr{LDP}]
\label{thm:chi-squared-contraction}
فرض کنید \mech یک مکانیزم \LDP\ باشد. برای هر دو توزیع $P_1$ و $P_2$ روی $\Xset$، اگر $M_1$ و $M_2$ توزیع‌های خروجی باشند، داریم:
\begin{equation}
\label{eq:chi-sq-bound}
D_{\chi^2}(M_1 \| M_2) \le (e^\alpha + 1)(e^\alpha - 1)^2 \norm{P_1 - P_2}_{TV}^2
\end{equation}
\end{قضیه}

\begin{اثبات}
اثبات این قضیه نیز بر پایه تحدب مشترک بنا شده است. تابع $D_{\chi^2}(P \| Q) = \int \frac{(dP - dQ)^2}{dQ}$ نسبت به جفت $(P, Q)$ محدب است. مشابه اثبات لم \ref{thm:ldp-contraction}، ماکسیمم انقباض روی نقاط حدی دامنه (توزیع‌های دیراک) رخ می‌دهد.
برای دو ورودی $x, x'$، و خروجی $z$، با استفاده از شرط \LDP\ و کران‌دار بودن نسبت چگالی‌ها، می‌توان نشان داد که واریانس نسبت درست‌نمایی کنترل می‌شود. عبارت $(e^\alpha - 1)^2$ ناشی از فاصله $L_1$ توزیع‌هاست و ضریب $(e^\alpha + 1)$ از نرمال کردن مخرج در تعریف $\chi^2$ حاصل می‌شود. جزئیات کامل جبری این اثبات در \cite{Duchi2013} (لم ۳) آمده است.
\end{اثبات}

\subsection{مزیت استفاده از $\chi^2$ نسبت به \lr{KL}}
در ادامه فصل و هنگام اثبات کران‌های پایین برای تخمین چگالی و میانگین، ما عمدتاً از واگرایی $\chi^2$ استفاده خواهیم کرد. دلایل این انتخاب عبارتند از:

\begin{enumerate}
    \item \textbf{رفتار حول صفر:} واگرایی \lr{KL} رفتار نامتقارنی دارد و بسط تیلور آن شامل جملات لگاریتمی است. در مقابل، $D_{\chi^2}$ به صورت محلی شبیه یک فرم مربعی (فاصله اقلیدسی وزن‌دار) عمل می‌کند. از آن‌جا که مکانیزم‌های \LDP\ توزیع‌ها را بسیار به هم نزدیک می‌کنند (رژیم محلی)، تقریب مرتبه دوم $\chi^2$ بسیار دقیق و کارآمد است.
    
    \item \textbf{سادگی در مخلوط‌ها:} در روش‌هایی مانند لم اسود یا فانو، ما نیاز به محاسبه واگرایی بین «مخلوطی از توزیع‌ها» ($\bar{P} = \Eset[P_\theta]$) داریم. به دلیل ساختار انتگرالی $D_{\chi^2}(P \| Q) = \int \frac{P^2}{Q} - 1$، محاسبه کران برای مخلوط‌ها ساده‌تر از \lr{KL} است که لگاریتم در داخل انتگرال دارد (نامساوی ینسن در \lr{KL} جهت نامطلوبی دارد، اما در $\chi^2$ می‌توان راحت‌تر کران بالا پیدا کرد).
    
    \item \textbf{رابطه با خطای میانگین مربعات:} در مسائل تخمین پارامتر، هدف معمولاً کمینه‌سازی خطای \lr{MSE} است. واگرایی $\chi^2$ ارتباط مستقیمی با واریانس تخمین‌گرها دارد و به طور طبیعی با کران‌های کرامر-رائو و اطلاعات فیشر مرتبط می‌شود.
\end{enumerate}

با در دست داشتن این ابزارها (انقباض \lr{KL} و $\chi^2$)، اکنون آماده‌ایم تا در بخش‌های بعدی نرخ‌های مینی‌مکس را برای مسائل خاص استخراج کنیم.


\section{اثبات نرخ مینی‌مکس برای تخمین میانگین}
\label{sec:minimax:mean-estimation}

در این بخش نهایی، ما تمام ابزارهای توسعه یافته در این فصل (لم اسود و نامساوی‌های انقباض) را ترکیب می‌کنیم تا یک کران پایین بنیادین برای مسئله کلاسیک «تخمین میانگین» در چارچوب \LDP\ اثبات کنیم. این نتیجه نشان می‌دهد که چرا روش‌هایی مانند پاسخ تصادفی (\lr{RR}) که در فصل \ref{ch:ldp} معرفی شدند، از نظر نرخ همگرایی بهینه هستند.

\subsection{تعریف مسئله}
فرض کنید داده‌های ورودی $X_1, \dots, X_n \in \{-1, 1\}^d$ بردارهای تصادفی مستقل با میانگین $\theta = \Eset[X_i]$ باشند، به طوری که $\theta \in [-1, 1]^d$. هدف ما تخمین پارامتر $\theta$ است.
ما خطای تخمین‌گر $\hat{\theta}$ را با استفاده از تابع زیان «مربع خطای اقلیدسی» ($L_2^2$) می‌سنجیم. هدف یافتن نرخ مینی‌مکس زیر است:
\begin{equation}
\mathfrak{M}_n(\theta, \mech) = \inf_{\mech, \hat{\theta}} \sup_{\theta \in [-1, 1]^d} \Eset \left[ \|\hat{\theta}(Z_1, \dots, Z_n) - \theta\|_2^2 \right]
\end{equation}
که در آن اینفیمم روی تمام مکانیزم‌های \LDP\ و تمام تخمین‌گرهای ممکن گرفته می‌شود.

\begin{قضیه}[کران پایین مینی‌مکس برای تخمین میانگین]
\label{thm:minimax-mean}
برای هر مکانیزم \LDP\ با $\alpha \in (0, 1]$، خطای مینی‌مکس در تخمین میانگین یک توزیع روی مکعب $\{-1, 1\}^d$ حداقل از مرتبه زیر است:
\begin{equation}
\inf_{\hat{\theta}} \sup_{P} \Eset \left[ \|\hat{\theta} - \theta(P)\|_2^2 \right] \ge c \cdot \frac{d}{n \alpha^2}
\end{equation}
که $c > 0$ یک ثابت عددی مطلق است.
\end{قضیه}

\begin{اثبات}
برای اثبات این قضیه، از روش «لم اسود»\LTRfootnote{Assouad's Lemma} (فصل \ref{ch:preliminaries}) استفاده می‌کنیم. استراتژی کلی این است که یک زیرمجموعه گسسته از فضای پارامتر (یک ابرمکعب) بسازیم و نشان دهیم که تشخیص رأس‌های این مکعب تحت محدودیت \LDP\ دشوار است.

\textbf{۱. ساختن فضای پارامتر گسسته:}
مجموعه رئوس ابرمکعب دودویی $\mathcal{V} = \{-1, 1\}^d$ را در نظر بگیرید. برای هر بردار $v \in \mathcal{V}$، یک توزیع احتمال $P_v$ روی داده‌های ورودی $\{-1, 1\}^d$ تعریف می‌کنیم به طوری که مولفه‌های آن مستقل باشند. برای هر مؤلفه $j \in \{1, \dots, d\}$، میانگین را به صورت زیر تنظیم می‌کنیم:
\begin{equation}
\Eset_{P_v}[(X)_j] = v_j \Delta
\end{equation}
که $\Delta \in (0, 1]$ پارامتری است که بعداً مقدار دقیق آن را تعیین می‌کنیم. به عبارت دیگر، بردار میانگین متناظر با $v$ برابر است با $\theta_v = \Delta \cdot v$.

\textbf{۲. اعمال لم اسود:}
طبق لم اسود، برای هر تخمین‌گر $\hat{\theta}$، بیشینه ریسک روی این مجموعه متناهی با مجموع خطاهای آزمون فرضیه باینری در هر مؤلفه کران‌دار می‌شود:
\begin{equation}
\label{eq:assouad-step1}
\max_{v \in \mathcal{V}} \Eset \|\hat{\theta} - \theta_v\|_2^2 \ge \frac{d}{2} \cdot \Delta^2 \cdot \min_{v, v': H(v, v')=1} \left( 1 - \|M_v^n - M_{v'}^n\|_{TV} \right)
\end{equation}
در این‌جا $H(v, v')$ فاصله همینگ است و $M_v^n$ توزیع مشترک $n$ خروجی مشاهده شده مکانیزم \mech تحت توزیع ورودی $P_v$ است.

\textbf{۳. استفاده از خاصیت انقباض \lr{LDP}:}
اکنون باید فاصله تغییرات کل $\|M_v^n - M_{v'}^n\|_{TV}$ را کران‌دار کنیم. دو همسایه $v$ و $v'$ را در نظر بگیرید که تنها در یک مؤلفه (مثلاً مؤلفه $j$) تفاوت دارند.
طبق نامساوی پینسکر و خاصیت تانسوری واگرایی \lr{KL} برای نمونه‌های مستقل ($Z_1, \dots, Z_n$):
\begin{equation}
\|M_v^n - M_{v'}^n\|_{TV}^2 \le \frac{1}{2} D_{KL}(M_v^n \| M_{v'}^n) = \frac{n}{2} D_{KL}(M_v \| M_{v'})
\end{equation}
در این‌جا $M_v$ توزیع خروجی یک‌بار اجرای مکانیزم برای یک داده ورودی است.
حال از «قضیه انقباض قوی» (قضیه \ref{thm:ldp-contraction}) استفاده می‌کنیم. می‌دانیم که واگرایی خروجی توسط واگرایی ورودی و بودجه محرمانگی کنترل می‌شود:
\begin{equation}
D_{KL}(M_v \| M_{v'}) \le (e^\alpha - 1)^2 \|P_v - P_{v'}\|_{TV}^2
\end{equation}
چون $P_v$ و $P_{v'}$ توزیع‌های برنولی ضربی هستند که تنها در مؤلفه $j$ تفاوت دارند (با میانگین‌های $+\Delta$ و $-\Delta$)، فاصله تغییرات کل آن‌ها دقیقاً برابر است با تفاوت میانگین‌ها تقسیم بر دامنه (در این‌جا ساده‌سازی شده):
\begin{equation}
\|P_v - P_{v'}\|_{TV} = \frac{(1+\Delta) - (1-\Delta)}{2} = \Delta
\end{equation}
با ترکیب این روابط و فرض $\alpha \le 1$ (که نتیجه می‌دهد $(e^\alpha - 1)^2 \approx \alpha^2$):
\begin{equation}
\|M_v^n - M_{v'}^n\|_{TV}^2 \le \frac{n}{2} \alpha^2 \Delta^2
\end{equation}
بنابراین:
\begin{equation}
\|M_v^n - M_{v'}^n\|_{TV} \le \alpha \Delta \sqrt{\frac{n}{2}}
\end{equation}

\textbf{۴. تنظیم پارامتر $\Delta$:}
برای این‌که کران پایین در رابطه \eqref{eq:assouad-step1} غیر صفر و بزرگ باشد، باید عبارت داخل پرانتز مثبت باشد. ما $\Delta$ را چنان انتخاب می‌کنیم که فاصله \lr{TV} برابر یک مقدار ثابت کوچک (مثلاً $1/2$) شود:
\begin{equation}
\alpha \Delta \sqrt{\frac{n}{2}} = \frac{1}{2} \implies \Delta = \frac{1}{\alpha \sqrt{2n}}
\end{equation}
با جایگذاری این مقدار $\Delta$ در رابطه لم اسود:
\begin{align}
\sup_{\theta} \Eset \|\hat{\theta} - \theta\|_2^2 &\ge \frac{d}{2} \cdot \Delta^2 \cdot \left( 1 - \frac{1}{2} \right) \nonumber \\
&= \frac{d}{4} \left( \frac{1}{2n \alpha^2} \right) \nonumber \\
&= \frac{d}{8 n \alpha^2}
\end{align}
این اثبات نشان می‌دهد که کران پایین از مرتبه $\Omega(\frac{d}{n \alpha^2})$ است.
\end{اثبات}

\subsection{بحث و تفسیر}
نتیجه به دست آمده در قضیه \ref{thm:minimax-mean} دارای پیام‌های مهمی برای طراحی سیستم‌های خصوصی است:

\begin{enumerate}
    \item \textbf{هزینه محرمانگی:} در مقایسه با تخمین میانگین در حالت غیرخصوصی (یا متمرکز) که نرخ خطا $\frac{d}{n}$ است، در حالت \LDP\ خطا با ضریب $\frac{1}{\alpha^2}$ افزایش می‌یابد. این یعنی برای جبران نویز اضافه شده توسط محرمانگی، حجم داده‌ها ($n$) باید متناسب با مربع بودجه محرمانگی افزایش یابد.
    
    \item \textbf{وابستگی به ابعاد:} خطا به صورت خطی با بعد داده ($d$) رشد می‌کند. این رفتار مشابه حالت کلاسیک است و نشان می‌دهد که \LDP\ وابستگی به ابعاد را تغییر نمی‌دهد، بلکه تنها ضریب ثابت را بدتر می‌کند.
    
    \item \textbf{بهینگی مکانیزم‌ها:} در \ref{sec:duchi-mechanism} دیدیم که مکانیزم تخمین میانگین دوچی دقیقاً واریانسی از مرتبه $\frac{d \log d}{n \alpha^2}$ (برای روش‌های تنک) یا $\frac{d}{n \alpha^2}$ تولید می‌کند. تطابق کران پایین ثابت شده در این‌جا با کران بالای آن مکانیزم‌ها، ثابت می‌کند که الگوریتم‌های موجود نه تنها خوب، بلکه «بهینه مینی‌مکس» هستند و بهبود قابل توجهی در نرخ همگرایی آن‌ها ممکن نیست.
\end{enumerate}

به این ترتیب، ما نشان دادیم که محدودیت‌های ذاتی \LDP\ مانع از دست‌یابی به دقت‌های بالاتر می‌شود و این محدودیت ناشی از انقباض اطلاعاتی است که در ذات تعریف محرمانگی نهفته است.