\chapter{تحلیل مینی‌مکس و هندسه اطلاعاتی در \lr{LDP}}
\label{ch:minimax-theory}

\section{مقدمه}
\label{sec:minimax:intro}
در فصل قبل (\ref{ch:ldp})، چارچوب محرمانگی تفاضلی موضعی (\lr{LDP}) را به عنوان جایگزینی برای مدل متمرکز معرفی کردیم. مشاهده شد که مکانیزم‌های استاندارد مانند «پاسخ تصادفی تعمیم‌یافته» (\lr{GRR}) و «کدگذاری‌های یکانی» (\lr{UE})، اگرچه امنیت داده‌ها را در سطح کاربر تضمین می‌کنند، اما بهای سنگینی را از منظر دقت آماری تحمیل می‌نمایند. به طور خاص، همان‌طور که در تحلیل چالش سودمندی دیدیم، در مسئله‌ی تخمین میانگین، واریانس خطا از مرتبه $\mathcal{O}\left( \frac{1}{n^2\eps^2} \right)$ در مدل متمرکز به مرتبه $\mathcal{O}\left( \frac{1}{n\al^2} \right)$ در مدل موضعی افزایش می‌یابد، که در آن $n$ نشان‌دهنده تعداد کاربران و $\eps$ و $\al$ به ترتیب بودجه‌های محرمانگی متمرکز و موضعی هستند.

این مشاهده یک پرسش بنیادین را مطرح می‌سازد: آیا این کاهش دقت، ناشی از ضعف در طراحی مکانیزم‌های موجود است، یا یک محدودیت ذاتی و غیرقابل اجتناب در طبیعت محرمانگی موضعی به شمار می‌رود؟ به بیان دیگر، آیا می‌توان مکانیزم هوشمندانه‌تری طراحی کرد که ضمن ارضای محدودیت \lr{LDP}، به نرخ خطای مدل متمرکز نزدیک شود؟

هدف اصلی این فصل، پاسخ به این پرسش با بهره‌گیری از ابزارهای قدرتمند نظریه مینی‌مکس است. نشان خواهیم داد که هزینه پرداختی در مدل موضعی (که معمولاً به صورت ضریبی از $\sqrt{n}$ در تقلیل نرخ همگرایی ظاهر می‌شود)، یک مانع اطلاعاتی بنیادین است و هیچ الگوریتمی قادر به عبور از آن نیست.

برای اثبات این ادعا، از زیرساخت‌های ریاضی بنا شده در فصل‌های پیشین، از جمله $f$-واگرایی‌ها، لم لوکم، نامساوی فانو و لم اسود استفاده خواهیم کرد. با این وجود، به‌کارگیری مستقیم این ابزارها در محیط \lr{LDP} امکان‌پذیر نیست. نوآوری اصلی این فصل، بررسی چارچوب «محرمانگی به عنوان انقباض»\LTRfootnote{Privacy as Contraction} است که به طور گسترده توسط دوچی، جردن و وین‌رایت \cite{Duchi2013, duchi2018} توسعه یافته است.

ایده مرکزی این چارچوب آن است که مکانیزم‌های \LDP\ همانند یک فیلتر اطلاعاتی عمل می‌کنند که فاصله آماری بین توزیع‌های ورودی را منقبض کرده و در نتیجه، تشخیص آن‌ها را دشوارتر می‌سازند. بر این اساس، در روند این فصل نخست خاصیت انقباضی مکانیزم‌های موضعی را بر حسب واگرایی\lr{KL} و فاصله \lr{TV} فرمول‌بندی می‌کنیم. سپس، این ویژگی را با نامساوی‌های پردازش داده\LTRfootnote{Data Processing Inequalities} ترکیب می‌نماییم تا کران‌های پایین مینی‌مکس را برای مسائل کلاسیک آماری استخراج کنیم. در نهایت، با استناد به این کران‌ها ثابت می‌کنیم که مکانیزم‌های معرفی شده در فصل قبل، در حقیقت بهینه مینی‌مکس بوده و به بهترین نرخ ممکن دست می‌یابند.

\section{محرمانگی به عنوان انقباض}
\label{sec:privacy-contraction}

همان‌طور که در فصل پیش‌نیازها (\ref{sec:bg:f-divergence}) بحث شد، $f$-واگرایی‌ها ابزاری قدرتمند برای سنجش تمایز بین توزیع‌های احتمالی هستند. ایده مرکزی در تحلیل مینی‌مکس تحت \lr{LDP} این است که مکانیزم‌های محرمانگی به عنوان «عملگرهای انقباضی»\LTRfootnote{Contraction Operators} عمل می‌کنند. به عبارت دیگر، شرط \LDP\ باعث می‌شود که توزیع‌های خروجی مکانیزم برای هر دو ورودی دل‌خواه، به یک‌دیگر بسیار نزدیک شوند و در نتیجه واگرایی بین آن‌ها محدود شود.

در این بخش، ما لم اساسی دوچی و همکاران \cite{duchi2018} را بیان و اثبات می‌کنیم. این لم نشان می‌دهد که واگرایی \lr{KL} بین توزیع‌های خروجی، توسط فاصله \lr{TV} توزیع‌های ورودی و ضریبی از بودجه محرمانگی \al کران‌دار می‌شود.
\begin{قضیه}[کران انقباض قوی برای \lr{LDP}]
\label{thm:ldp-contraction}
فرض کنید $\mech: \Xset \to \Zset$ یک مکانیزم \LDP باشد. برای هر جفت توزیع احتمالی $P_1$ و $P_2$ روی فضای ورودی $\Xset$، اگر $M_1$ و $M_2$ توزیع‌های حاشیه‌ای القا شده روی خروجی $\Zset$ باشند (یعنی $M_i(S) = \int_{\Xset} \mech(S|x) dP_i(x)$)، آنگاه نامساوی زیر برقرار است:
\begin{equation}
\label{eq:kl-contraction-bound}
D_{KL}\left(M_1 \| M_2\right) + D_{KL}\left(M_2 \| M_1\right) \lee \left(e^\al - 1\right)^2 \left(\norm{P_1 - P_2}_{TV}\right)^2
\end{equation}
\end{قضیه}

\begin{اثبات}
برای اثبات این قضیه، از خاصیت تحدب مشترک $f$-واگرایی‌ها و تعریف \LDP استفاده می‌کنیم. اثبات در دو گام انجام می‌شود: ابتدا مسئله را به ورودی‌های نقطه‌ای تقلیل می‌دهیم و سپس کران را برای آن محاسبه می‌کنیم.

\textbf{گام اول: تقلیل به ورودی‌های نقطه‌ای.}\textbf{گام اول: تقلیل به ورودی‌های نقطه‌ای.}
هدف ما در این گام این است که نشان دهیم برای یافتن کران بالای واگرایی، بررسی توزیع‌های پیچیده و کلی $P_1$ و $P_2$ ضرورتی ندارد و می‌توان مسئله را به توزیع‌های متمرکز روی تک‌نقطه‌ها تقلیل داد.

همان‌طور که در بخش پیش‌نیازها (\ref{sec:bg:f-div-def}) بررسی شد، نگاشت واگرایی $(P, Q) \mapsto D_{KL}\left(P \| Q\right)$ نسبت به هر دو ورودی خود از خاصیت تحدب مشترک\LTRfootnote{Joint Convexity} برخوردار است. از آن‌جا که توزیع‌های حاشیه‌ای $M_i$ در واقع ترکیب‌های خطی (مخلوط‌هایی) از توزیع‌های شرطی $\mech(\cdot|x)$ با وزن‌های $P_i$ هستند، کل عبارت واگرایی متقارن در سمت چپ نامساوی \eqref{eq:kl-contraction-bound} نسبت به جفت توزیع‌های ورودی $(P_1, P_2)$ تابعی محدب خواهد بود. 

بر اساس اصول بنیادین آنالیز محدب، بیشینه یک تابع محدب بر روی یک مجموعه (در این‌جا سیمپلکس تمام توزیع‌های احتمالی)، همواره در نقاط فرین\LTRfootnote{Extreme Points} آن مجموعه رخ می‌دهد. نقاط فرین در فضای توزیع‌های احتمالی، همان توزیع‌های دیراک\LTRfootnote{Dirac Distributions} متمرکز بر یک نقطه منفرد (یعنی $\delta_x$ برای $x \in \Xset$) هستند. هم‌چنین می‌دانیم که فاصله تغییرات کل $\norm{\cdot}_{TV}$ نیز محدب است و بیشینه آن زمانی حاصل می‌شود که توزیع‌های $P_1$ و $P_2$ کم‌ترین هم‌پوشانی را با یک‌دیگر داشته باشند (یعنی تکیه‌گاه آن‌ها کاملاً مجزا یا متعامد باشند).

بنابراین، با بهره‌گیری از تحدب مشترک واگرایی و خطی بودن عملگر مکانیزم $\mech$ روی مخلوط‌های احتمالی، می‌توان به لحاظ ریاضی ثابت کرد که کران بالا همواره با ارزیابی توزیع‌های نقطه‌ای به دست می‌آید \cite{Duchi2013}. فرض کنید $x, x' \in \Xset$ دو نقطه دل‌خواه از فضای ورودی باشند و $Q_x = \mech(\cdot|x)$ و $Q_{x'} = \mech(\cdot|x')$ توزیع‌های خروجی متناظر با آن‌ها فرض شوند. برای این ورودی‌های نقطه‌ای متمایز (یعنی $P_1 = \delta_x$ و $P_2 = \delta_{x'}$)، فاصله تغییرات کل دقیقاً برابر با بیشینه مقدار خود است: $\norm{\delta_x - \delta_{x'}}_{TV} = 1$. 

در نتیجه، اگر بتوانیم نشان دهیم که برای هر جفت خروجی $Q_x$ و $Q_{x'}$ نامساوی زیر برقرار است:
\begin{equation}
D_{KL}\left(Q_x \| Q_{x'}\right) + D_{KL}\left(Q_{x'} \| Q_x\right) \lee \left(e^\al - 1\right)^2
\end{equation}
آنگاه با توجه به این‌که برای ورودی‌های نقطه‌ای $\norm{\delta_x - \delta_{x'}}_{TV} = 1$ است، حکم قضیه برای حالت کلی توزیع‌ها به طور مستقیم از طریق اعمال نامساوی ینسن\LTRfootnote{Jensen's Inequality} و تحدب نتیجه می‌شود؛ به این صورت که مربع فاصله تغییرات کل توزیع‌های اولیه، یعنی $\left(\norm{P_1 - P_2}_{TV}\right)^2$، به عنوان ضریب جریمه (یا عامل نرمال‌ساز) در کران نهایی ظاهر می‌گردد.

\textbf{گام دوم: محاسبه کران برای ورودی‌های ثابت.}
طبق تعریف \LDP، برای هر خروجی $z \in \Zset$ و هر جفت ورودی $x, x'$ داریم:
\begin{equation}
e^{-\al} \lee \frac{q(z|x)}{q(z|x')} \lee e^\al
\end{equation}
که در آن $q(\cdot|x)$ تابع چگالی (یا جرم) احتمال مکانیزم است. برای درک چگونگی به دست آمدن کران نهایی، کافی است به تعریف انتگرالی واگرایی کولبک-لایبلر رجوع کنیم. واگرایی متقارن (یا واگرایی جفریز\LTRfootnote{Jeffreys Divergence}) حاصل جمع دو واگرایی $D_{KL}\left(Q_x \| Q_{x'}\right)$ و $D_{KL}\left(Q_{x'} \| Q_x\right)$ است. با استفاده از خاصیت جبری لگاریتم یعنی $\ln(A/B) = -\ln(B/A)$، می‌توانیم کسر داخل لگاریتم در انتگرال دوم را معکوس کرده و یک علامت منفی ایجاد کنیم. با فاکتورگیری از عبارت لگاریتمی مشترک، دو انتگرال به سادگی تجمیع می‌شوند:
\begin{align}
D_{KL}\left(Q_x \| Q_{x'}\right) + D_{KL}\left(Q_{x'} \| Q_x\right) &= \int_{\Zset} q(z|x) \ln\left( \frac{q(z|x)}{q(z|x')} \right) \mu(dz) - \int_{\Zset} q(z|x') \ln\left( \frac{q(z|x)}{q(z|x')} \right) \mu(dz) \nonumber \\
&= \int_{\Zset} \left( q(z|x) - q(z|x') \right) \ln\left( \frac{q(z|x)}{q(z|x')} \right) \mu(dz)
\end{align}
اهمیت بنیادین این بسط فشرده در آن است که واگرایی را به حاصل‌ضرب دو بخش تفکیک می‌کند: «اختلاف چگالی‌ها» (که با فاصله $L_1$ مرتبط است) و «لگاریتم نسبت چگالی‌ها» (که مستقیماً توسط محدودیت \LDP در بازه $\left[-\al, \al\right]$ کران‌دار است).

برای محاسبه‌ی دقیق این کران، از یک نامساوی کمکی استفاده می‌کنیم که ارتباط میان واگرایی متقارن و فاصله‌ی $L_1$ را تحت محدودیت‌های نسبت چگالی مشخص می‌کند. این نتیجه که معادل با «لم ۱» در مقاله‌ی دوچی و همکاران \cite{Duchi2013} است، به صورت زیر بیان و اثبات می‌شود:

\begin{لم}[کران واگرایی جفریز تحت نسبت چگالی محدود \cite{Duchi2013}]
\label{lem:duchi-jeffreys}
فرض کنید $P$ و $Q$ دو توزیع احتمالی روی فضای $\Zset$ با توابع چگالی $p$ و $q$ باشند، به طوری که برای تمام $z \in \Zset$ شرط $e^{-\al} \lee \frac{p(z)}{q(z)} \lee e^\al$ برقرار باشد. در این صورت، واگرایی متقارن میان آن‌ها به صورت زیر کران‌دار می‌شود:
\begin{equation}
\label{eq:duchi-lemma1}
D_{KL}\left(P \| Q\right) + D_{KL}\left(Q \| P\right) \lee \al \normo{P - Q} \lee \left(e^\al - 1\right) \normo{P - Q}
\end{equation}
\end{لم}

\begin{اثبات}
همان‌طور که می‌دانیم، با استفاده از تقارن، مجموع دو واگرایی را می‌توان به صورت یک انتگرال واحد نوشت:
\begin{equation}
D_{KL}\left(P \| Q\right) + D_{KL}\left(Q \| P\right) = \int_{\Zset} \left( p(z) - q(z) \right) \ln\left( \frac{p(z)}{q(z)} \right) \mu(dz)
\end{equation}
برای هر $z$ مشخص، عبارت داخل انتگرال را بررسی می‌کنیم. از آنجا که این عبارت نسبت به جابجایی $p$ و $q$ کاملاً متقارن است، بدون کاستن از کلیت مسئله فرض می‌کنیم $p(z) \gee q(z)$. در این حالت، اختلاف آن‌ها برابر با قدر مطلقشان خواهد بود، یعنی $p(z) - q(z) = \left| p(z) - q(z) \right|$.
بر اساس فرضِ محدود بودن نسبت چگالی‌ها، می‌دانیم که $1 \lee \frac{p(z)}{q(z)} \lee e^\al$. با اعمال تابع صعودی لگاریتم طبیعی بر این نامساوی، نتیجه می‌شود که $\ln\left(\frac{p(z)}{q(z)}\right) \lee \al$. 
اکنون با ضرب کردن این نتیجه در مقدار نامنفی $\left| p(z) - q(z) \right|$ به دست می‌آوریم:
\begin{equation}
\left( p(z) - q(z) \right) \ln\left( \frac{p(z)}{q(z)} \right) \lee \al \left| p(z) - q(z) \right|
\end{equation}
علاوه بر این، با استفاده از بسط تیلور برای تابع نمایی می‌دانیم که نامساوی پایه $\al \lee e^\al - 1$ برای تمام $\al \gee 0$ همواره برقرار است. بنابراین می‌توانیم کران فوق را بسط دهیم:
\begin{equation}
\left( p(z) - q(z) \right) \ln\left( \frac{p(z)}{q(z)} \right) \lee \left(e^\al - 1\right) \left| p(z) - q(z) \right|
\end{equation}
با انتگرال‌گیری از هر دو طرف این نامساوی بر روی کل فضای $\Zset$، اثبات لم کامل می‌گردد.
\end{اثبات}

اکنون به ادامه‌ی اثبات قضیه‌ی اصلی بازمی‌گردیم. طبق تعریف ذاتی مکانیزم \LDP، توزیع‌های خروجی $Q_x$ و $Q_{x'}$ شرط محدودیتِ نسبت چگالی را در بازه‌ی $\left[e^{-\al}, e^\al\right]$ به طور کامل ارضا می‌کنند. بنابراین با استناد مستقیم به لم \ref{lem:duchi-jeffreys} داریم:
\begin{equation}
D_{KL}\left(Q_x \| Q_{x'}\right) + D_{KL}\left(Q_{x'} \| Q_x\right) \lee \left(e^\al - 1\right) \normo{Q_x - Q_{x'}}
\end{equation}
همچنین می‌دانیم که تحت شرط \LDP، فاصله $L_1$ خروجی‌ها نیز محدود است (زیرا جرم احتمال نمی‌تواند سریع‌تر از ضریب $e^\al$ جابجا شود):
\begin{equation}
\normo{Q_x - Q_{x'}} \lee e^\al - 1
\end{equation}
با ترکیب این دو نتیجه برای ورودی‌های نقطه‌ای داریم:
\begin{align}
D_{KL}\left(Q_x \| Q_{x'}\right) + D_{KL}\left(Q_{x'} \| Q_x\right) &\lee \left(e^\al - 1\right) \normo{Q_x - Q_{x'}} \nonumber \\
&\lee \left(e^\al - 1\right)^2
\end{align}
این اثبات برای حالت ورودی‌های نقطه‌ای کامل می‌شود. تعمیم نهایی به توزیع‌های کلی $P_1$ و $P_2$ از طریق اعمال نامساوی پردازش داده (\lr{DPI}) و تحدب مشترک حاصل می‌شود که در آن، ضریب $\left(\norm{P_1 - P_2}_{TV}\right)^2$ به عنوان جریمه‌ی فاصله اولیه توزیع‌ها ظاهر می‌گردد.
\end{اثبات}

\subsection{تفسیر رژیم‌های محرمانگی}
نامساوی \eqref{eq:kl-contraction-bound} پیامدهای بسیار مهمی برای نرخ‌های همگرایی آماری دارد. رفتار ضریب $\left(e^\al - 1\right)^2$ در دو رژیم حدی قابل توجه است:

\begin{itemize}
    \item \textbf{رژیم محرمانگی بالا (رژیم $\al \lee 1$):}
    در این حالت، می‌توان از بسط تیلور استفاده کرد که نتیجه می‌دهد $e^\al - 1 \approx \al$. بنابراین کران انقباض به صورت زیر تقریب زده می‌شود:
    \begin{equation}
    D_{KL}\left(M_1 \| M_2\right) \lesssim \al^2 \norm{P_1 - P_2}_{TV}^2
    \end{equation}
    این نتیجه به وضوح نشان می‌دهد که میزان اطلاعات (و به تبع آن اطلاعات فیشر) با توان دوم $\al$ کاهش می‌یابند. این پدیده، دلیل اصلی و ریاضیاتیِ نرخ همگرایی کندتر در مدل موضعی (ظهور عامل $1/\al^2$) نسبت به مدل متمرکز است.
    
    \item \textbf{رژیم محرمانگی پایین (رژیم $\al \to \infty$):}
    در این حالت، $e^\al$ به سرعت رشد کرده و کران به سمت بی‌نهایت میل می‌کند. این رفتار کاملاً منطقی است؛ زیرا وقتی $\al$ بسیار بزرگ باشد، مکانیزم \LDP تقریباً هیچ محدودیتی اعمال نمی‌کند و اجازه می‌دهد توزیع‌های خروجی کاملاً متمایز از یک‌دیگر باقی بمانند (عدم وقوع انقباض اطلاعاتی).
\end{itemize}

این خاصیت انقباضی، ابزار اصلی ما در بخش‌های بعدی این فصل برای اثبات کران‌های پایین مینی‌مکس خواهد بود؛ جایی که نشان می‌دهیم به دلیل این انقباض، تمایز قائل شدن میان پارامترهای مدل حتی با در اختیار داشتن حجم عظیمی از داده‌ها، هم‌چنان دشوار باقی می‌ماند.

\subsection{تعمیم به $n$ کاربر مستقل (خاصیت تنسوری شدن)}
\label{sec:minimax:tensorization}
پیش از ورود به استخراج کران‌های مینی‌مکس، باید بررسی کنیم که این انقباضِ موضعی چگونه بر روی توزیع توأم یک مجموعه‌داده‌ی کامل اثر می‌گذارد. فرض کنید $n$ کاربر به طور کاملاً مستقل داده‌های خود را از طریق مکانیزم‌های \LDP (یعنی $\mech_1, \dots, \mech_n$) پردازش و منتشر می‌کنند.

اگر $M_v^n$ نشان‌دهنده‌ی توزیع توأم مشاهدات خروجی $Z^n = (Z_1, \dots, Z_n)$ به شرط توزیع ورودی $P_v$ باشد، به دلیل استقلال شرطی خروجی کاربران، واگرایی کولبک-لایبلرِ توأم دقیقاً برابر با مجموع واگرایی‌های حاشیه‌ای هر یک از آن‌ها خواهد بود. این ویژگی در نظریه اطلاعات به عنوان «خاصیت تنسوری شدن»\LTRfootnote{Tensorization Property} شناخته می‌شود:
\begin{equation}
D_{KL}\left(M_1^n \| M_2^n\right) = \sum_{i=1}^n D_{KL}\left(M_{1, i} \| M_{2, i}\right)
\end{equation}
اکنون با اعمال قضیه انقباض قوی (قضیه \ref{thm:ldp-contraction}) به صورت مجزا برای مکانیزم هر کاربر، کران انقباض برای کل مجموعه‌داده به شکل زیر در می‌آید:
\begin{equation}
D_{KL}\left(M_1^n \| M_2^n\right) \lee n \left(e^\al - 1\right)^2 \norm{P_1 - P_2}_{TV}^2
\end{equation}
این نامساوی بسیار کلیدی است؛ زیرا نشان می‌دهد که حتی با جمع‌آوری داده از $n$ کاربر، اطلاعات موجود در شبکه تنها با ضریبِ تضعیف‌شده‌ی $n\al^2$ رشد می‌کند. این رابطه همان پل ارتباطی مهمی است که در بخش‌های آتی، مستقیماً در نامساوی‌های لوکم و فانو جای‌گذاری خواهد شد.
\section{نامساوی‌های پردازش داده قوی (\lr{SDPI})}
\label{sec:sdpi}



در بخش‌های پیشین مشاهده کردیم که اعمال مکانیزم‌های \LDP\ به طور ذاتی باعث کاهش و انقباض فاصله میان توزیع‌های آماری می‌شود. این مفهوم بنیادین را می‌توان به صورت صوری‌تر و در چارچوب نظریه اطلاعات، تحت عنوان «نامساوی پردازش داده قوی»\LTRfootnote{Strong Data Processing Inequality (SDPI)} فرمول‌بندی کرد. در حالی که نامساوی پردازش داده‌ی استاندارد (\lr{DPI}) صرفاً بیان می‌کند که اعمال یک کانال تصادفی واگرایی را افزایش نمی‌دهد (یعنی $D_f\left(\mech(P) \| \mech(Q)\right) \lee D_f\left(P \| Q\right)$)، نسخه‌ی قویِ آن تضمین می‌کند که در حضور محدودیت‌هایی نظیر محرمانگی، این واگرایی با ضریبی مشخص، اکیداً کاهش می‌یابد.

\begin{تعریف}[ضریب انقباض دوبروشین\LTRfootnote{Dobrushin Contraction Coefficient}]
برای یک مکانیزم یا کانال مارکوف $\mech$ و یک واگرایی دل‌خواه $f$، ضریب انقباض $\eta_f(\mech)$ به صورت زیر تعریف می‌شود:
\begin{equation}
\label{eq:contraction-coeff}
\eta_f(\mech) = \sup_{P \neq Q} \frac{D_f\left(\mech(P) \| \mech(Q)\right)}{D_f\left(P \| Q\right)}
\end{equation}
که اگر برای یک کانال $\eta_f(\mech) < 1$ باشد، اصطلاحاً می‌گوییم آن مکانیزم دارای خاصیت \lr{SDPI} است.
\end{تعریف}

برای خانواده‌ی مکانیزم‌های \LDP، می‌توان به لحاظ ریاضی ثابت کرد که این ضریب همواره اکیداً کم‌تر از یک است، که این امر نشان‌دهنده‌ی اتلاف حتمی و غیرقابل‌بازگشت اطلاعات در فرآیند خصوصی‌سازی است. با این وجود، در تحلیل‌های مینی‌مکس ما غالباً به کران‌هایی نیاز داریم که واگرایی توزیع‌های خروجی را مستقیماً به فاصله‌ی تغییرات کل (\lr{TV}) توزیع‌های ورودی مرتبط سازند؛ چرا که در بسیاری از مسائل آماری، فاصله‌ی \lr{TV} مترِ طبیعی و استاندارد بر روی فضای پارامترها محسوب می‌شود.

\subsection{کران انقباض برای واگرایی کای-دو ($\chi^2$)}
اگرچه واگرایی کولبک-لایبلر (\lr{KL}) ابزارِ استاندارد و رایجی در نظریه اطلاعات است، اما کار با واگرایی $\chi^2$ در مسائل مربوط به \lr{LDP} غالباً به مراتب ساده‌تر و کارآمدتر است. قضیه‌ی زیر که برگرفته از تحلیل‌های دقیق دوچی و همکاران \cite{Duchi2013} است، کران انقباض را به طور خاص برای واگرایی $\chi^2$ بیان می‌کند.

\begin{قضیه}[انقباض $\chi^2$ تحت محدودیت \lr{LDP}]
\label{thm:chi-squared-contraction}
فرض کنید $\mech$ یک مکانیزم \LDP\ باشد. برای هر دو توزیع دل‌خواه $P_1$ و $P_2$ روی فضای ورودی $\Xset$، اگر $M_1 = \mech(P_1)$ و $M_2 = \mech(P_2)$ توزیع‌های القاشده در فضای خروجی باشند، نامساوی زیر همواره برقرار است:
\begin{equation}
\label{eq:chi-sq-bound}
D_{\chi^2}\left(M_1 \| M_2\right) \lee \left(e^\al + 1\right)\left(e^\al - 1\right)^2 \left(\norm{P_1 - P_2}_{TV}\right)^2
\end{equation}
\end{قضیه}
\begin{اثبات}
برای استخراج دقیق این کران، از تکنیک تجزیه توزیع‌ها (مبتنی بر فاصله تغییرات کل) و اعمال مستقیم محدودیت‌های \LDP\ بهره می‌گیریم. 

فرض کنید $L = \norm{P_1 - P_2}_{TV}$ فاصله تغییرات کل میان دو توزیع ورودی باشد. بر اساس اصول نظریه اندازه، می‌توانیم هر دو توزیع $P_1$ و $P_2$ را به صورت یک مخلوط احتمالی\LTRfootnote{Probability Mixture} از یک بخش مشترک و یک بخش کاملاً متمایز بازنویسی کنیم. به عبارت دیگر، توزیع مشترک $P_0$ و توزیع‌های مجزای $P_1'$ و $P_2'$ وجود دارند به طوری که:
\begin{equation}
P_1 = (1 - L) P_0 + L P_1', \qquad P_2 = (1 - L) P_0 + L P_2'
\end{equation}
با توجه به خطی بودن عملگر مکانیزم $\mech$ روی توزیع‌های احتمالی، توزیع‌های خروجی ($M_1$ و $M_2$) نیز از همین ساختار مخلوط پیروی می‌کنند:
\begin{equation}
M_1 = (1 - L) M_0 + L M_1', \qquad M_2 = (1 - L) M_0 + L M_2'
\end{equation}
که در آن $M_0, M_1'$ و $M_2'$ توزیع‌های خروجی حاصل از اعمال مکانیزم روی $P_0, P_1'$ و $P_2'$ هستند. 

اکنون با کم کردن این دو رابطه از یکدیگر، بخش مشترک حذف شده و تفاضل خروجی‌ها دقیقاً متناسب با $L$ به دست می‌آید:
\begin{equation}
M_1(z) - M_2(z) = L \left( M_1'(z) - M_2'(z) \right)
\end{equation}
با جایگذاری این تفاضل در تعریف انتگرالی واگرایی کای-دو، عامل $L^2$ (مربع فاصله تغییرات کل) به طور طبیعی از انتگرال خارج می‌شود:
\begin{equation}
\label{eq:chi-sq-step1}
D_{\chi^2}\left(M_1 \| M_2\right) = \int_{\Zset} \frac{\left( M_1(z) - M_2(z) \right)^2}{M_2(z)} \nu(dz) = L^2 \int_{\Zset} \frac{\left( M_1'(z) - M_2'(z) \right)^2}{M_2(z)} \nu(dz)
\end{equation}

در گام بعدی، باید عبارت داخل انتگرال را به کمک شرط \LDP\ کران‌دار کنیم. از آن‌جا که خروجی‌های $M_1'$ و $M_2'$ از یک مکانیزم $\al$-موضعی امن عبور کرده‌اند، برای هر $z \in \Zset$ نسبت چگالی آن‌ها محدود است: $M_1'(z) \lee e^\al M_2'(z)$. 
این نامساوی ایجاب می‌کند که اختلاف آن‌ها به صورت زیر کران‌دار شود:
\begin{equation}
\left| M_1'(z) - M_2'(z) \right| \lee \left(e^\al - 1\right) M_2'(z)
\end{equation}
بنابراین، صورت کسر در رابطه‌ی \eqref{eq:chi-sq-step1} حداکثر برابر با $\left(e^\al - 1\right)^2 M_2'(z)^2$ خواهد بود.

از سوی دیگر، برای مخرج کسر نیز باید ارتباطی میان $M_2(z)$ و $M_2'(z)$ بیابیم. از آن‌جا که هر دوی این توزیع‌ها خروجی مکانیزم $\mech$ (به ازای ورودی‌های متفاوت) هستند، مجدداً شرط \LDP\ تضمین می‌کند که $M_2'(z) \lee e^\al M_2(z)$. با جایگذاری این کران در مخرج، کسر داخل انتگرال به شکل زیر ساده می‌شود:
\begin{equation}
\frac{\left( M_1'(z) - M_2'(z) \right)^2}{M_2(z)} \lee \frac{\left(e^\al - 1\right)^2 M_2'(z)^2}{M_2(z)} \lee \left(e^\al - 1\right)^2 e^\al \left( \frac{M_2'(z) M_2(z)}{M_2(z)} \right) = e^\al \left(e^\al - 1\right)^2 M_2'(z)
\end{equation}

در نهایت، با جایگذاری این کران در انتگرال \eqref{eq:chi-sq-step1} و استفاده از این واقعیت که انتگرال یک توزیع احتمال برابر با یک است ($\int M_2'(z) \nu(dz) = 1$)، به دست می‌آوریم:
\begin{equation}
D_{\chi^2}\left(M_1 \| M_2\right) \lee L^2 \cdot e^\al \left(e^\al - 1\right)^2 \int_{\Zset} M_2'(z) \nu(dz) = e^\al \left(e^\al - 1\right)^2 \left(\norm{P_1 - P_2}_{TV}\right)^2
\end{equation}
از آن‌جا که به صورت بدیهی $e^\al \lee e^\al + 1$ است، کران استخراج‌شده به طور کامل در نامساوی بیان‌شده در قضیه صدق می‌کند و اثبات کامل می‌گردد.
\end{اثبات}

\subsection{مزیت‌های تحلیلی واگرایی $\chi^2$ نسبت به \lr{KL}}
در ادامه‌ی این فصل و به ویژه هنگام استخراج کران‌های پایین مینی‌مکس برای مسائلی نظیر تخمین میانگین در ابعاد بالا، ما رویکرد خود را از \lr{KL} به سمت واگرایی $\chi^2$ تغییر خواهیم داد. این انتخابِ استراتژیک ریشه در سه ویژگی بنیادین دارد:

نخست، «رفتار متقارن حول صفر»؛ واگرایی \lr{KL} ذاتاً نامتقارن است و بسط تیلورِ آن شامل جملات پیچیده‌ی لگاریتمی می‌شود. در مقابل، $D_{\chi^2}$ به صورت محلی رفتاری کاملاً شبیه به یک فرم مربعی (فاصله‌ی اقلیدسی وزن‌دار) از خود نشان می‌دهد. از آن‌جا که مکانیزم‌های \LDP توزیع‌ها را به شدت منقبض کرده و به یکدیگر نزدیک می‌سازند (قرارگیری در رژیم محلی)، تقریب مرتبه دومِ $\chi^2$ برای تحلیل این مجاورت بسیار دقیق‌تر و از نظر جبری خوش‌رفتارتر است.

دلیل دوم، «سادگی در محاسبه‌ی مخلوط‌های احتمالی» است. در تکنیک‌های پیشرفته‌ای مانند لم اسود یا روش فانو، ما نیازمند محاسبه‌ی واگرایی میان یک فرض منفرد و «مخلوطی از توزیع‌ها» (مانند $\bar{P} = \Eset\left[P_\theta\right]$) هستیم. به دلیل ساختار انتگرالیِ ساده‌تر در واگرایی کای-دو که به فرم $D_{\chi^2}\left(P \| Q\right) = \int \left(P^2 / Q\right) - 1$ نوشته می‌شود، محاسبات جبری برای مخلوط‌ها به مراتب سرراست‌تر از \lr{KL} است که عملگر لگاریتم را درون انتگرال خود حبس کرده است.

در نهایت، «ارتباط ذاتی با خطای میانگین مربعات (\lr{MSE})»؛ در مسائل تخمین پارامتر، هدفِ غایی معمولاً کمینه‌سازی ریسک مربعات است. واگرایی $\chi^2$ به طور طبیعی و مستقیم با واریانس تخمین‌گرها، کران کرامر-رائو\LTRfootnote{Cramér-Rao Bound} و ماتریس اطلاعات فیشر\LTRfootnote{Fisher Information} پیوند دارد که این امر مسیر استخراج کران‌های مینی‌مکس را هموارتر می‌سازد.

\subsection{تنسوری شدن واگرایی $\chi^2$ برای $n$ کاربر}
پیش از ورود به قضایای اصلی، باید به یک تفاوت اساسی میان \lr{KL} و $\chi^2$ در مواجهه با داده‌های مستقل توجه کنیم. در واگرایی \lr{KL}، اطلاعاتِ $n$ کاربر مستقل به صورت خطی با هم جمع می‌شوند. اما واگرایی $\chi^2$ خاصیت جمع‌پذیری ندارد و به صورت ضرب‌شونده\LTRfootnote{Multiplicative Tensorization} عمل می‌کند. اگر $P^n$ و $Q^n$ نشان‌دهنده‌ی توزیع توأم مشاهدات مستقل $n$ کاربر باشند، رابطه‌ی زیر برقرار است:
\begin{equation}
1 + D_{\chi^2}\left(P^n \| Q^n\right) = \left( 1 + D_{\chi^2}\left(P \| Q\right) \right)^n
\end{equation}
این ویژگیِ نمایی در نگاه اول ممکن است نگران‌کننده به نظر برسد، اما در رژیم محرمانگی بالا (جایی که $D_{\chi^2}\left(P \| Q\right)$ به دلیل وجود ضریب $\al^2$ بسیار کوچک و نزدیک به صفر است)، با استفاده از بسط دوجمله‌ای تقریب $(1+x)^n \approx 1 + nx$ برقرار می‌شود. بنابراین، واگرایی توأمِ مجموعه‌داده به شکل زیر مهار می‌گردد:
\begin{equation}
\label{eq:chi-tensorization-approx}
D_{\chi^2}\left(P^n \| Q^n\right) \approx n D_{\chi^2}\left(P \| Q\right) \lee n \left(e^\al + 1\right)\left(e^\al - 1\right)^2 \left(\norm{P_1 - P_2}_{TV}\right)^2
\end{equation}
همین رابطه‌ی تنسوری است که به ما اجازه می‌دهد در لم اسود، تأثیر تجمیعی $n$ کاربر را به شکلی تحلیلی وارد محاسبات کرده و کران‌های دقیق بر حسب $n$ و $\al$ به دست آوریم.
\section{اثبات نرخ مینی‌مکس برای تخمین میانگین}
\label{sec:minimax:mean-estimation}

در این بخش نهایی، ما تمام ابزارهای توسعه یافته در این فصل (لم اسود و نامساوی‌های انقباض) را ترکیب می‌کنیم تا یک کران پایین بنیادین برای مسئله کلاسیک «تخمین میانگین» در چارچوب \LDP\ اثبات کنیم. این نتیجه نشان می‌دهد که چرا روش‌هایی مانند پاسخ تصادفی (\lr{RR}) که در فصل \ref{ch:ldp} معرفی شدند، از نظر نرخ همگرایی بهینه هستند.

\subsection{تعریف مسئله}
فرض کنید داده‌های ورودی $X_1, \dots, X_n \in \{-1, 1\}^d$ بردارهای تصادفی مستقل با میانگین $\theta = \Eset[X_i]$ باشند، به طوری که $\theta \in [-1, 1]^d$. هدف ما تخمین پارامتر $\theta$ است.
ما خطای تخمین‌گر $\hat{\theta}$ را با استفاده از تابع زیان «مربع خطای اقلیدسی» ($L_2^2$) می‌سنجیم. هدف یافتن نرخ مینی‌مکس زیر است:
\begin{equation}
\mathfrak{M}_n(\theta, \mech) = \inf_{\mech, \hat{\theta}} \sup_{\theta \in [-1, 1]^d} \Eset \left[ \|\hat{\theta}(Z_1, \dots, Z_n) - \theta\|_2^2 \right]
\end{equation}
که در آن اینفیمم روی تمام مکانیزم‌های \LDP\ و تمام تخمین‌گرهای ممکن گرفته می‌شود.

\begin{قضیه}[کران پایین مینی‌مکس برای تخمین میانگین]
\label{thm:minimax-mean}
برای هر مکانیزم \LDP\ با $\alpha \in (0, 1]$، خطای مینی‌مکس در تخمین میانگین یک توزیع روی مکعب $\{-1, 1\}^d$ حداقل از مرتبه زیر است:
\begin{equation}
\inf_{\hat{\theta}} \sup_{P} \Eset \left[ \|\hat{\theta} - \theta(P)\|_2^2 \right] \ge c \cdot \frac{d}{n \alpha^2}
\end{equation}
که $c > 0$ یک ثابت عددی مطلق است.
\end{قضیه}

\begin{اثبات}
برای اثبات این قضیه، از روش «لم اسود»\LTRfootnote{Assouad's Lemma} (فصل \ref{ch:preliminaries}) استفاده می‌کنیم. استراتژی کلی این است که یک زیرمجموعه گسسته از فضای پارامتر (یک ابرمکعب) بسازیم و نشان دهیم که تشخیص رأس‌های این مکعب تحت محدودیت \LDP\ دشوار است.

\textbf{۱. ساختن فضای پارامتر گسسته:}
مجموعه رئوس ابرمکعب دودویی $\mathcal{V} = \{-1, 1\}^d$ را در نظر بگیرید. برای هر بردار $v \in \mathcal{V}$، یک توزیع احتمال $P_v$ روی داده‌های ورودی $\{-1, 1\}^d$ تعریف می‌کنیم به طوری که مولفه‌های آن مستقل باشند. برای هر مؤلفه $j \in \{1, \dots, d\}$، میانگین را به صورت زیر تنظیم می‌کنیم:
\begin{equation}
\Eset_{P_v}[(X)_j] = v_j \Delta
\end{equation}
که $\Delta \in (0, 1]$ پارامتری است که بعداً مقدار دقیق آن را تعیین می‌کنیم. به عبارت دیگر، بردار میانگین متناظر با $v$ برابر است با $\theta_v = \Delta \cdot v$.

\textbf{۲. اعمال لم اسود:}
طبق لم اسود، برای هر تخمین‌گر $\hat{\theta}$، بیشینه ریسک روی این مجموعه متناهی با مجموع خطاهای آزمون فرضیه باینری در هر مؤلفه کران‌دار می‌شود:
\begin{equation}
\label{eq:assouad-step1}
\max_{v \in \mathcal{V}} \Eset \|\hat{\theta} - \theta_v\|_2^2 \ge \frac{d}{2} \cdot \Delta^2 \cdot \min_{v, v': H(v, v')=1} \left( 1 - \|M_v^n - M_{v'}^n\|_{TV} \right)
\end{equation}
در این‌جا $H(v, v')$ فاصله همینگ است و $M_v^n$ توزیع مشترک $n$ خروجی مشاهده شده مکانیزم \mech تحت توزیع ورودی $P_v$ است.

\textbf{۳. استفاده از خاصیت انقباض \lr{LDP}:}
اکنون باید فاصله تغییرات کل $\|M_v^n - M_{v'}^n\|_{TV}$ را کران‌دار کنیم. دو همسایه $v$ و $v'$ را در نظر بگیرید که تنها در یک مؤلفه (مثلاً مؤلفه $j$) تفاوت دارند.
طبق نامساوی پینسکر و خاصیت تانسوری واگرایی \lr{KL} برای نمونه‌های مستقل ($Z_1, \dots, Z_n$):
\begin{equation}
\|M_v^n - M_{v'}^n\|_{TV}^2 \le \frac{1}{2} D_{KL}(M_v^n \| M_{v'}^n) = \frac{n}{2} D_{KL}(M_v \| M_{v'})
\end{equation}
در این‌جا $M_v$ توزیع خروجی یک‌بار اجرای مکانیزم برای یک داده ورودی است.
حال از «قضیه انقباض قوی» (قضیه \ref{thm:ldp-contraction}) استفاده می‌کنیم. می‌دانیم که واگرایی خروجی توسط واگرایی ورودی و بودجه محرمانگی کنترل می‌شود:
\begin{equation}
D_{KL}(M_v \| M_{v'}) \le (e^\alpha - 1)^2 \|P_v - P_{v'}\|_{TV}^2
\end{equation}
چون $P_v$ و $P_{v'}$ توزیع‌های برنولی ضربی هستند که تنها در مؤلفه $j$ تفاوت دارند (با میانگین‌های $+\Delta$ و $-\Delta$)، فاصله تغییرات کل آن‌ها دقیقاً برابر است با تفاوت میانگین‌ها تقسیم بر دامنه (در این‌جا ساده‌سازی شده):
\begin{equation}
\|P_v - P_{v'}\|_{TV} = \frac{(1+\Delta) - (1-\Delta)}{2} = \Delta
\end{equation}
با ترکیب این روابط و فرض $\alpha \le 1$ (که نتیجه می‌دهد $(e^\alpha - 1)^2 \approx \alpha^2$):
\begin{equation}
\|M_v^n - M_{v'}^n\|_{TV}^2 \le \frac{n}{2} \alpha^2 \Delta^2
\end{equation}
بنابراین:
\begin{equation}
\|M_v^n - M_{v'}^n\|_{TV} \le \alpha \Delta \sqrt{\frac{n}{2}}
\end{equation}

\textbf{۴. تنظیم پارامتر $\Delta$:}
برای این‌که کران پایین در رابطه \eqref{eq:assouad-step1} غیر صفر و بزرگ باشد، باید عبارت داخل پرانتز مثبت باشد. ما $\Delta$ را چنان انتخاب می‌کنیم که فاصله \lr{TV} برابر یک مقدار ثابت کوچک (مثلاً $1/2$) شود:
\begin{equation}
\alpha \Delta \sqrt{\frac{n}{2}} = \frac{1}{2} \implies \Delta = \frac{1}{\alpha \sqrt{2n}}
\end{equation}
با جایگذاری این مقدار $\Delta$ در رابطه لم اسود:
\begin{align}
\sup_{\theta} \Eset \|\hat{\theta} - \theta\|_2^2 &\ge \frac{d}{2} \cdot \Delta^2 \cdot \left( 1 - \frac{1}{2} \right) \nonumber \\
&= \frac{d}{4} \left( \frac{1}{2n \alpha^2} \right) \nonumber \\
&= \frac{d}{8 n \alpha^2}
\end{align}
این اثبات نشان می‌دهد که کران پایین از مرتبه $\Omega(\frac{d}{n \alpha^2})$ است.
\end{اثبات}

\subsection{بحث و تفسیر}
نتیجه به دست آمده در قضیه \ref{thm:minimax-mean} دارای پیام‌های مهمی برای طراحی سیستم‌های خصوصی است:

\begin{enumerate}
    \item \textbf{هزینه محرمانگی:} در مقایسه با تخمین میانگین در حالت غیرخصوصی (یا متمرکز) که نرخ خطا $\frac{d}{n}$ است، در حالت \LDP\ خطا با ضریب $\frac{1}{\alpha^2}$ افزایش می‌یابد. این یعنی برای جبران نویز اضافه شده توسط محرمانگی، حجم داده‌ها ($n$) باید متناسب با مربع بودجه محرمانگی افزایش یابد.
    
    \item \textbf{وابستگی به ابعاد:} خطا به صورت خطی با بعد داده ($d$) رشد می‌کند. این رفتار مشابه حالت کلاسیک است و نشان می‌دهد که \LDP\ وابستگی به ابعاد را تغییر نمی‌دهد، بلکه تنها ضریب ثابت را بدتر می‌کند.
    
    \item \textbf{بهینگی مکانیزم‌ها:} در \ref{sec:duchi-mechanism} دیدیم که مکانیزم تخمین میانگین دوچی دقیقاً واریانسی از مرتبه $\frac{d \log d}{n \alpha^2}$ (برای روش‌های تنک) یا $\frac{d}{n \alpha^2}$ تولید می‌کند. تطابق کران پایین ثابت شده در این‌جا با کران بالای آن مکانیزم‌ها، ثابت می‌کند که الگوریتم‌های موجود نه تنها خوب، بلکه «بهینه مینی‌مکس» هستند و بهبود قابل توجهی در نرخ همگرایی آن‌ها ممکن نیست.
\end{enumerate}

به این ترتیب، ما نشان دادیم که محدودیت‌های ذاتی \LDP\ مانع از دست‌یابی به دقت‌های بالاتر می‌شود و این محدودیت ناشی از انقباض اطلاعاتی است که در ذات تعریف محرمانگی نهفته است.