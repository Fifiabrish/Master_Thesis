\chapter{محرمانگی تفاضلی محلی}
\label{ch:ldp}

در فصل گذشته، مبانی نظری حریم خصوصی متمرکز (\lr{CDP}) و ابزارهای آماری لازم برای تحلیل آن را مرور کردیم. در این فصل، به طور اختصاصی به \textbf{چارچوب محرمانگی تفاضلی محلی\LTRfootnote{Local Differential Privacy (LDP)}(\lr{LDP})} می‌پردازیم. این مدل، که امروزه در سیستم‌های توزیع‌شده و جمع‌آوری داده‌های بزرگ‌مقیاس کاربرد فراوان دارد، پارادایم اعتماد را از «سرور مرکزی» به «کاربر نهایی» تغییر می‌دهد.

\section{مقدمه و گذار از مدل متمرکز}
\label{sec:ldp:intro}

همان‌طور که در بخش \ref{sec:bg:cdp} دیدیم، مدل متمرکز نیازمند وجود یک متصدی مورد اعتماد\LTRfootnote{Trusted Curator} است که به داده‌های خام دسترسی داشته باشد. اگرچه این مدل دقت آماری بالایی را فراهم می‌کند، اما در دنیای واقعی با چالش‌های امنیتی و حقوقی جدی روبروست:
\begin{itemize}
    \item \textbf{نقطه شکست مرکزی\LTRfootnote{Single Point of Failure}:} سرور مرکزی هدف جذابی برای مهاجمان است. نشت اطلاعات از سرور (چه بر اثر هک و چه بر اثر خطای انسانی) حریم خصوصی تمام کاربران را به خطر می‌اندازد.
    \item \textbf{عدم اعتماد کاربران:} در بسیاری از کاربردها (مانند جمع‌آوری داده‌های پزشکی یا تاریخچه مرورگر)، کاربران تمایلی ندارند داده‌های حساس خود را حتی به یک سرور «مطمئن» بسپارند.
\end{itemize}

در پاسخ به این چالش‌ها، مدل محرمانگی تفاضلی محلی مطرح شد. در \lr{LDP}، فرآیند خصوصی‌سازی (افزودن نویز) به سمت کلاینت (کاربر) منتقل می‌شود. به این معنا که داده‌ها \textit{قبل} از ترک دستگاه کاربر، نویزدار می‌شوند و سرور تنها به داده‌های بی‌نام و نویزدار دسترسی دارد (شکل \ref{fig:ldp-model}).

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/LDP-model.jpg}
	\caption{مدل محرمانگی تفاضلی محلی (\lr{LDP}). نویز به صورت محلی روی دستگاه کاربر اضافه می‌شود.}
	\label{fig:ldp-model}
\end{figure}

این رویکرد توسط شرکت‌های بزرگ فناوری برای جمع‌آوری داده‌های تله‌متری پذیرفته شده است. برای مثال، گوگل از مکانیزم \lr{RAPPOR} در مرورگر کروم، و اپل و مایکروسافت از روش‌های مشابهی برای جمع‌آوری داده‌های آماری از سیستم‌عامل‌های خود استفاده می‌کنند.

\section{تعاریف رسمی و مدل‌های محاسباتی}
\label{sec:ldp:definitions}

در مدل محلی، ما $n$ کاربر داریم که هر کدام یک داده‌ی خصوصی $X_i$ از دامنه $\Xset$ در اختیار دارند. هر کاربر به طور مستقل یک الگوریتم تصادفی (مکانیزم) را اجرا می‌کند و خروجی $Z_i$ را منتشر می‌کند.

\subsection{تعریف \lr{LDP}}

هسته‌ی اصلی این مدل، «تصادفی‌ساز محلی» است.

\begin{تعریف}[تصادفی‌ساز محلی\LTRfootnote{Local Randomizer}]
یک مکانیزم تصادفی $\mech: \Xset \to \Zset$ را یک تصادفی‌ساز محلی می‌نامیم که ورودی $x \in \Xset$ را می‌گیرد و خروجی $z \in \Zset$ را بر اساس توزیع احتمال شرطی $Q(z|x)$ تولید می‌کند.
\end{تعریف}

شرط محرمانگی در اینجا تضمین می‌کند که با مشاهده‌ی خروجی $z$، تمایز قائل شدن بین هر دو ورودی اولیه $x$ و $x'$ دشوار باشد. تفاوت کلیدی این تعریف با مدل متمرکز در این است که در \lr{CDP} ما دو پایگاه داده‌ی همسایه را مقایسه می‌کردیم، اما در اینجا هر دو مقدار ورودی ممکن مقایسه می‌شوند.

\begin{تعریف}[$\alpha$-محرمانگی تفاضلی محلی]
\label{def:alpha-ldp}
یک مکانیزم $\mech$ دارای $\alpha$-محرمانگی تفاضلی محلی ($\alpha$-\LDP) است اگر برای تمام جفت ورودی‌های $x, x' \in \Xset$ و هر زیرمجموعه از خروجی‌ها $\Sset \subseteq \Zset$ داشته باشیم:
\begin{equation}
\label{eq:ldp-def}
\sup_{S} \frac{\mathrm{Pr}[\mech(x) \in \Sset]}{\mathrm{Pr}[\mech(x') \in \Sset]} \le e^{\alpha}
\end{equation}
\end{تعریف}
(نکته: در متون آماری مانند \cite{duchi} معمولاً از پارامتر $\alpha$ به جای $\eps$ برای نمایش بودجه حریم خصوصی محلی استفاده می‌شود تا تمایز آن با مدل متمرکز مشخص باشد. ما نیز در این فصل و فصول بعدی از این نمادگذاری پیروی می‌کنیم).

این تعریف معادل شرط زیر بر روی واگرایی ماکزیمم ($D_\infty$) بین توزیع‌های شرطی است:
\begin{equation}
\sup_{x, x' \in \Xset} D_\infty(Q(\cdot|x) || Q(\cdot|x')) \le \alpha
\end{equation}
\subsection{تعمیم‌ها و خواص}

علاوه بر تعریف استاندارد $\alpha$-\LDP (معادله \ref{eq:ldp-def})، دو مفهوم دیگر نیز در تحلیل‌های نظری و طراحی مکانیزم‌ها اهمیت دارند: محرمانگی تقریبی و خاصیت ترکیب.

\begin{تعریف}[$(\alpha, \delta)$-محرمانگی تفاضلی محلی]
\label{def:approx-ldp}
یک مکانیزم تصادفی $\mech$ دارای محرمانگی تفاضلی محلی تقریبی یا $(\alpha, \delta)$-\LDP است اگر برای هر دو ورودی $x, x' \in \Xset$ و هر زیرمجموعه خروجی $\Sset \subseteq \Zset$ داشته باشیم:
\begin{equation}
\mathrm{Pr}[\mech(x) \in \Sset] \le e^{\alpha} \cdot \mathrm{Pr}[\mech(x') \in \Sset] + \delta
\end{equation}
این تعریف (که در \cite{wang2020comprehensive} نیز بررسی شده است)، اجازه‌ی یک احتمال شکست کوچک $\delta$ را می‌دهد. اهمیت نظری این تعریف در آن است که ارتباط مستقیمی با واگرایی $E_\gamma$ (که در فصل قبل معرفی شد) دارد.
\end{تعریف}

\begin{قضیه}[ترکیب ترتیبی\LTRfootnote{Sequential Composition}]
\label{thm:seq-composition}
اگر یک کاربر در $k$ مرحله‌ی مختلف در پروتکل‌های $\mech_1, \dots, \mech_k$ شرکت کند که هر کدام به ترتیب دارای بودجه‌ی حریم خصوصی $\alpha_i$ باشند، آنگاه کل فرآیند دارای محرمانگی تفاضلی محلی با بودجه‌ی $\sum_{i=1}^k \alpha_i$ خواهد بود.
این خاصیت در تحلیل پروتکل‌های تعاملی (بخش \ref{sec:ldp:interaction}) که در آن خروجی‌های بعدی به خروجی‌های قبلی وابسته هستند، نقش بنیادین دارد.
\end{قضیه}

\subsection{پروتکل‌های تعاملی و غیرتعاملی}
\label{sec:ldp:interaction}

یکی از جنبه‌های مهم در تحلیل نرخ‌های مینیماکس (که در فصل بعد به آن می‌پردازیم)، نحوه‌ی تعامل کاربران با سرور است. دوچِی و همکاران \cite{duchi2013local} پروتکل‌های محلی را به دو دسته تقسیم می‌کنند:

\textbf{۱. پروتکل‌های غیرتعاملی\LTRfootnote{Non-interactive}:}
در این حالت، خروجی هر کاربر $Z_i$ تنها به ورودی خودش $X_i$ وابسته است و مستقل از داده‌ها یا خروجی‌های سایر کاربران تولید می‌شود.
\begin{equation}
Z_i = \mech_i(X_i)
\end{equation}
این مدل ساده‌ترین و رایج‌ترین شکل پیاده‌سازی \lr{LDP} است.

\textbf{۲. پروتکل‌های تعاملی (ترتیبی)\LTRfootnote{Sequential/Interactive}:}
در این حالت، مکانیزم کاربر $i$ می‌تواند به خروجی‌های منتشر شده توسط کاربران قبلی ($Z_1, \dots, Z_{i-1}$) وابسته باشد. به عبارت دیگر، کانال ارتباطی $Q_i$ می‌تواند به صورت پویا بر اساس تاریخچه تغییر کند:
\begin{equation}
Z_i \sim Q_i(\cdot | X_i, Z_1, \dots, Z_{i-1})
\end{equation}
این مدل به الگوریتم‌های تطبیقی اجازه می‌دهد تا دقت تخمین را بهبود بخشند. با این حال، همان‌طور که در فصل بعد خواهیم دید، حتی با وجود تعامل، محدودیت‌های بنیادی $f$-واگرایی همچنان مانع از کاهش چشمگیر نرخ خطا می‌شوند.


\section{مکانیزم‌های پایه در \lr{LDP}}
\label{sec:ldp:mechanisms}

مکانیزم‌های پایه در مدل محلی با مدل متمرکز متفاوت هستند. دو مورد از رایج‌ترین آن‌ها عبارتند از:

\begin{itemize}
    \item 
    \textbf{پاسخ تصادفی\LTRfootnote{Randomized Response}:}
    پایه‌ای‌ترین مکانیزم در مدل محلی، پاسخ تصادفی است که در اصل توسط وارنر \cite{warner1965randomized} (حتی پیش از \lr{DP}) معرفی شد. این مکانیزم اغلب برای پرس‌وجوهای دودویی (مانند «آیا شما ویژگی $P$ را دارید؟») استفاده می‌شود.
    در این روش، کاربر به صورت زیر عمل می‌کند:
    \begin{enumerate}
        \item یک سکه پرتاب کن.
        \item اگر «شیر» آمد، پاسخ حقیقی را بگو.
        \item اگر «خط» آمد، سکه‌ی دومی پرتاب کن و بر اساس نتیجه‌ی آن سکه (شیر = بله، خط = خیر) پاسخ بده.
    \end{enumerate}
    این روش تضمین می‌کند که هر پاسخی (چه «بله» و چه «خیر») دارای امکان انکارپذیری قابل قبول\LTRfootnote{Plausible Deniability} است. برای مثال، پاسخ «بله» لزوماً به معنای مثبت بودن ویژگی در فرد نیست، چرا که ممکن است در اثر پرتاب سکه‌ی دوم حاصل شده باشد.
    
    \item
    \textbf{مکانیزم‌های دامنه بزرگ:}
    برای داده‌هایی با دامنه‌های بزرگ‌تر (مانند کلمات در یک جستجو یا تخمین فراوانی)، مکانیزم پاسخ تصادفی ساده کارایی ندارد. در این موارد از مکانیزم‌های مبتنی بر کدگذاری یگانی\LTRfootnote{Unary Encoding} مانند \lr{RAPPOR} یا \lr{OUE} استفاده می‌شود.
\end{itemize}

\section{چالش سودمندی در مدل محلی}
\label{sec:ldp:utility}

بهای عدم اعتماد به سرور، کاهش شدید سودمندی\LTRfootnote{Utility} آماری است. از آنجایی که نویز به داده‌ی هر فرد به صورت مستقل اضافه می‌شود، خطای تجمیعی در مدل \lr{LDP} بسیار بیش‌تر از مدل متمرکز \lr{CDP} است.

برای رسیدن به سطح دقت مشابه، مدل محلی معمولاً به تعداد کاربران $n$ بسیار بیشتری نیاز دارد. به طور کلی، در حالی که خطای مکانیزم‌های \lr{CDP} اغلب با $O(1/n)$ کاهش می‌یابد، خطای مکانیزم‌های \lr{LDP} معمولاً با $O(1/\sqrt{n})$ کاهش می‌یابد. این کاهش در «اندازه نمونه مؤثر» یکی از موضوعات اصلی است که در فصل آینده با استفاده از ابزارهای $f$-واگرایی آن را اثبات خواهیم کرد.



\section{مکانیزم‌های پایه در \lr{LDP}}
\label{sec:ldp:mechanisms}

در این بخش، مکانیزم‌های بنیادین را معرفی می‌کنیم که برای تحقق محرمانگی تفاضلی محلی استفاده می‌شوند. این مکانیزم‌ها بلوک‌های سازنده‌ی پروتکل‌های پیچیده‌تر هستند.

\subsection{پاسخ تصادفی دودویی (\lr{RR})}
\label{sec:ldp:rr}

پایه‌ای‌ترین و کلاسیک‌ترین مکانیزم در مدل محلی، «پاسخ تصادفی»\LTRfootnote{Randomized Response (RR)} است که توسط وارنر \cite{warner1965randomized} معرفی شد. فرض کنید دامنه ورودی دودویی باشد ($\Xset = \{0, 1\}$).

مکانیزم $\mech_{RR}$ با ورودی $x \in \{0, 1\}$، خروجی $z \in \{0, 1\}$ را طبق احتمالات زیر تولید می‌کند:
\begin{equation}
\mathrm{Pr}[z = x] = p, \quad \mathrm{Pr}[z \neq x] = 1 - p
\end{equation}
برای اینکه این مکانیزم شرط $\alpha$-\LDP را برآورده کند، طبق تعریف \ref{def:alpha-ldp} باید نسبت احتمالات حداکثر $e^\alpha$ باشد:
\begin{equation}
\frac{\mathrm{Pr}[z=1 | x=1]}{\mathrm{Pr}[z=1 | x=0]} = \frac{p}{1-p} \le e^\alpha
\end{equation}
بنابراین، با انتخاب $p = \frac{e^\alpha}{1+e^\alpha}$، مکانیزم بهینه $\alpha$-\LDP حاصل می‌شود. در این حالت، واریانس تخمین‌گر حاصل از این مکانیزم برابر است با:
\begin{equation}
\mathrm{Var}[\hat{x}] = \frac{e^\alpha}{(e^\alpha - 1)^2}
\end{equation}
که نشان می‌دهد برای $\alpha$های کوچک، واریانس به سرعت افزایش می‌یابد (تقریباً با نرخ $1/\alpha^2$).

\subsection{پاسخ تصادفی تعمیم‌یافته (\lr{GRR})}
\label{sec:ldp:grr}

زمانی که دامنه ورودی شامل $k > 2$ عنصر باشد ($\Xset = \{1, \dots, k\}$)، از نسخه تعمیم‌یافته پاسخ تصادفی\LTRfootnote{Generalized Randomized Response (GRR)} استفاده می‌شود \cite{kairouz2014extremal}.
در این مکانیزم، برای ورودی $x$:
\begin{equation}
\mathrm{Pr}[\mech(x) = z] = 
\begin{cases} 
p & \text{if } z = x \\
q & \text{if } z \neq x
\end{cases}
\end{equation}
از آنجا که مجموع احتمالات باید ۱ باشد، داریم $p + (k-1)q = 1$. برای برقراری شرط $\alpha$-\LDP، باید $\frac{p}{q} \le e^\alpha$ باشد. با حل این دستگاه معادلات، مقادیر بهینه $p$ و $q$ به صورت زیر به دست می‌آیند:
\begin{equation}
p = \frac{e^\alpha}{e^\alpha + k - 1}, \quad q = \frac{1}{e^\alpha + k - 1}
\end{equation}
این مکانیزم برای دامنه‌های کوچک ($k$ کوچک) بسیار کارآمد است، اما با افزایش $k$، دقت آن به شدت کاهش می‌یابد زیرا احتمال گزارش پاسخ صحیح ($p$) به سمت صفر میل می‌کند.

\subsection{مکانیزم لاپلاس در مدل محلی}
\label{sec:ldp:laplace}

برای داده‌های عددی پیوسته یا گسسته (مثلاً $x \in [-1, 1]$)، می‌توان از مکانیزم لاپلاس استفاده کرد. اگرچه این مکانیزم در مدل متمرکز بسیار محبوب است، اما در مدل محلی چالش‌برانگیز است.
طبق تعریف، حساسیت سراسری در مدل محلی برابر با قطر دامنه است (چون هر دو ورودی $x, x'$ باید تمایزناپذیر باشند). اگر $\Xset = [-1, 1]$ باشد، حساسیت $\Delta = |1 - (-1)| = 2$ است.
بنابراین، مکانیزم لاپلاس محلی به صورت زیر تعریف می‌شود:
\begin{equation}
\mech_{Lap}(x) = x + \eta, \quad \eta \sim \mathrm{Lap}\left(\frac{2}{\alpha}\right)
\end{equation}
نکته مهمی که دوچی و همکاران \cite{duchi2013local} به آن اشاره کرده‌اند این است که مکانیزم لاپلاس در مدل محلی، به خصوص برای ابعاد بالا ($d > 1$)، \textbf{زیر-بهینه} (Sub-optimal) است و نرخ خطای آن بیشتر از مکانیزم‌های تخصصی‌تر (مانند نمونه‌برداری هایپرکیوب) است که در تحلیل مینیماکس فصل بعد بررسی خواهیم کرد.