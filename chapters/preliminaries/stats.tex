\section{مبانی آماری و کران‌های اطلاعاتی}
\label{sec:bg:statistical-foundations}

در بخش‌های پیشین، ابزارهای سنجش فاصله بین توزیع‌ها (مانند $f$-واگرایی‌ها) را در بخش \ref{sec:bg:f-divergence} معرفی کردیم. در این بخش، به معرفی چارچوب «نظریه تصمیم آماری»\LTRfootnote{Statistical Decision Theory} می‌پردازیم. هدف ما ایجاد پلی مستحکم میان مفاهیم انتزاعی فاصله و مسأله‌ی عملیاتی تخمین پارامتر است. تعاریف و لم‌های ارائه شده در این بخش، ابزار اصلی برای اثبات کران‌های پایین مینی‌مکس در فصل‌های آینده خواهند بود.

\subsection{نظریه تصمیم و ریسک مینی‌مکس}
\label{sec:bg:minimax-risk}

فرض کنید مدل آماری ما شامل فضای نمونه $\Xset$ و خانواده‌ای از توزیع‌های احتمال $\mathcal{P} = \{P_\thh : \thh \in \Thh\}$ باشد که توسط فضای پارامتر $\Thh$ اندیس‌گذاری شده‌اند. داده‌های مشاهده شده $X^n = (X_1, \dots, X_n) \in \Xset^n$، متغیرهای تصادفی مستقلی هستند که از توزیع ناشناخته $P_\thh$ نمونه‌برداری شده‌اند.

یک تخمین‌گر\LTRfootnote{Estimator}، تابعی اندازه‌پذیر به صورت $\hat{\thh}: \Xset^n \to \Thh$ است که بر اساس مشاهدات، تقریبی از پارامتر واقعی ارائه می‌دهد. برای سنجش کیفیت این تقریب، از یک متریک (یا شبه‌متریک) $\rho: \Thh \times \Thh \to \mathbb{R}_{\ge 0}$ بر روی فضای پارامتر استفاده می‌کنیم. تابع زیان معمولاً تابعی صعودی از این متریک است:
\begin{equation}
L\left(\hat{\thh}(X^n), \thh\right) = \Phi\left(\rho\left(\hat{\thh}(X^n), \thh\right)\right)
\end{equation}
که در آن $\Phi: \mathbb{R}_{\ge 0} \to \mathbb{R}_{\ge 0}$ تابعی صعودی و غیرمنفی است (مانند $\Phi(t) = t^2$ برای خطای میانگین مربعات).

«ریسک»\LTRfootnote{Risk} یک تخمین‌گر در نقطه $\thh$، برابر با امید ریاضی زیان تحت توزیع $P_\thh$ تعریف می‌شود:
\begin{equation}
R\left(\hat{\thh}, \thh\right) = \Eset_{X^n \sim P_\thh^n} \left[ \Phi\left(\rho\left(\hat{\thh}(X^n), \thh\right)\right) \right]
\end{equation}

در تحلیل مینی‌مکس، ما بدترین عملکرد ممکن یک تخمین‌گر را در سراسر فضای پارامتر در نظر می‌گیریم.

\begin{تعریف}[ریسک مینی‌مکس]
\label{def:minimax-rate}
ریسک مینی‌مکس $\mathfrak{M}_n(\Thh)$ برای خانواده $\mathcal{P}$ و متریک $\rho$، برابر است با کمینه‌ی بیشینه‌ی ریسک:
\begin{equation}
\label{eq:minimax-def}
\mathfrak{M}_n\left(\Thh, \Phi \circ \rho\right) = \inf_{\hat{\thh}} \sup_{\thh \in \Thh} \Eset_{P_\thh^n} \left[ \Phi\left(\rho\left(\hat{\thh}(X^n), \thh\right)\right) \right]
\end{equation}
که در آن اینفیمم بر روی تمامی تخمین‌گرهای اندازه‌پذیر $\hat{\thh}$ گرفته می‌شود و سوپریمم روی تمام پارامترهای عضو خانواده‌ی $\Thh$.
\end{تعریف}

\subsection{تقلیل به آزمون فرض (روش بسته‌بندی)}
\label{sec:bg:hypothesis-testing}

محاسبه مستقیم مقدار \eqref{eq:minimax-def} دشوار است. استراتژی استاندارد ما برای غلبه بر این دشواری، «تبدیل فضای پیوسته پارامتر به یک مجموعه گسسته» است. به عبارت دیگر، مسئله‌ی «تخمین» را به مسئله‌ی «آزمون فرض چندگزینه‌ای» تقلیل می‌دهیم.

این فرآیند بر پایه این شهود استوار است که هر تخمین‌گر دقیق، باید بتواند بین پارامترهایی که فاصله‌ی تقریباً زیادی از هم دارند، تمایز قائل شود. برای رسمی‌سازی این ایده، از مفهوم «بسته‌بندی» استفاده می‌کنیم.

\begin{تعریف}[مجموعه بسته‌بندی\LTRfootnote{Packing Set}]
مجموعه‌ی متناهی $\mathcal{V} = \{\thh_1, \dots, \thh_M\} \subset \Thh$ یک $2\delta$-بسته‌بندی برای فضای $(\Thh, \rho)$ نامیده می‌شود اگر اعضای آن حداقل به اندازه $2\delta$ از یک‌دیگر فاصله داشته باشند:
\begin{equation}
\min_{i \neq j} \rho\left(\thh_i, \thh_j\right) \ge 2\delta
\label{eq:deltapackineq}
\end{equation}
\end{تعریف}

نکته کلیدی در این است که اگر تخمین‌گر $\hat{\thh}$ به پارامتر واقعی $\thh_v$ نزدیک باشد ($\rho(\hat{\thh}, \thh_v) < \delta$)، آنگاه به دلیل نامساوی مثلث، فاصله آن از سایر نقاط شبکه بیش از $\delta$ خواهد بود. بنابراین، وقوع خطای آزمون فرضیه، مستلزم وقوع خطای تخمین بزرگ است.
با استفاده از مفاهیم فاصله که در بخش \ref{sec:bg:f-divergence} دیدیم و ترکیب آن با نامساوی مارکوف، به قضیه‌ی زیر می‌رسیم:

\begin{قضیه}[کران پایین عمومی]
فرض کنید $\{\thh_1, \dots, \thh_M\}$ یک $2\delta$-بسته‌بندی برای $\Thh$ باشد. آنگاه ریسک مینی‌مکس به خطای آزمون فرض محدود می‌شود:
\begin{align}
\mathfrak{M}_n(\Thh) &\ge \Phi(\delta) \cdot \inf_{\hat{\thh}} \sup_{\thh \in \mathcal{V}} \Pr_{P_\thh^n}\left(\rho\left(\hat{\thh}, \thh\right) \ge \delta\right) \\
&\ge \Phi(\delta) \cdot \inf_{\psi} \bar{P}_{err}(\psi) \label{eq:reduction-inequality}
\end{align}
که در آن $\bar{P}_{err}(\psi) = \frac{1}{M} \sum_{v=1}^M P_{\thh_v}^n\left(\psi(X^n) \neq v\right)$ میانگین احتمال خطا روی فرضیه‌های گسسته است و $\psi$ آزمون‌گری است که سعی در بازیافت اندیس $v$ دارد.
\end{قضیه}

\subsection{نامساوی‌های کران پایین}
\label{sec:bg:lower-bounds}

اکنون که مسئله را به آزمون فرض روی $M$ نقطه تقلیل دادیم، برای اثبات کران‌های پایین نهایی نیاز به ابزارهایی داریم که $\bar{P}_{err}$ را از پایین محدود کنند. سه روش اصلی که بر پایه $f$-واگرایی‌ها بنا شده‌اند عبارتند از:

\begin{قضیه}[نامساوی لو کم\LTRfootnote{Le Cam's Inequality}]
\label{thm:le-cam}
این روش معمولاً برای آزمون بین دو توزیع $P_1$ و $P_2$ ($M=2$) استفاده می‌شود و برای «کران‌های محلی» حول یک نقطه مناسب است.
\begin{equation}
\label{eq:le-cam}
\inf_{\psi} \bar{P}_{err}(\psi) \ge \frac{1}{2} \left( 1 - \left\|P_1^n - P_2^n\right\|_{TV} \right)
\end{equation}
\textbf{تفسیر:} این نامساوی بیان می‌کند که اگر فاصله $TV$ بین دو توزیع کم باشد، همپوشانی آن‌ها زیاد است و هیچ آزمون‌گری نمی‌تواند با خطای ناچیز آن‌ها را تفکیک کند.
\end{قضیه}

\begin{قضیه}[نامساوی فانو\LTRfootnote{Fano's Inequality}]
\label{thm:fano}
زمانی که پارامتر مورد نظر متعلق به مجموعه‌ای بزرگ‌تر $\mathcal{V}$ باشد \\ ($M = |V| > 2$)، نامساوی فانو کران پایین قوی‌تری ارائه می‌دهد:
\begin{equation}
\label{eq:fano}
\inf_{\psi} \bar{P}_{err}(\psi) \ge 1 - \frac{I\left(X^n; V\right) + \log 2}{\log M}
\end{equation}
که در آن $I(X^n; V)$ اطلاعات متقابل بین داده‌ها و اندیس پارامتر است.

\textbf{تفسیر:} نامساوی فانو مسئله خطا را به اطلاعات متقابل $I(X^n; V)$ گره می‌زند. اگر داده‌ها حاوی اطلاعات کافی درباره اندیس واقعی $V$ نباشند (ظرفیت کانال نسبت به تعداد فرضیه‌ها $\log M$ کم باشد)، خطا اجتناب‌ناپذیر است.
\end{قضیه}

\begin{لم}[لم اسود\LTRfootnote{Assouad's Lemma}]
\label{lem:assouad}
این لم ابزاری قدرتمند برای فضاهای پارامتر با ابعاد بالا (مانند $\{-1, 1\}^d$) است. قدرت لم اسود در شکستن یک مسئله $d$-بعدی دشوار به $d$ مسئله ۱-بعدی مستقل است.
\begin{equation}
\label{eq:assouad}
\mathfrak{M}_n(\Thh) \ge \frac{\delta}{2} \sum_{j=1}^d \left[ 1 - \left\|M_{+j}^n - M_{-j}^n\right\|_{TV} \right]
\end{equation}
که در آن $M_{+j}^n$ و $M_{-j}^n$ توزیع‌های مخلوط حاشیه‌ای هستند (میانگین توزیع‌هایی که بیت $j$ام آن‌ها به ترتیب $+1$ و $-1$ است). اگر در هر بُعد تمایز قائل شدن سخت باشد، مجموع خطاها با تعداد ابعاد $d$ جمع شده و کران محکمی می‌سازد.
\end{لم}