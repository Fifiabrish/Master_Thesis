\section{مبانی آماری و نظریه اطلاعات}
\label{sec:bg:statistical-foundations}

در بخش‌های پیشین، ابزارهای سنجش فاصله بین توزیع‌ها (مانند $f$-واگرایی‌ها) را معرفی کردیم. در این بخش، به معرفی چارچوب آماری می‌پردازیم که در آن از این ابزارها برای تحلیل حدود پایین خطا در حضور محدودیت‌های محرمانگی استفاده می‌شود.
این تعاریف و قضایا عمدتاً بر اساس چارچوب ارائه‌شده در \cite{Duchi2013} تدوین شده‌اند.

\subsection{معیارهای فاصله اطلاعاتی}
\label{sec:bg:info-metrics}

برای دو توزیع احتمال $P$ و $Q$ که روی فضای $\Xset$ تعریف شده‌اند و نسبت به یک اندازه‌ی پایه $\mu$ مطلقاً پیوسته هستند (با توابع چگالی $p$ و $q$)، معیارهای زیر را تعریف می‌کنیم:

\begin{تعریف}[واگرایی کولبک-لایبلر]
\label{def:kl-divergence}
واگرایی کولبک-لایبلر (\lr{KL}) بین دو توزیع $P$ و $Q$ به صورت زیر تعریف می‌شود:
\begin{equation}
\label{eq:kl-def}
D_{KL}(P || Q) = \int_{\Xset} p(x) \log \frac{p(x)}{q(x)} d\mu(x)
\end{equation}
\end{تعریف}

\begin{تعریف}[فاصله‌ی واریانس کل]
\label{def:tv-distance}
فاصله‌ی واریانس کل\LTRfootnote{Total Variation Distance} بین دو توزیع $P$ و $Q$ به صورت زیر تعریف می‌شود:
\begin{equation}
\label{eq:tv-def}
\|P - Q\|_{TV} = \sup_{S \in \sigma(\Xset)} |P(S) - Q(S)|
= \frac{1}{2} \int_{\Xset} |p(x) - q(x)| d\mu(x)
\end{equation}
\end{تعریف}

\begin{تعریف}[اطلاعات متقابل]
\label{def:mutual-info}
اگر $X$ و $V$ دو متغیر تصادفی باشند، اطلاعات متقابل\LTRfootnote{Mutual Information} بین آن‌ها به صورت امید ریاضی واگرایی \lr{KL} بین توزیع شرطی و توزیع حاشیه‌ای تعریف می‌شود:
\begin{equation}
\label{eq:mutual-info}
I(X; V) = D_{KL}(P_{X,V} || P_X \otimes P_V) = \mathbb{E}_{V} \left[ D_{KL}(P_{X|V} || P_X) \right]
\end{equation}
این معیار نقش کلیدی در نامساوی فانو (که در ادامه می‌آید) ایفا می‌کند.
\end{تعریف}

\subsection{ریسک مینیماکس}
\label{sec:bg:minimax-risk}

در نظریه تصمیم آماری، هدف تخمین یک پارامتر $\theta(P)$ از یک توزیع ناشناخته $P \in \mathcal{P}$ است.
اگر $\hat{\theta}$ یک تخمین‌گر باشد که تابعی از داده‌های مشاهده شده (مانند $Z_1, \dots, Z_n$) است، کیفیت آن با استفاده از یک تابع زیان صعودی $\Phi \circ \rho$ سنجیده می‌شود (که $\rho$ یک شبه‌متر روی فضای پارامتر است).

نرخ مینیماکس\LTRfootnote{Minimax Rate}، کمترین خطای ممکنی است که یک تخمین‌گر در بدترین سناریو (بدترین توزیع $P$ در کلاس $\mathcal{P}$) متحمل می‌شود.

\begin{تعریف}[نرخ مینیماکس]
\label{def:minimax-rate}
برای یک کلاس از توزیع‌ها $\mathcal{P}$ و پارامتر $\theta$، نرخ مینیماکس $\mathfrak{M}_n$ به صورت زیر تعریف می‌شود:
\begin{equation}
\label{eq:minimax-def}
\mathfrak{M}_n(\theta(\mathcal{P}), \Phi \circ \rho) = \inf_{\hat{\theta}} \sup_{P \in \mathcal{P}} \mathbb{E}_{P} [\Phi(\rho(\hat{\theta}(Z^n), \theta(P)))]
\end{equation}
که در آن اینفیمم روی تمام تخمین‌گرهای ممکن $\hat{\theta}$ گرفته می‌شود.
\end{تعریف}

در حالتی که محدودیت محرمانگی تفاضلی موضعی با پارامتر $\alpha$ وجود داشته باشد، نرخ مینیماکس خصوصی 
\lr{($\alpha$-Private Minimax Rate)}
 با در نظر گرفتن اینفیمم روی تمام مکانیزم‌های کانال $Q$ که شرط \LDP را برآورده می‌کنند، تعریف می‌شود \cite{Duchi2013}.

\section{آزمون فرض آماری و روش تقلیل}
\label{sec:bg:hypothesis-testing}

برای اثبات حدود پایین نرخ‌های مینیماکس، روش استاندارد این است که مسئله‌ی تخمین پارامتر را به یک مسئله‌ی آزمون فرض\LTRfootnote{Hypothesis Testing} تقلیل دهیم.
ایده اصلی این است: اگر نتوانیم بین چند مقدار گسسته از پارامتر با دقت بالا تمایز قائل شویم، قطعاً نمی‌توانیم پارامتر را در فضای پیوسته با خطای کم تخمین بزنیم.

\subsection{آزمون فرض دودویی}
\label{sec:bg:binary-testing}

ساده‌ترین حالت آزمون فرض، تصمیم‌گیری بین دو توزیع احتمال $P_0$ و $P_1$ است.
فرض کنید داده‌ی مشاهده شده $Z$ از یکی از این دو توزیع تولید شده است. ما دو فرض داریم:
\begin{itemize}
    \item فرض صفر ($H_0$): $Z \sim P_0$
    \item فرض مقابل ($H_1$): $Z \sim P_1$
\end{itemize}

یک آزمون (یا تابع تست) $\psi: \Zset \to \{0, 1\}$ تابعی است که بر اساس داده‌ی مشاهده شده، حدس می‌زند کدام فرض صحیح است.
خطای این آزمون به صورت مجموع احتمال خطای نوع اول و دوم تعریف می‌شود:
\begin{equation}
\label{eq:testing-error}
P_{err}(\psi) = \mathrm{Pr}_{H_0}(\psi(Z) = 1) + \mathrm{Pr}_{H_1}(\psi(Z) = 0)
\end{equation}

لم نیمن-پیرسون\LTRfootnote{Neyman-Pearson Lemma} نشان می‌دهد که کمترین خطای ممکن برای هر آزمون دودویی، مستقیماً با فاصله‌ی واریانس کل ($d_{TV}$) بین دو توزیع ارتباط دارد:
\begin{equation}
\label{eq:binary-min-error}
\inf_{\psi} P_{err}(\psi) = 1 - \|P_0 - P_1\|_{TV}
\end{equation}
این رابطه نشان می‌دهد که هرچه دو توزیع $P_0$ و $P_1$ به هم شبیه‌تر باشند (فاصله‌ی \lr{TV} کمتر)، احتمال خطا بیشتر شده و به ۱ (حدس تصادفی) نزدیک‌تر می‌شود.
در فضای \LDP، نویز اضافه شده باعث کاهش شدید فاصله‌ی \lr{TV} و در نتیجه افزایش خطای آزمون می‌شود.

\subsection{تقلیل تخمین به آزمون (روش بسته‌بندی)}
\label{sec:bg:reduction-packing}

برای استفاده از ابزارهای آزمون فرض در مسئله‌ی تخمین نرخ مینیماکس (معادله \ref{eq:minimax-def})، از تکنیک گسسته‌سازی فضای پارامتر $\Theta$ استفاده می‌کنیم.
این روش شامل مراحل زیر است:

\begin{enumerate}
    \item \textbf{ساخت مجموعه‌ی بسته‌بندی\LTRfootnote{Packing Set}:}
    مجموعه‌ای متناهی از پارامترها $\mathcal{V} = \{\theta_1, \dots, \theta_M\} \subset \Theta$ را انتخاب می‌کنیم به طوری که از یکدیگر فاصله‌ی معناداری داشته باشند.
    به طور دقیق‌تر، اگر $\rho$ متریک خطا باشد، برای هر $i \neq j$ باید داشته باشیم $\rho(\theta_i, \theta_j) \ge 2\delta$.
    
    \item \textbf{تعریف مسئله‌ی آزمون:}
    فرض می‌کنیم طبیعت\LTRfootnote{Nature} یک اندیس $V$ را به صورت تصادفی و یکنواخت از مجموعه $\{1, \dots, M\}$ انتخاب می‌کند و داده‌ها بر اساس توزیع $P_{\theta_V}$ تولید می‌شوند.
    هدف، یافتن $V$ بر اساس داده‌های مشاهده شده است.
    
    \item \textbf{ارتباط خطاها:}
    اگر یک تخمین‌گر $\hat{\theta}$ وجود داشته باشد که خطای تخمین آن با احتمال بالا کمتر از $\delta$ باشد، می‌توانیم از آن برای حل مسئله‌ی آزمون فرض استفاده کنیم (با انتخاب نزدیک‌ترین $\theta_i$ به $\hat{\theta}$).
    بنابراین، کران پایین روی خطای آزمون فرض، یک کران پایین برای خطای تخمین ایجاد می‌کند:
    \begin{equation}
    \label{eq:reduction-inequality}
    \mathfrak{M}_n(\theta(\mathcal{P})) \ge \Phi(\delta) \cdot \inf_{\psi} \mathrm{Pr}(\psi(Z^n) \neq V)
    \end{equation}
\end{enumerate}

\subsection{نامساوی‌های کران پایین}
\label{sec:bg:lower-bounds}

برای اثبات کران‌های پایین، سه روش اصلی که بر پایه $f$-واگرایی‌ها بنا شده‌اند را معرفی می‌کنیم:

\begin{قضیه}[نامساوی لو کم\LTRfootnote{Le Cam's Inequality}]
\label{thm:le-cam}
این روش برای آزمون بین دو توزیع $P_1$ و $P_2$ استفاده می‌شود.
کمینه احتمال خطا با استفاده از فاصله‌ی واریانس کل (رابطه \ref{eq:tv-def}) کران‌دار می‌شود:
\begin{equation}
\label{eq:le-cam}
\inf_{\psi} \mathrm{Pr}(\psi(Z^n) \neq V) \ge \frac{1}{2} \left( 1 - \|P_1^n - P_2^n\|_{TV} \right)
\end{equation}
این روش زمانی مفید است که مسئله را به تشخیص بین دو حالت ساده تقلیل دهیم.
\end{قضیه}

\begin{قضیه}[نامساوی فانو\LTRfootnote{Fano's Inequality}]
\label{thm:fano}
زمانی که پارامتر مورد نظر متعلق به مجموعه‌ای بزرگتر $\mathcal{V}$ باشد (تعداد فرضیه‌ها $|\mathcal{V}| > 2$)، نامساوی فانو کران پایین قوی‌تری ارائه می‌دهد که مبتنی بر اطلاعات متقابل است:
\begin{equation}
\label{eq:fano}
\inf_{\psi} \mathrm{Pr}(\psi(Z^n) \neq V) \ge 1 - \frac{I(Z^n; V) + \log 2}{\log |\mathcal{V}|}
\end{equation}
که در آن $V$ متغیر تصادفی یکنواخت روی مجموعه اندیس‌ها $\mathcal{V}$ است.
\end{قضیه}

\begin{لم}[لم اسود\LTRfootnote{Assouad's Lemma}]
\label{lem:assouad}
این لم مسئله تخمین را به چندین آزمون فرض دودویی مستقل روی مختصات یک ابرمکعب $\{-1, 1\}^d$ تبدیل می‌کند.
نسخه دقیق‌تر آن که در \cite{Duchi2013} استفاده شده است، کران پایین را بر اساس فاصله‌ی واریانس کل توزیع‌های مخلوط حاشیه‌ای بیان می‌کند:
\begin{equation}
\label{eq:assouad}
\mathfrak{M}_n(\theta(\mathcal{P})) \ge \delta \sum_{j=1}^d \left[ 1 - \|M_{+j}^n - M_{-j}^n\|_{TV} \right]
\end{equation}
که در آن $M_{+j}^n$ و $M_{-j}^n$ توزیع‌های حاشیه‌ای مخلوط روی مقادیر $+1$ و $-1$ در بُعد $j$-ام هستند.
\end{لم}