
\section{ریسک مینیماکس و روش‌های کران پایین}
\label{sec:bg:minimax}

در بخش‌های پیشین، ابزارهای سنجش فاصله بین توزیع‌ها را معرفی کردیم. در این بخش، به معرفی چارچوب آماری می‌پردازیم که در آن از این ابزارها برای تحلیل حدود پایین خطا در حضور محدودیت‌های حریم خصوصی استفاده می‌شود. تعاریف و قضایای این بخش عمدتاً بر اساس چارچوب ارائه‌شده در \cite{duchi2013local} هستند.

\subsection{اطلاعات متقابل}
\label{sec:bg:mutual-info}

یکی دیگر از مفاهیم کلیدی نظریه اطلاعات که ارتباط تنگاتنگی با واگرایی \lr{KL} دارد، اطلاعات متقابل است.

\begin{تعریف}[اطلاعات متقابل]
\label{def:mutual-info}
اگر $X$ و $V$ دو متغیر تصادفی باشند، اطلاعات متقابل\LTRfootnote{Mutual Information} بین آن‌ها به صورت امید ریاضی واگرایی \lr{KL} بین توزیع شرطی و توزیع حاشیه‌ای تعریف می‌شود:
\begin{equation}
\label{eq:mutual-info}
I(X; V) = D_{KL}(P_{X,V} || P_X \otimes P_V) = \mathbb{E}_{V} \left[ D_{KL}(P_{X|V} || P_X) \right]
\end{equation}
\end{تعریف}

این معیار در نامساوی فانو (که در ادامه می‌آید) نقش کلیدی ایفا می‌کند.

\subsection{ریسک مینیماکس}
\label{sec:bg:minimax-risk}

در نظریه تصمیم آماری، هدف تخمین یک پارامتر $\theta(P)$ از یک توزیع ناشناخته $P \in \mathcal{P}$ است. اگر $\hat{\theta}$ یک تخمین‌گر باشد که تابعی از داده‌های مشاهده شده (مانند $Z_1, \dots, Z_n$) است، کیفیت آن با استفاده از یک تابع زیان صعودی $\Phi \circ \rho$ سنجیده می‌شود (که $\rho$ یک شبه‌متر روی فضای پارامتر است).

نرخ مینیماکس\LTRfootnote{Minimax Rate}، کمترین خطای ممکنی است که یک تخمین‌گر در بدترین سناریو (بدترین توزیع $P$ در کلاس $\mathcal{P}$) متحمل می‌شود.

\begin{تعریف}[نرخ مینیماکس]
\label{def:minimax-rate}
برای یک کلاس از توزیع‌ها $\mathcal{P}$ و پارامتر $\theta$، نرخ مینیماکس $\mathfrak{M}_n$ به صورت زیر تعریف می‌شود:
\begin{equation}
\label{eq:minimax-def}
\mathfrak{M}_n(\theta(\mathcal{P}), \Phi \circ \rho) = \inf_{\hat{\theta}} \sup_{P \in \mathcal{P}} \mathbb{E}_{P} [\Phi(\rho(\hat{\theta}(Z^n), \theta(P)))]
\end{equation}
که در آن اینفیمم روی تمام تخمین‌گرهای ممکن $\hat{\theta}$ گرفته می‌شود.
\end{تعریف}

در حالتی که محدودیت حریم خصوصی تفاضلی محلی با پارامتر $\alpha$ وجود داشته باشد، نرخ مینیماکس خصوصی ($\alpha$-Private Minimax Rate) با در نظر گرفتن اینفیمم روی تمام مکانیزم‌های کانال $Q$ که شرط \LDP را برآورده می‌کنند، تعریف می‌شود \cite{duchi2013local}.

\subsection{نامساوی‌های کران پایین}
\label{sec:bg:lower-bounds}

برای اثبات کران‌های پایین روی نرخ مینیماکس، معمولاً مسئله‌ی تخمین به یک مسئله‌ی آزمون فرض\LTRfootnote{Hypothesis Testing} چندگانه تقلیل داده می‌شود. در اینجا سه روش اصلی که بر پایه \f-واگرایی‌ها بنا شده‌اند را معرفی می‌کنیم:

\begin{قضیه}[نامساوی لو کم\LTRfootnote{Le Cam's Inequality}]
\label{thm:le-cam}
این روش برای آزمون بین دو توزیع $P_1$ و $P_2$ استفاده می‌شود. کمینه احتمال خطا با استفاده از فاصله‌ی واریانس کل (رابطه \ref{eq:tv-dist}) کران‌دار می‌شود:
\begin{equation}
\inf_{\psi} \mathrm{Pr}(\psi(Z^n) \neq V) \ge \frac{1}{2} \left( 1 - \|P_1^n - P_2^n\|_{TV} \right)
\end{equation}
این روش زمانی مفید است که مسئله را به تشخیص بین دو حالت ساده تقلیل دهیم.
\end{قضیه}

\begin{قضیه}[نامساوی فانو\LTRfootnote{Fano's Inequality}]
\label{thm:fano}
زمانی که پارامتر مورد نظر متعلق به مجموعه‌ای بزرگتر $\mathcal{V}$ باشد (تعداد فرضیه‌ها $|\mathcal{V}| > 2$)، نامساوی فانو کران پایین قوی‌تری ارائه می‌دهد که مبتنی بر اطلاعات متقابل است:
\begin{equation}
\inf_{\psi} \mathrm{Pr}(\psi(Z^n) \neq V) \ge 1 - \frac{I(Z^n; V) + \log 2}{\log |\mathcal{V}|}
\end{equation}
که در آن $V$ متغیر تصادفی یکنواخت روی مجموعه اندیس‌ها $\mathcal{V}$ است.
\end{قضیه}

\begin{لم}[لم اسود\LTRfootnote{Assouad's Lemma}]
\label{lem:assouad}
این لم مسئله تخمین را به چندین آزمون فرض دودویی مستقل روی مختصات یک ابرمکعب $\{-1, 1\}^d$ تبدیل می‌کند. نسخه دقیق‌تر آن که در \cite{duchi2013local} استفاده شده است، کران پایین را بر اساس فاصله‌ی واریانس کل توزیع‌های مخلوط حاشیه‌ای بیان می‌کند:
\begin{equation}
\mathfrak{M}_n(\theta(\mathcal{P})) \ge \delta \sum_{j=1}^d \left[ 1 - \|M_{+j}^n - M_{-j}^n\|_{TV} \right]
\end{equation}
که در آن $M_{+j}^n$ و $M_{-j}^n$ توزیع‌های حاشیه‌ای مخلوط روی مقادیر $+1$ و $-1$ در بُعد $j$-ام هستند.
\end{لم}