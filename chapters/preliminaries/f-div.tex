\section{$f$-واگرایی‌ها}
\label{sec:bg:f-divergence}
در بخش‌های پیشین، مکانیزم‌های محرمانگی تفاضلی را ابزاری برای ایجاد «شباهت آماری» بین خروجی‌های دو پایگاه‌داده‌ی همسایه معرفی کردیم. برای کمّی‌سازی دقیق این شباهت و اثبات کران‌های پایین در فصل‌های آینده، نیازمند معیاری هستیم که فاصله میان توزیع‌های احتمالی را در یک چارچوب عمومی بسنجد. این ابزار، خانواده‌ی $f$-واگرایی‌ها\LTRfootnote{$f$-divergences} است که نخستین بار توسط سیسر \cite{csiszar1963information} و علی و سیلوِی \cite{Ali1966Silvey} معرفی شد.

\subsection{تعریف رسمی در فضای اندازه‌پذیر}
\label{sec:bg:f-div-def}

برای ارائه تعریفی دقیق و مستقل از نوع متغیرهای تصادفی (پیوسته یا گسسته)، از زبان نظریه اندازه استفاده می‌کنیم.
فرض کنید $(\Omega, \mathcal{F})$ یک فضای اندازه‌پذیر\LTRfootnote{Measurable Space} باشد و $P$ و $Q$ دو «اندازه احتمال»\LTRfootnote{Probability Measures} تعریف شده روی این فضا باشند.

پیش از تعریف واگرایی، مفهوم «پیوستگی مطلق» که شرط وجود چگالی نسبت است را یادآوری می‌کنیم.

\begin{تعریف}[پیوستگی مطلق\LTRfootnote{Absolute Continuity}]
می‌گوییم اندازه $P$ نسبت به $Q$ مطلقاً پیوسته است و می‌نویسیم $P \ll Q$، اگر برای هر مجموعه اندازه‌پذیر $A \in \mathcal{F}$:
\begin{equation}
Q(A) = 0 \implies P(A) = 0
\end{equation}
این شرط تضمین می‌کند که هر رویدادی که تحت توزیع $Q$ غیرممکن باشد، تحت $P$ نیز غیرممکن است.
\end{تعریف}

اگر $P \ll Q$ باشد، بنابر قضیه رادون-نیکودیم\LTRfootnote{Radon-Nikodym Theorem}، تابعی اندازه‌پذیر و غیرمنفی روی $\Omega$ وجود دارد که «مشتق رادون-نیکودیم» $P$ نسبت به $Q$ نامیده می‌شود و با $\frac{dP}{dQ}$ نمایش داده می‌شود. این مشتق نقش همان نسبت درست‌نمایی\LTRfootnote{Likelihood Ratio} را در حالت کلی ایفا می‌کند.

\begin{تعریف}[$f$-واگرایی]
\label{def:f-divergence}
فرض کنید $P$ و $Q$ دو اندازه احتمال روی $(\Omega, \mathcal{F})$ باشند به طوری که $P \ll Q$.
برای هر تابع محدب\LTRfootnote{Convex} $f: (0, \infty) \to \mathbb{R}$ با این شرط که $f(1)=0$، $f$-واگراییِ $P$ از $Q$ به صورت امید ریاضی تابع $f$ بر روی مشتق رادون-نیکودیم (تحت اندازه $Q$) تعریف می‌شود:
\begin{equation}
\label{eq:f-divergence-measure}
D_f(P \| Q) \triangleq \int_{\Omega} f\left( \frac{dP}{dQ}(\omega) \right) dQ(\omega)
\end{equation}
یا به بیانی دیگر با استفاده از نماد امید ریاضی:
\begin{equation}
D_f(P \| Q) = \Eset_Q \left[ f\left( \frac{dP}{dQ} \right) \right]
\end{equation}
\end{تعریف}

\textbf{تفسیر اجزاء:}
\begin{itemize}
    \item \textbf{تابع مولد $f$:} تابع $f$ تعیین‌کننده نوع هندسه و خواص واگرایی است. تحدب $f$ شرطی حیاتی برای خوش‌رفتاری ریاضی این اندازه است.
    \item \textbf{غیرمنفی بودن:} با استفاده از نامساوی ینسن\LTRfootnote{Jensen's Inequality} و شرط محدب بودن $f$، می‌توان نشان داد که واگرایی همواره نامنفی است:
    \begin{equation}
    D_f(P \| Q) = \Eset_Q \left[ f\left( \frac{dP}{dQ} \right) \right] \ge f\left( \Eset_Q \left[ \frac{dP}{dQ} \right] \right) = f(1) = 0
    \end{equation}
    تساوی $D_f(P\|Q) = 0$ برقرار است اگر و تنها اگر $P=Q$ (در صورت اکیداً محدب بودن $f$).
\end{itemize}

در حالت‌های خاص که فضای نمونه $\Omega$ گسسته یا پیوسته (اقلیدسی) باشد و چگالی‌های $p$ و $q$ نسبت به یک اندازه پایه (مانند شمارشی یا لبگ) وجود داشته باشند، مشتق رادون-نیکودیم به نسبت معمولی چگالی‌ها $\frac{p(x)}{q(x)}$ تبدیل می‌شود و تعریف انتگرالی بالا به فرم‌های آشنای زیر تقلیل می‌یابد:
\begin{equation}
D_f(P \| Q) = \int_{\mathcal{X}} q(x) f\left( \frac{p(x)}{q(x)} \right) dx \quad \text{یا} \quad \sum_{x \in \mathcal{X}} q(x) f\left( \frac{p(x)}{q(x)} \right)
\end{equation}
\subsection{نمونه‌های مهم و توابع مولد}
\label{sec:bg:f-div-examples}

با انتخاب‌های متفاوت برای تابع مولد محدب $f(t)$، می‌توان طیف وسیعی از اندازه‌های فاصله را تولید کرد. در ادامه، مهم‌ترین نمونه‌ها را معرفی می‌کنیم.
در تعاریف زیر، فرض می‌کنیم $P$ و $Q$ دو اندازه احتمال باشند که دارای چگالی (یا جرم) احتمال $p(x)$ و $q(x)$ نسبت به یک اندازه پایه هستند.

\begin{itemize}
    \item \textbf{فاصله تغییرات کل\LTRfootnote{Total Variation (TV)}:}
    این فاصله، شهودی‌ترین متریک برای سنجش تمایزپذیری دو توزیع است و بیان‌گر بیش‌ترین تفاوت احتمالی است که دو توزیع می‌توانند روی یک پیشامد داشته باشند. تابع مولد آن $f(t) = \frac{1}{2}|t-1|$ است.
    \begin{align}
    \mathrm{TV}(P, Q) &= \frac{1}{2} \int_{\Xset} |p(x) - q(x)| dx \\
    &= \sup_{A \in \mathcal{F}} |P(A) - Q(A)|
    \end{align}

    \item \textbf{واگرایی کولبک-لایبلر\LTRfootnote{Kullback-Leibler (KL)}:}
    معروف‌ترین واگرایی در نظریه اطلاعات که آنتروپی نسبی\LTRfootnote{Relative Entropy} نیز نامیده می‌شود. این معیار نامتقارن است و نقش اساسی در فشرده‌سازی داده‌ها و استنتاج بیزی دارد. تابع مولد آن $f(t) = t \ln t$ است.
    \begin{equation}
    \mathrm{KL}(P \| Q) = \int_{\Xset} p(x) \ln \frac{p(x)}{q(x)} dx
    \end{equation}


    \item \textbf{اطلاعات متقابل}
    \label{def:mutual-info}
    اگر $X$ و $V$ دو متغیر تصادفی باشند، اطلاعات متقابل\LTRfootnote{Mutual Information} بین آن‌ها به صورت امید ریاضی واگرایی \lr{KL} بین توزیع شرطی و توزیع حاشیه‌ای تعریف می‌شود:
    \begin{equation}
    \label{eq:mutual-info}
    I(X; V) = D_{KL}(P_{X,V} || P_X \otimes P_V) = \mathbb{E}_{V} \left[ D_{KL}(P_{X|V} || P_X) \right]
    \end{equation}
    این معیار نقش کلیدی در نامساوی فانو (که در بخش بعد می‌بینیم) ایفا می‌کند. 

    \item \textbf{واگرایی کای-دو\LTRfootnote{Chi-Squared ($\chi^2$)}:}
    تابع مولد آن $f(t) = (t-1)^2$ است. این واگرایی اغلب برای تقریب‌زنی سایر فواصل (مانند \lr{KL}) در همسایگی‌های کوچک استفاده می‌شود و محاسبه آن ساده‌تر است.
    \begin{equation}
    \chi^2(P \| Q) = \int_{\Xset} \frac{(p(x) - q(x))^2}{q(x)} dx
    \end{equation}

    \item \textbf{فاصله هلینگر-دو\LTRfootnote{Squared Hellinger}:}
    تابع مولد آن $f(t) = (\sqrt{t}-1)^2$ است. این فاصله به دلیل خواص ریاضی خوش‌رفتار (مانند متریک بودن و کران‌دار بودن)، در نظریه برآورد\LTRfootnote{Estimation Theory} و استخراج کران‌های مینیماکس (مانند روش \lr{Le Cam}) کاربرد فراوان دارد.
    \begin{equation}
    H^2(P, Q) = \int_{\Xset} (\sqrt{p(x)} - \sqrt{q(x)})^2 dx
    \end{equation}
\item \textbf{واگرایی $E_\gamma$ (یا واگرایی هاکی-استیک\LTRfootnote{Hockey-Stick Divergence}):}
    این واگرایی ابزاری کلیدی در تحلیل‌های مدرن حریم خصوصی و آزمون‌های فرضیه است. تابع مولد آن برای پارامتر $\gamma \ge 1$ به صورت $f_\gamma(t) = [t - \gamma]_+ = \max\{0, t - \gamma\}$ است.
    
    \textbf{تعریف و صورت‌های معادل:}
    تعریف اصلی بر اساس انتگرالِ جرمِ اضافی نسبت درست‌نمایی است، اما صورت‌های معادل زیر بینش عملیاتی‌تری ارائه می‌دهند:
    \begin{subequations}
    \begin{align}
    E_\gamma(P \| Q) &= \int_{\Xset} \max\{0, p(x) - \gamma q(x)\} dx \\
    &= \sup_{A \subseteq \Xset} (P(A) - \gamma Q(A)) \label{eq:egamma-sup} \\
    &= P(Z > \gamma) - \gamma Q(Z > \gamma) \quad (\text{که } Z = \frac{p(x)}{q(x)}) \label{eq:egamma-prob}
    \end{align}
    \end{subequations}
    رابطه \eqref{eq:egamma-sup} نشان می‌دهد که این واگرایی بیانگر بیشینه‌ی تفاضل وزن‌دار احتمالات است که مستقیماً با موازنه خطای نوع اول و دوم در آزمون فرضیه مرتبط است.
    
    \textbf{نکات تحلیلی و تاریخی:}
    نام‌گذاری توصیفی این واگرایی به «هاکی-استیک» که نخستین بار توسط ساسون و وِردو \cite{sason2016f} پیشنهاد شد، برخاسته از شکل هندسی نمودار تابع مولد $f(t)$ است (تخت بودن تا $\gamma$ شبیه تیغه، و صعود خطی پس از آن شبیه دسته چوب هاکی). نمادگذاری $E_\gamma$ و تدوین نقش بنیادی آن، پیش‌تر توسط پولیانسکی و همکاران \cite{polyanskiy2010channel} صورت گرفته بود.
    
    \textbf{کاربرد در حریم خصوصی:}
    شرط محرمانگی تفاضلی تقریبی (\DP[(\eps, \del)]) دقیقاً معادل است با اینکه برای هر دو دیتابیس همسایه، واگرایی هاکی-استیک خروجی‌ها از مقدار $\delta$ تجاوز نکند: $E_{e^\eps}(P \| Q) \le \delta$.

    \item \textbf{واگرایی رنی\LTRfootnote{Rényi Divergence}:}
    برای پارامتر $\alpha \in (1, \infty)$، این واگرایی به صورت زیر تعریف می‌شود:
    \begin{equation}
    D_\alpha(P \| Q) = \frac{1}{\alpha - 1} \ln \int_{\Xset} p(x)^\alpha q(x)^{1-\alpha} dx
    \end{equation}
    این واگرایی پلی میان واگرایی \lr{KL} (در حد $\alpha \to 1$) و واگرایی ماکزیمم (در حد $\alpha \to \infty$) است. اگرچه فرم لگاریتمی دارد، اما تبدیلی یک‌نوا از «واگرایی سالیس»\LTRfootnote{Tsallis Divergence} است که خود یک $f$-واگرایی می‌باشد.

    \item \textbf{واگرایی ماکزیمم ($D_\infty$):}
    این واگرایی متناظر با بدترین نسبت درست‌نمایی نقطه‌ای است و به عنوان حدِ واگرایی رنی به دست می‌آید:
    \begin{equation}
    D_\infty(P \| Q) = \lim_{\alpha \to \infty} D_\alpha(P \| Q) = \sup_{x \in \Xset} \ln \frac{p(x)}{q(x)}
    \end{equation}
    \textbf{کاربرد در حریم خصوصی:}
    شرط \DP (خالص) دقیقاً معادل است با کران‌دار بودن این واگرایی توسط بودجه حریم خصوصی: $D_\infty(P \| Q) \le \eps$.

\end{itemize}


\subsection{خواص بنیادین و روابط بین $f$-واگرایی‌ها}
\label{sec:bg:f-div-properties}

خانواده‌ی $f$-واگرایی‌ها تنها مجموعه‌ای از فرمول‌های انتگرالی نیستند، بلکه دارای خواص ساختاری عمیقی هستند که آن‌ها را برای تحلیل سیستم‌های اطلاعاتی و حریم خصوصی ایده‌آل می‌سازد. در این بخش، سه ویژگی حیاتی این معیارها را بررسی می‌کنیم.

\subsubsection{نامساوی پردازش داده (DPI)}
مهم‌ترین ویژگی $f$-واگرایی‌ها در نظریه اطلاعات، خاصیت یک‌نواختی\LTRfootnote{Monotonicity} آن‌ها تحت پردازش است. این ویژگی بیان می‌کند که هیچ عملیات پردازشی روی داده‌ها (اعم از قطعی یا تصادفی) نمی‌تواند تمایزپذیری بین دو توزیع را افزایش دهد.

\begin{قضیه}[نامساوی پردازش داده\LTRfootnote{Data Processing Inequality (DPI)}]
فرض کنید $P$ و $Q$ دو توزیع احتمال روی فضای $\Xset$ باشند و $\kappa: \Xset \to \Yset$ یک هسته‌ی احتمالاتی (کانال مارکوف)\LTRfootnote{Markov Kernel / Probability Kernel} باشد که داده‌ها را از فضای $\Xset$ به $\Yset$ نگاشت می‌کند. اگر $P\kappa$ و $Q\kappa$ توزیع‌های خروجی پس از اعمال کرنل باشند، آنگاه برای هر $f$-واگرایی داریم:
\begin{equation}
D_f(P\kappa \| Q\kappa) \le D_f(P \| Q)
\end{equation}
\end{قضیه}

\textbf{تفسیر در حریم خصوصی:} این قضیه تضمین می‌کند که اگر یک مهاجم نتواند دو دیتابیس را بر اساس خروجی مکانیزم از هم تشخیص دهد، با انجام هیچ‌گونه پس‌پردازشی\LTRfootnote{Post-processing} روی آن خروجی نیز قادر به بهبود توان تشخیص خود نخواهد بود. به عبارت دیگر، اطلاعات (و حریم خصوصی) با پردازش بیش‌تر، «خلق» یا «تخریب» نمی‌شود.

\subsubsection{تحدب مشترک}
تابع $f$-واگرایی نسبت به جفت توزیع‌های ورودی خود، محدب است.
\begin{قضیه}[تحدب مشترک\LTRfootnote{Joint Convexity}]
نگاشت $(P, Q) \mapsto D_f(P \| Q)$ یک تابع محدب مشترک است. یعنی برای هر $\lambda \in [0, 1]$ و توزیع‌های $P_1, P_2, Q_1, Q_2$:
\begin{equation}
D_f(\lambda P_1 + (1-\lambda)P_2 \| \lambda Q_1 + (1-\lambda)Q_2) \le \lambda D_f(P_1 \| Q_1) + (1-\lambda) D_f(P_2 \| Q_2)
\end{equation}
\end{قضیه}
این ویژگی در تحلیل مکانیزم‌هایی که ترکیبی از چند مکانیزم ساده‌تر هستند، بسیار کاربرد دارد.

\subsubsection{روابط بین واگرایی‌ها}
اگرچه انتخاب‌های مختلف $f$ معیارهای متفاوتی تولید می‌کنند، اما این معیارها مستقل نیستند و می‌توان آن‌ها را با یکدیگر کران‌دار کرد.

\begin{itemize}
    \item \textbf{نامساوی پینسکر\LTRfootnote{Pinsker's Inequality}:}
    این نامساوی مشهور، ارتباط هندسه (فاصله تغییرات کل) و اطلاعات (واگرایی \lr{KL}) را برقرار می‌کند و نشان می‌دهد که همگرایی در آنتروپی نسبی، همگرایی در $L_1$ را تضمین می‌کند:
    \begin{equation}
    \label{eq:pinsker}
    \mathrm{TV}(P, Q) \le \sqrt{\frac{1}{2} \mathrm{KL}(P \| Q)}
    \end{equation}
    
    \item \textbf{رابطه هاکی-استیک و \lr{TV}:}
    می‌توان به سادگی دید واگرایی هاکی-استیک تعمیمی از فاصله تغییرات کل است. به طور مشخص، در نقطه $\gamma=1$ این دو معیار بر هم منطبق می‌شوند:
    \begin{equation}
    E_1(P \| Q) = \mathrm{TV}(P, Q)
    \end{equation}
    این تساوی پل ارتباطی مهمی بین تعاریف \DP[(\eps,\del)] و تحلیل‌های مبتنی بر فاصله تغییرات کل فراهم می‌کند.
\end{itemize}