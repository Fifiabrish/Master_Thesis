\section{$f$-واگرایی‌ها}
\label{sec:bg:f-divergence}
در بخش‌های پیشین، مکانیزم‌های محرمانگی تفاضلی را ابزاری برای ایجاد «شباهت آماری» بین خروجی‌های دو پایگاه‌داده‌ی همسایه معرفی کردیم. برای کمّی‌سازی دقیق این شباهت و اثبات کران‌های پایین در فصل‌های آینده، نیازمند معیاری هستیم که فاصله میان توزیع‌های احتمالی را در یک چارچوب عمومی بسنجد. این ابزار، خانواده‌ی $f$-واگرایی‌ها\LTRfootnote{$f$-divergences} است که نخستین بار توسط سیزار \cite{csiszar1963information} و علی و سیلوِی \cite{Ali1966Silvey} معرفی شد.

\subsection{تعریف رسمی در فضای اندازه‌پذیر}
\label{sec:bg:f-div-def}

برای ارائه تعریفی دقیق و مستقل از نوع متغیرهای تصادفی (پیوسته یا گسسته)، از زبان نظریه اندازه\LTRfootnote{Measure Theory} استفاده می‌کنیم.
فرض کنید $(\Omega, \mathcal{F})$ یک فضای اندازه‌پذیر\LTRfootnote{Measurable Space} باشد و $P$ و $Q$ دو «اندازه احتمال»\LTRfootnote{Probability Measures} تعریف شده روی این فضا باشند.

پیش از تعریف واگرایی، مفهوم «پیوستگی مطلق» که شرط وجود چگالی نسبت است را یادآوری می‌کنیم.

\begin{تعریف}[پیوستگی مطلق\LTRfootnote{Absolute Continuity}]
می‌گوییم اندازه $P$ نسبت به $Q$ مطلقاً پیوسته است و می‌نویسیم $P \ll Q$، اگر برای هر مجموعه اندازه‌پذیر $A \in \mathcal{F}$:
\begin{equation}
Q(A) = 0 \implies P(A) = 0
\end{equation}
این شرط تضمین می‌کند که هر رویدادی که تحت توزیع $Q$ غیرممکن باشد، تحت $P$ نیز غیرممکن است.
\end{تعریف}

اگر $P \ll Q$ باشد، بنابر قضیه رادون-نیکودیم\LTRfootnote{Radon-Nikodym Theorem}، تابعی اندازه‌پذیر و غیرمنفی روی $\Omega$ وجود دارد که «مشتق رادون-نیکودیم» $P$ نسبت به $Q$ نامیده می‌شود و با $\frac{dP}{dQ}$ نمایش داده می‌شود. این مشتق نقش همان نسبت درست‌نمایی\LTRfootnote{Likelihood Ratio} را در حالت کلی ایفا می‌کند.

\begin{تعریف}[$f$-واگرایی]
\label{def:f-divergence}
فرض کنید $P$ و $Q$ دو اندازه احتمال روی $(\Omega, \mathcal{F})$ باشند به طوری که $P \ll Q$.
برای هر تابع محدب\LTRfootnote{Convex} $f: (0, \infty) \to \mathbb{R}$ با این شرط که $f(1)=0$، $f$-واگراییِ $P$ از $Q$ به صورت امید ریاضی تابع $f$ بر روی مشتق رادون-نیکودیم (تحت اندازه $Q$) تعریف می‌شود:
\begin{equation}
\label{eq:f-divergence-measure}
D_f(P \| Q) \triangleq \int_{\Omega} f\left( \frac{dP}{dQ}(\omega) \right) dQ(\omega)
\end{equation}
یا به بیانی دیگر با استفاده از نماد امید ریاضی:
\begin{equation}
D_f(P \| Q) = \Eset_Q \left[ f\left( \frac{dP}{dQ} \right) \right]
\end{equation}
\end{تعریف}

\textbf{تفسیر اجزاء:}
\begin{itemize}
    \item \textbf{تابع مولد $f$:} تابع $f$ تعیین‌کننده نوع هندسه و خواص واگرایی است. تحدب $f$ شرطی حیاتی برای خوش‌رفتاری ریاضی این اندازه است.
    \item \textbf{غیرمنفی بودن:} با استفاده از نامساوی ینسن\LTRfootnote{Jensen's Inequality} و شرط محدب بودن $f$، می‌توان نشان داد که واگرایی همواره نامنفی است:
    \begin{equation}
    D_f(P \| Q) = \Eset_Q \left[ f\left( \frac{dP}{dQ} \right) \right] \ge f\left( \Eset_Q \left[ \frac{dP}{dQ} \right] \right) = f(1) = 0
    \end{equation}
    تساوی $D_f(P\|Q) = 0$ برقرار است اگر و تنها اگر $P=Q$ (در صورت اکیداً محدب بودن $f$).
\end{itemize}

در حالت‌های خاص که فضای نمونه $\Omega$ گسسته یا پیوسته (اقلیدسی) باشد و چگالی‌های $p$ و $q$ نسبت به یک اندازه پایه (مانند شمارشی یا لبگ) وجود داشته باشند، مشتق رادون-نیکودیم به نسبت معمولی چگالی‌ها $\frac{p(x)}{q(x)}$ تبدیل می‌شود و تعریف انتگرالی بالا به فرم‌های آشنای زیر تقلیل می‌یابد:
\begin{equation}
D_f(P \| Q) = \int_{\mathcal{X}} q(x) f\left( \frac{p(x)}{q(x)} \right) dx \quad \text{یا} \quad \sum_{x \in \mathcal{X}} q(x) f\left( \frac{p(x)}{q(x)} \right)
\end{equation}

\textbf{تفاوت واگرایی با متر ریاضی:}
اگرچه در ادبیات موضوع، گاهی با اغماض از واژه‌ی «فاصله»\LTRfootnote{Distance} برای واگرایی‌ها استفاده می‌شود، اما باید توجه داشت که واگرایی‌ها لزوماً خواص یک متریک حقیقی را ندارند. یک تابع $d(P, Q)$ تنها در صورتی یک متر ریاضی است که علاوه بر غیرمنفی بودن و اصل همانی، دو شرط تقارن\LTRfootnote{Symmetry} ($d(P, Q) = d(Q, P)$) و نامساوی مثلث\LTRfootnote{Triangle Inequality} ($d(P, R) \le d(P, Q) + d(Q, R)$) را ارضا کند. 

بسیاری از واگرایی‌های معرفی شده در این بخش، به‌ویژه واگرایی کولبک-لایبلر (بخش~\ref{def:kl})، این دو شرط را نقض می‌کنند. عدم تقارن در واگرایی‌ها دارای تفسیر آماری مهمی است؛ به عنوان مثال $\KL(P\|Q)$ بیان‌گر میزان اطلاعات از دست رفته زمانی است که از توزیع $Q$ برای مدل‌سازی توزیع واقعی $P$ استفاده می‌کنیم. این مفهوم ماهیتی «جهت‌دار» دارد و با جابجایی جایگاه فرضیه و واقعیت تغییر می‌کند. با این حال، $f$-واگرایی‌ها هم‌چنان خواص توپولوژیکی و هندسی قدرتمندی دارند که در فصل‌های آتی برای استخراج کران‌های پایین مینی‌مکس از آن‌ها بهره خواهیم برد.
\subsection{نمونه‌های مهم و توابع مولد}
\label{sec:bg:f-div-examples}

با انتخاب‌های متفاوت برای تابع مولد محدب $f(t)$، می‌توان طیف وسیعی از اندازه‌های فاصله را تولید کرد. در ادامه، مهم‌ترین نمونه‌ها را معرفی می‌کنیم.
در تعاریف زیر، فرض می‌کنیم $P$ و $Q$ دو اندازه احتمال باشند که دارای چگالی (یا جرم) احتمال $p(x)$ و $q(x)$ نسبت به یک اندازه پایه هستند.


\begin{تعریف}[تغییرات کل]
\label{def:tv}

فاصله تغییرات کل\LTRfootnote{Total Variation (TV)}،
 شهودی‌ترین متریک برای سنجش تمایزپذیری دو توزیع است و بیان‌گر بیش‌ترین تفاوت احتمالی است که دو توزیع می‌توانند روی یک پیشامد داشته باشند.
تابع مولد آن $f(t) = \frac{1}{2}|t-1|$ است.
\begin{align}
\TV(P, Q) &= \frac{1}{2} \int_{\Xset} \left|p(x) - q(x)\right| dx \\
&= \sup_{A \subseteq \Xset} \left|P(A) - Q(A)\right|
\end{align}
این واگرایی ممکن است با نمادهایی مانند $d_{\TV}$ یا $\|.\|_{\TV}$ نیز نمایش داده ‌شود.
\end{تعریف}

\begin{تعریف}[کولبک-لایبلر]
\label{def:kl}
واگرایی کولبک-لایبلر\LTRfootnote{Kullback-Leibler (KL)}،
معروف‌ترین واگرایی در نظریه اطلاعات که آنتروپی نسبی\LTRfootnote{Relative Entropy} نیز نامیده می‌شود. این معیار نامتقارن است و نقش اساسی در فشرده‌سازی داده‌ها و استنتاج بیزی دارد. تابع مولد آن $f(t) = t \ln t$ است.
\begin{equation}
\label{eq:kl-def}
\KL(P \| Q) = \int_{\Xset} p(x) \ln \left(\frac{p(x)}{q(x)}\right) dx
\end{equation}
\end{تعریف}

\begin{تعریف}[اطلاعات متقابل]
\label{def:mutual-info}
اگر $X$ و $V$ دو متغیر تصادفی باشند، اطلاعات متقابل\LTRfootnote{Mutual Information} بین آن‌ها به صورت امید ریاضی واگرایی \lr{KL} بین توزیع شرطی و توزیع حاشیه‌ای تعریف می‌شود:
\begin{equation}
\label{eq:mutual-info}
I(X; V) = \KL(P_{X,V} \| P_X \otimes P_V) = \Eset_{V} \left[ \KL(P_{X|V} \| P_X) \right]
\end{equation}
این معیار نقش کلیدی در نامساوی فانو و تحلیل کانال‌های اطلاعاتی ایفا می‌کند \cite{duchi2018}.
\end{تعریف}

\begin{تعریف}[کای-دو]
\label{def:chi2}
واگرایی کای-دو\LTRfootnote{Chi-Squared ($\chi^2$)}،
 اغلب برای تقریب‌زنی سایر فواصل (مانند \lr{KL}) در همسایگی‌های کوچک استفاده می‌شود و به دلیل فرم مربعی، محاسبه آن معمولاً ساده‌تر است.
 تابع مولد آن $f(t) = (t-1)^2$ است.
\begin{equation}
\chi^2(P \| Q) = \int_{\Xset} \frac{\left(p(x) - q(x)\right)^2}{q(x)} dx
\end{equation}
\end{تعریف}

\begin{تعریف}[هلینگر-دو]
\label{def:hellinger}
فاصله هلینگر-دو\LTRfootnote{Squared Hellinger}،
تابع مولد آن $f(t) = (\sqrt{t}-1)^2$ است. این فاصله به دلیل خواص ریاضی خوش‌رفتار (مانند متریک و کران‌دار بودن بین ۰ و ۲)، در نظریه برآورد\LTRfootnote{Estimation Theory} و استخراج کران‌های مینی‌مکس (مانند روش \lr{Le Cam}) کاربرد فراوان دارد.
\begin{equation}
H^2(P, Q) = \int_{\Xset} \left(\sqrt{p(x)} - \sqrt{q(x)}\right)^2 dx
\end{equation}
\end{تعریف}

\begin{تعریف}[$E_\gamma$ یا هاکی-استیک]
\label{def:egamma}
واگرایی $E_\gamma$ یا واگرایی هاکی-استیک\LTRfootnote{Hockey-Stick Divergence}،
ابزاری کلیدی در تحلیل‌های مدرن حریم خصوصی و آزمون‌های فرضیه است. تابع مولد آن برای پارامتر $\gamma \ge 1$ به صورت $f_\gamma(t) = [t - \gamma]_+ = \max\left\{0, t - \gamma\right\}$ است.

\textbf{تعریف و صورت‌های معادل:}
تعریف اصلی بر اساس انتگرالِ جرمِ اضافی نسبت درست‌نمایی است، اما صورت‌های معادل زیر بینش عملیاتی‌تری ارائه می‌دهند:
\begin{subequations}
\begin{align}
E_\gamma(P \| Q) &= \int_{\Xset} \max\left\{0, p(x) - \gamma q(x)\right\} dx \\
&= \sup_{A \subseteq \Xset} \left(P(A) - \gamma Q(A)\right) \label{eq:egamma-sup} \\
&= P(Z > \gamma) - \gamma Q(Z > \gamma) \quad \left(\text{که } Z = \frac{p(x)}{q(x)}\right) \label{eq:egamma-prob}
\end{align}
\end{subequations}
رابطه \eqref{eq:egamma-sup} نشان می‌دهد که این واگرایی بیان‌گر بیشینه‌ی تفاضل وزن‌دار احتمالات است که مستقیماً با موازنه خطای نوع اول و دوم در آزمون فرضیه مرتبط است.

\textbf{نکات تحلیلی و تاریخی:}

نام‌گذاری توصیفی این واگرایی به «هاکی-استیک» که نخستین بار توسط ساسون و وِردو \cite{sason2016f} پیشنهاد شد، برخاسته از شکل هندسی نمودار تابع مولد $f(t)$ است. نمادگذاری $E_\gamma$ و تدوین نقش بنیادی آن، پیش‌تر توسط پولیانسکی و همکاران \cite{polyanskiy2010channel} صورت گرفته بود.

    تابع $\gamma \mapsto E_\gamma(P \| Q)$ یک تابع نزولی و محدب است. هر چه $\gamma$ بزرگ‌تر شود، "جریمه" اختصاص داده شده به $Q$ بیش‌تر شده و واگرایی کم‌تر می‌شود.
    
    
\textbf{کاربرد در حریم خصوصی:}
شرط محرمانگی تفاضلی تقریبی (\DP[(\eps, \del)]) دقیقاً معادل است با اینکه برای هر دو دیتابیس همسایه، واگرایی هاکی-استیک خروجی‌ها از مقدار $\delta$ تجاوز نکند:
\begin{equation}
E_{e^\eps}(P \| Q) \le \delta
\end{equation}
\end{تعریف}

\begin{تعریف}[رنی]
\label{def:renyi}
واگرایی رنی\LTRfootnote{Rényi Divergence}،
با پارامتر $\alpha \in (1, \infty)$، به صورت زیر تعریف می‌شود:
\begin{equation}
D_\alpha(P \| Q) = \frac{1}{\alpha - 1} \ln \left( \int_{\Xset} p(x)^\alpha q(x)^{1-\alpha} dx \right)
\end{equation}
این واگرایی پلی میان واگرایی \lr{KL} (در حد $\alpha \to 1$) و واگرایی ماکزیمم (در حد $\alpha \to \infty$) است و در تحلیل ترکیب‌‌پذیری مکانیزم‌ها کاربرد دارد \cite{mironov2017renyi}.
\end{تعریف}

\begin{تعریف}[ماکزیمم ($D_\infty$)]
\label{def:maxdiv}
این واگرایی متناظر با بدترین نسبت درست‌نمایی نقطه‌ای است و به عنوان حدِ واگرایی رنی به دست می‌آید:
\begin{equation}
D_\infty(P \| Q) = \lim_{\alpha \to \infty} D_\alpha(P \| Q) = \sup_{x \in \Xset} \ln \left(\frac{p(x)}{q(x)}\right)
\end{equation}
\textbf{کاربرد در حریم خصوصی:}
    شرط \DP (خالص) دقیقاً معادل است با کران‌دار بودن این واگرایی توسط بودجه حریم خصوصی: $D_\infty(P \| Q) \le \eps$.

\end{تعریف}
\subsection{خواص بنیادین و روابط بین $f$-واگرایی‌ها}
\label{sec:bg:f-div-properties}

خانواده‌ی $f$-واگرایی‌ها تنها مجموعه‌ای از فرمول‌های انتگرالی نیستند، بلکه دارای خواص ساختاری عمیقی هستند که آن‌ها را برای تحلیل سیستم‌های اطلاعاتی و حریم خصوصی ایده‌آل می‌سازد. در این بخش، سه ویژگی حیاتی این معیارها را بررسی می‌کنیم.

\subsubsection{نامساوی پردازش داده (DPI)}
مهم‌ترین ویژگی $f$-واگرایی‌ها در نظریه اطلاعات، خاصیت یک‌نواختی\LTRfootnote{Monotonicity} آن‌ها تحت پردازش است. این ویژگی بیان می‌کند که هیچ عملیات پردازشی روی داده‌ها (اعم از قطعی یا تصادفی) نمی‌تواند تمایزپذیری بین دو توزیع را افزایش دهد.

\begin{قضیه}[نامساوی پردازش داده\LTRfootnote{Data Processing Inequality (DPI)}]
فرض کنید $P$ و $Q$ دو توزیع احتمال روی فضای $\Xset$ باشند و $\kappa: \Xset \to \Yset$ یک هسته‌ی احتمالاتی (کانال مارکوف)\LTRfootnote{Markov Kernel / Probability Kernel} باشد که داده‌ها را از فضای $\Xset$ به $\Yset$ نگاشت می‌کند. اگر $P\kappa$ و $Q\kappa$ توزیع‌های خروجی پس از اعمال کرنل باشند، آنگاه برای هر $f$-واگرایی داریم:
\begin{equation}
D_f(P\kappa \| Q\kappa) \le D_f(P \| Q)
\end{equation}
\end{قضیه}

\textbf{تفسیر در حریم خصوصی:} این قضیه تضمین می‌کند که اگر یک مهاجم نتواند دو دیتابیس را بر اساس خروجی مکانیزم از هم تشخیص دهد، با انجام هیچ‌گونه پس‌پردازشی\LTRfootnote{Post-processing} روی آن خروجی نیز قادر به بهبود توان تشخیص خود نخواهد بود. به عبارت دیگر، اطلاعات (و حریم خصوصی) با پردازش بیش‌تر، «خلق» یا «تخریب» نمی‌شود.

\subsubsection{تحدب مشترک}
تابع $f$-واگرایی نسبت به جفت توزیع‌های ورودی خود، محدب است.
\begin{قضیه}[تحدب مشترک\LTRfootnote{Joint Convexity}]
نگاشت $(P, Q) \mapsto D_f(P \| Q)$ یک تابع محدب مشترک است. یعنی برای هر $\lambda \in [0, 1]$ و توزیع‌های $P_1, P_2, Q_1, Q_2$:
\begin{equation}
D_f\left(\lambda P_1 + (1-\lambda)P_2 \| \lambda Q_1 + (1-\lambda)Q_2\right) \le \lambda D_f(P_1 \| Q_1) + (1-\lambda) D_f(P_2 \| Q_2)
\end{equation}
\end{قضیه}
این ویژگی در تحلیل مکانیزم‌هایی که ترکیبی از چند مکانیزم ساده‌تر هستند، بسیار کاربرد دارد.
\subsubsection{روابط بین واگرایی‌ها}
اگرچه انتخاب‌های مختلف $f$ معیارهای متفاوتی تولید می‌کنند، اما این معیارها مستقل نیستند و می‌توان آن‌ها را با یکدیگر مرتبط ساخت.
\begin{itemize}
    \item \textbf{نامساوی پینسکر\LTRfootnote{Pinsker's Inequality}:}
    این نامساوی مشهور، ارتباط هندسه (فاصله تغییرات کل) و اطلاعات (واگرایی \lr{KL}) را برقرار می‌کند و نشان می‌دهد که همگرایی در آنتروپی نسبی، همگرایی در نرم $L_1$ را تضمین می‌کند:
    \begin{equation}
    \label{eq:pinsker}
    \| P - Q \|_{TV} \le \sqrt{\frac{1}{2} \KL(P \| Q)}
    \end{equation}
    
    \item \textbf{واگرایی $E_\gamma$ (چوب هاکی) و ارتباط با \lr{TV}:}
    واگرایی $E_\gamma$ که به صورت $E_\gamma(P\|Q) = \int (dP - \gamma dQ)^+$ تعریف می‌شود، تعمیمی از فاصله تغییرات کل است. به طور مشخص، در نقطه $\gamma=1$ این دو معیار بر هم منطبق می‌شوند:
    \begin{equation}
    E_1(P \| Q) = \| P - Q \|_{TV} = \frac{1}{2} \| P - Q \|_1
    \end{equation}
    این ویژگی نشان می‌دهد که $E_\gamma$ طیفی از فواصل را می‌سازد که از هندسه محض ($\gamma=1$) شروع شده و به معیارهای اطلاعاتی می‌رسد.

    \item \textbf{نمایش انتگرالی $f$-واگرایی‌ها\LTRfootnote{Integral Representation of f-Divergences}:}
    یکی از عمیق‌ترین نتایج نظری در مقاله ساسون و وردو \cite{sason2016f} (قضیه ۱۱)، بیان می‌کند که $E_\gamma$ به عنوان یک «مؤلفه‌ی سازنده» یا «پایه» برای تمامی $f$-واگرایی‌ها عمل می‌کند. هر $f$-واگرایی محدب $D_f$ (برای تابع $f$ دوبار مشتق‌پذیر با $f(1)=0$) را می‌توان به صورت ترکیب انتگرالی خطی از واگرایی‌های $E_\gamma$ بازنویسی کرد:
    \begin{equation}
    \label{eq:integral_rep_sason}
    D_f(P \| Q) = \int_{1}^{\infty} \left( f''(\gamma) E_\gamma(P \| Q) + \gamma^{-3} f''(\gamma^{-1}) E_\gamma(Q \| P) \right) d\gamma
    \end{equation}
    اهمیت ریاضی این رابطه در آن است که اثبات قضایا و نامساوی‌ها را بسیار ساده می‌کند؛ اگر بتوانیم یک ویژگی (مانند کران‌دار بودن یا تحدب) را برای $E_\gamma$ اثبات کنیم، به دلیل مثبت بودن $f''$ (ناشی از تحدب $f$) و خطی بودن انتگرال، آن ویژگی به صورت خودکار برای تقریباً تمام خانواده‌ی $f$-واگرایی‌ها (شامل \lr{KL}، $\chi^2$ و هلینگر) تعمیم می‌یابد.
\end{itemize}