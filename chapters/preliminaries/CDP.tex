\section{محرمانگی تفاضلی متمرکز (\lr{CDP})}
\label{sec:bg:cdp}

مفهوم محرمانگی تفاضلی\LTRfootnote{Differential Privacy} یا به اختصار \lr{DP}، اولین بار توسط دِوُرک و همکاران \cite{dwork2006differential} معرفی شد و به سرعت به استاندارد طلایی برای حفظ حریم خصوصی در تحلیل داده‌ها تبدیل گشت. این چارچوب، یک تعریف ریاضی قوی از حریم خصوصی ارائه می‌دهد که مبتنی بر پنهان‌سازی حضور یا عدم حضور یک فرد خاص در مجموعه‌داده است.

\subsection{مدل اعتماد و تعریف رسمی}
\label{sec:bg:cdp-def}

در مدل متمرکز\LTRfootnote{Centralized}، فرض بر این است که یک متصدی مورد اعتماد\LTRfootnote{Trusted Curator} وجود دارد. تمام افراد داده‌های خام و حساس خود را در اختیار این متصدی قرار می‌دهند (شکل \ref{fig:cdp-model} را ببینید). متصدی، مجموعه‌داده‌ی کامل \Dset \ را در اختیار دارد. وظیفه‌ی متصدی این است که با اجرای یک مکانیزم تصادفی\LTRfootnote{Randomized Mechanism} \mech \ بر روی مجموعه‌داده‌ی \Dset، نتایجی (مثلاً پاسخ به یک پرس‌وجو\LTRfootnote{Query}) را به صورت عمومی منتشر کند، به طوری که اطلاعات حساس افراد فاش نشود.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
        node distance=2.5cm,
        box/.style={rectangle, draw, minimum width=2.2cm, minimum height=1.5cm, align=center, thick},
        smallbox/.style={rectangle, draw, minimum width=1.5cm, minimum height=1.2cm, align=center, thick},
        arrow/.style={-stealth, thick},
        user/.style={inner sep=0pt, minimum size=1.5cm}
    ]

    % --- Individuals (Faces) ---
    \node[user] (u1) at (0, 1.8) {\scalebox{3}{\smiley}};
    \node[user] (u2) at (0, 0.4) {\scalebox{3}{\smiley}};
    \node at (0, -0.6) {\Huge $\vdots$};
    \node[user] (un) at (0, -1.8) {\scalebox{3}{\smiley}};
    \node[above=0.3cm of u1] {\rl{افراد (داده‌ها)}};
    
    % --- Trusted Curator ---
    \node (curator) [box, right of=u2, xshift=2cm] {\rl{متصدی مورد اعتماد} \\ \rl{کارشناس محرمانگی}};

    % --- DP Mechanism ---
    \node (mech) [smallbox, right of=curator, xshift=1.2cm] {\rl{مکانیزم \lr{DP}}};

    % --- Public ---
    \node (public) [smallbox, right of=mech, xshift=1.2cm] {\rl{عمومی}};

    % --- Arrows and Labels ---
    \draw [arrow] (u1.east) -- node[above, pos=0.5] {$X_1$} (curator.155);
    \draw [arrow] (u2.east) -- node[below, pos=0.5] {$X_2$} (curator.180);
    \draw [arrow] (un.east) -- node[above, pos=0.5] {$X_n$} (curator.205);
    
    \draw [arrow] (curator) -- node[above] {$\underline{X}$} (mech);
    \draw [arrow] (mech) -- node[above, align=center, font=\footnotesize] {\rl{انتشار خروجی} \\ \rl{DP}} (public);

    \end{tikzpicture}
    \caption{\rl{مدل محرمانگی تفاضلی متمرکز (\lr{CDP}) با یک متصدی مورد اعتماد.}}    
\label{fig:cdp-model}
\end{figure}

% تعریف جهان داده‌ها و پایگاه‌داده
\begin{تعریف}[جهان داده‌ها و پایگاه‌داده]
\label{def:universe-db}
مجموعه‌ی تمام مقادیر ممکن برای یک رکورد داده را «جهان داده‌ها»\LTRfootnote{Data Universe} می‌نامیم و آن را با \Xset \ نمایش می‌دهیم (در ادامه‌ی پایان‌نامه نیز هرجا از \Xset استفاده شد منظور جهان داده‌هاست). یک پایگاه‌داده‌ی \Dset \ مجموعه‌ای از رکوردهاست که اعضای آن از \Xset \ انتخاب شده‌اند.
در ادبیات محرمانگی تفاضلی، پایگاه‌داده معمولاً به صورت یک بردار (هیستوگرام) $x \in \mathbb{N}^{|\Xset|}$ نمایش داده می‌شود که در آن هر مولفه $x_i$ نشان‌دهنده‌ی تعداد تکرار عنصر $i$-ام از \Xset \ در پایگاه‌داده است.
\end{تعریف}

\begin{مثال}
فرض کنید می‌خواهیم وضعیت اشتغال افراد را بررسی کنیم. در اینجا جهان داده‌ها برابر است با $\Xset = \{\text{شاغل},\ \text{بیکار}\}$.
اگر در یک پایگاه‌داده ۳ نفر شاغل و ۱ نفر بیکار باشند، نمایش هیستوگرامی پایگاه‌داده \Dset \ به صورت بردار زیر خواهد بود:
\[ \Dset = (3, 1) \]
\end{مثال}

% تعریف الگوریتم تصادفی
\begin{تعریف}[الگوریتم تصادفی]
\label{def:randomized-algo}
یک الگوریتم (مکانیزم) تصادفی \mech \ تابعی است که دامنه‌ی آن مجموعه‌ی تمام پایگاه‌داده‌های ممکن و برد آن مجموعه‌ی خروجی‌های ممکن \Rset \ است. برخلاف الگوریتم‌های قطعی، خروجی \mech \ برای یک ورودی ثابت \Dset، یک متغیر تصادفی است. به عبارت دیگر، \mech \ یک توزیع احتمال روی \Rset \ ایجاد می‌کند.
\end{تعریف}

\begin{مثال}
فرض کنید تابعی داریم که تعداد افراد بیمار را می‌شمارد. یک مکانیزم تصادفی ساده می‌تواند به این صورت باشد: «تعداد واقعی بیماران را بشمار و سپس نتیجه‌ی پرتاب یک سکه (۰ یا ۱) را به آن اضافه کن». در این حالت، خروجی یا تعداد واقعی بیماران است و یا یک عدد بیش‌تر از آن.
\end{مثال}

% تعریف جامع فاصله و نرم‌ها
\begin{تعریف}[فاصله و نرم‌های $\ell_p$]
\label{def:db-distance}
برای سنجش میزان تفاوت دو پایگاه‌داده، از مفهوم نرم $\ell_p$ استفاده می‌شود. در حالت کلی برای $p \ge 1$، فاصله‌ی $\ell_p$ بین دو پایگاه‌داده‌ی \Dset[1] و \Dset[2] (با بردارهای تکرار متناظر) به صورت زیر تعریف می‌گردد:
\begin{equation}
\label{def:lp-norm}
\left\| \Dset[1] - \Dset[2] \right\|_p = \left( \sum_{i=1}^{|\Xset|} \left| x_{1,i} - x_{2,i} \right|^p \right)^{1/p}
\end{equation}
در ادبیات محرمانگی تفاضلی، \textbf{نرم $\ell_1$} (یا فاصله‌ی منهتن) به دلیل ارتباط مستقیم آن با تعداد رکوردها، معیار اصلی محسوب می‌شود. این فاصله دقیقاً تعداد رکوردهایی را می‌شمارد که باید تغییر کنند (اضافه یا حذف شوند) تا \Dset[1] به \Dset[2] تبدیل شود:
\[
\left\| \Dset[1] - \Dset[2] \right\|_1 = \sum_{i=1}^{|\Xset|} \left| x_{1,i} - x_{2,i} \right|
\]
\end{تعریف}

\begin{مثال}
فرض کنید $\Dset[1] = (3, 1)$ و $\Dset[2] = (3, 0)$ باشند (یعنی در پایگاه‌داده دوم، یک نفر بیکار حذف شده است). فاصله‌ی $\ell_1$ آن‌ها برابر است با:
\[
\left\| \Dset[1] - \Dset[2] \right\|_1 = |3-3| + |1-0| = 1
\]
\end{مثال}

% تعریف همسایگی
\begin{تعریف}[پایگاه‌داده‌های همسایه]
\label{def:adjacent-datasets}
دو پایگاه‌داده‌ی \Dset[1] و \Dset[2] را همسایه\LTRfootnote{Neighboring / Adjacent} می‌گوییم (و با $\Dset[1] \sim \Dset[2]$ نشان می‌دهیم) اگر فاصله‌ی $\ell_1$ آن‌ها حداکثر ۱ باشد:
\[
\left\| \Dset[1]-\Dset[2] \right\|_1 \leq 1
\]
این شرط تضمین می‌کند که دو پایگاه‌داده تنها در بود و نبودِ مشخصات یک فرد خاص با هم تفاوت دارند.
\end{تعریف}



ایده‌ی اصلی محرمانگی تفاضلی این است که خروجی مکانیزم برای دو مجموعه‌داده‌ی همسایه باید از نظر آماری «شبیه» باشد، به طوری که مهاجم نتواند تشخیص دهد ورودی واقعی کدام بوده است.

\begin{تعریف}[\eps-محرمانگی تفاضلی (\DP)]
\label{def:cdp}
یک مکانیزم تصادفی \ \mech \ با دامنه $\Xset^n$ و برد \Rset، ویژگی \ \eps-محرمانگی تفاضلی را برآورده می‌سازد، اگر برای هر دو پایگاه‌داده‌ی همسایه‌ی \Dset[1] و \Dset[2] ($\Dset[1] \sim \Dset[2]$) و برای هر زیرمجموعه از خروجی‌های ممکن $\Sset \subseteq \Rset$ (که در $\sigma$-جبر برد تعریف شده باشد)، داشته باشیم:
\begin{equation}
\label{eq:cdp}
\mathbb{P}\left[ \mech(\Dset[1]) \in \Sset \right] \le \exp(\eps) \cdot \mathbb{P}\left[ \mech(\Dset[2]) \in \Sset \right]
\end{equation}
در این نامساوی، پارامتر $\eps \ge 0$ را «بودجه‌ی محرمانگی»\LTRfootnote{Privacy Budget} می‌نامیم.
\end{تعریف}



\subsection{تفسیر پارامترهای محرمانگی}
پارامتر \eps\ نقش کنترل‌کننده‌ی توازن میان «محرمانگی» و «سودمندی» را ایفا می‌کند:
\begin{itemize}
    \item مقادیر کوچک \eps\ (مثلاً $\eps \le 1$) به معنای شباهت بسیار زیاد توزیع‌های خروجی است که منجر به محرمانگی قوی‌تر اما خطای بیش‌تر (نویز بیش‌تر) می‌شود.
    \item مقادیر بزرگ \eps\ اجازه‌ی تمایز بیش‌تر بین توزیع‌ها را می‌دهد که به معنای دقت بالاتر اما ریسک افشای بیش‌تر است.
    \item اگر $\eps=0$ باشد، خروجی مکانیزم باید کاملاً مستقل از ورودی باشد (امنیت کامل اما بدون هیچ‌گونه فایده‌ی آماری).
\end{itemize}

\begin{تعریف}[\lr{DP}-تقریبی یا \lr{$(\eps, \del)$-DP}]
\label{def:approx-cdp}
در بسیاری از موارد (مانند مکانیزم گوسی)، ارضای شرط \DP \ خالص ممکن نیست. در این شرایط، از تعریف انعطاف‌پذیرتری به نام \DP[(\eps, \del)] استفاده می‌شود که اجازه‌ی یک احتمال شکست کوچک \del \ را می‌دهد:
\begin{equation}
\label{eq:cdp-delta}
\mathbb{P}\left[ \mech(\Dset[1]) \in \Sset \right] \le \exp(\eps) \cdot \mathbb{P}\left[ \mech(\Dset[2]) \in \Sset \right] + \del
\end{equation}
پارامتر $\del \in [0, 1]$ را معمولاً «احتمال شکست»\LTRfootnote{Failure Probability} یا احتمال نشت اطلاعات می‌نامند. تفسیر شهودی این است که مکانیزم با احتمال حداقل $1-\del$، تضمین \DP را رعایت می‌کند. در کاربردهای عملی، مقدار \del \ باید بسیار ناچیز (کم‌تر از معکوس چندجمله‌ای اندازه‌ی داده‌ها، مثلاً $\del < 1/n$) در نظر گرفته شود.
\end{تعریف}

\subsection{تعاریف معادل و صورت‌بندی‌های جایگزین}
\label{sec:cdp-equivalent-defs}

برای تسهیل تحلیل‌های ریاضی و درک عمیق‌تر، می‌توان تعریف اصلی \DP\ را به صورت‌های هم‌ارز دیگری بیان کرد. در ادامه دو دیدگاه مهم «نقطه‌ای» و «واگرایی» را بررسی می‌کنیم.

\begin{enumerate}
    \item \textbf{دیدگاه نقطه‌ای و چگالی احتمال (لم هم‌ارزی):}\\
    اگرچه تعریف اصلی بر روی زیرمجموعه‌ها بنا شده است، اما لم زیر نشان می‌دهد که کنترل نسبت احتمالات در تک‌تک نقاط برای برقراری شرط کافی است.

    \begin{لم}[هم‌ارزی نقطه‌ای]
    \label{lem:pointwise-dp}
    یک مکانیزم \mech \ شرط \DP\ را برآورده می‌کند اگر و تنها اگر برای تمام همسایه‌های $\Dset[1] \sim \Dset[2]$ شرایط زیر برقرار باشد:
    \begin{itemize}
        \item \textit{در فضای گسسته:} برای هر خروجی $z \in \Rset$:
        \begin{equation}
        \frac{\mathbb{P}\left[ \mech(\Dset[1]) = z \right]}{\mathbb{P}\left[ \mech(\Dset[2]) = z \right]} \le e^\eps
        \end{equation}
        \item \textit{در فضای پیوسته:} با فرض وجود توابع چگالی $p(\cdot)$ و $q(\cdot)$، برای تمام $z \in \Rset$:
        \begin{equation}
        \label{eq:dp-pdf}
        p(z) \le e^\eps \cdot q(z)
        \end{equation}
    \end{itemize}
    \end{لم}

    \begin{اثبات}
    اثبات بر پایه‌ی خاصیت جمع‌پذیری (در حالت گسسته) و خاصیت یکنوایی انتگرال (در حالت پیوسته روی میدان‌های $\sigma$-جبر بورل) استوار است. فرض کنید شرط نقطه‌ای برقرار باشد؛ برای هر زیرمجموعه‌ی بورلی $\Sset \subseteq \Rset$:
    \[
    \mathbb{P}\left[ \mech(\Dset[1]) \in \Sset \right] = \int_{\Sset} p(z) d\mu(z) \le \int_{\Sset} e^\eps q(z) d\mu(z) = e^\eps \mathbb{P}\left[ \mech(\Dset[2]) \in \Sset \right]
    \]
    (در حالت گسسته، انتگرال با عمل‌گر جمع جایگزین می‌شود).
    \end{اثبات}
    
    \textbf{نکته:} این هم‌ارزی تنها برای $\del=0$ صادق است. برای \DP[(\eps, \del)]، بررسی نقطه‌به‌نقطه کافی نیست و شرط باید روی زیرمجموعه‌ها چک شود.

    \item \textbf{تعریف مبتنی بر واگرایی ماکزیمم:}\\
    از دیدگاه نظریه اطلاعات، محرمانگی تفاضلی خالص محدودیتی بر روی «واگرایی ماکزیمم»\LTRfootnote{Max Divergence} بین توزیع‌های خروجی است. واگرایی ماکزیمم به صورت $D_\infty(P || Q) = \sup_{S} \ln \frac{P(S)}{Q(S)}$ تعریف می‌شود. بنابراین تعریف \ref{def:cdp} معادل است با:
    \begin{equation}
    \label{eq:dp-max-div}
    \sup_{\Dset[1] \sim \Dset[2]} D_\infty\left( \mech(\Dset[1]) \parallel \mech(\Dset[2]) \right) \le \eps
    \end{equation}
    در بخش‌های بعدی با تعریف دقیق واگرایی آشنا خواهیم شد.
\end{enumerate}

\subsection{مکانیزم‌های پایه}
\label{sec:bg:cdp-mechanisms}

برای دست‌یابی به محرمانگی تفاضلی، باید به پاسخ دقیق پرس‌وجو «نویز»\LTRfootnote{Noise} اضافه کنیم. میزان نویز به حساسیت\LTRfootnote{Sensitivity} پرس‌وجو بستگی دارد.

\begin{تعریف}[حساسیت سراسری $\ell_p$]
\label{def:sensitivity-lp}
برای هر تابع پرس‌وجوی $f: \Xset^n \to \Rset^k$ که خروجی برداری دارد، «حساسیت سراسری $\ell_p$»\LTRfootnote{$\ell_p$-Global Sensitivity} که با $\Del_p f$ نمایش داده می‌شود، برابر است با بیشینه‌ی مقدار تغییرات خروجی تابع، به ازای تغییر تنها یک رکورد در ورودی.
با استفاده از تعریف نرم $\ell_p$ (تعریف \ref{def:lp-norm}) داریم:
\begin{equation}
\Del_p f = \max_{\Dset[1] \sim \Dset[2]} \left\| f(\Dset[1]) - f(\Dset[2]) \right\|_p
\end{equation}
که در آن ماکزیمم‌گیری روی تمام زوج پایگاه‌داده‌های همسایه ($\Dset[1] \sim \Dset[2]$) انجام می‌شود.
\end{تعریف}

در میان انواع حساسیت‌ها، دو مورد زیر به دلیل کاربردشان در مکانیزم‌های پایه، از اهمیت ویژه‌ای برخوردارند:

\begin{تعریف}[حساسیت $\ell_1$، $\ell_2$ و $\ell_\infty$]
\label{def:sensitivity-specific}
سه نوع حساسیت زیر بیش‌ترین کاربرد را در طراحی مکانیزم‌های محرمانگی دارند:

\begin{enumerate}
    \item \textbf{حساسیت $\ell_1$ ($\Del_1 f$):}
    این حساسیت برابر با ماکزیمم فاصله منهتن بین خروجی‌هاست و پارامتر اصلی در مکانیزم لاپلاس می‌باشد:
    \begin{equation}
    \Del_1 f = \max_{\Dset[1] \sim \Dset[2]} \left\| f(\Dset[1]) - f(\Dset[2]) \right\|_1
    \end{equation}
    
    \item \textbf{حساسیت $\ell_2$ ($\Del_2 f$):}
    این حساسیت برابر با ماکزیمم فاصله اقلیدسی است و در مکانیزم گوسی کاربرد اساسی دارد. معمولاً استفاده از این حساسیت در ابعاد بالا منجر به خطای کم‌تری نسبت به $\ell_1$ می‌شود:
    \begin{equation}
    \Del_2 f = \max_{\Dset[1] \sim \Dset[2]} \left\| f(\Dset[1]) - f(\Dset[2]) \right\|_2
    \end{equation}

    \item \textbf{حساسیت $\ell_\infty$ ($\Del_\infty f$):}
    این حساسیت برابر با بیشینه‌ی تغییر در «تک‌تک مولفه‌های» خروجی است (نرم ماکزیمم). این معیار نشان می‌دهد که مقدار یک درایه خاص از خروجی چقدر می‌تواند تغییر کند:
    \begin{equation}
    \Del_\infty f = \max_{\Dset[1] \sim \Dset[2]} \left\| f(\Dset[1]) - f(\Dset[2]) \right\|_\infty
    \end{equation}
\end{enumerate}
\end{تعریف}

\begin{مثال}
فرض کنید $f$ یک تابع هیستوگرام شمارشی باشد (تعداد افراد در دسته‌های مجزا). اگر مشخصات یک فرد تغییر کند، او از یک دسته خارج (تغییر $-1$) و به دسته‌ی دیگر وارد (تغییر $+1$) می‌شود. سایر دسته‌ها ثابت می‌مانند ($0$).
بردار تغییرات برابر است با $(0, \dots, +1, \dots, -1, \dots, 0)$. حال حساسیت‌ها را محاسبه می‌کنیم:
\begin{itemize}
    \item \textbf{حساسیت $\ell_1$:} مجموع قدرمطلق تغییرات: $|1| + |-1| = 2$.
    \item \textbf{حساسیت $\ell_2$:} جذر مجموع مربعات: $\sqrt{1^2 + (-1)^2} = \sqrt{2}$.
    \item \textbf{حساسیت $\ell_\infty$:} ماکزیمم قدرمطلق تغییرات: $\max(|1|, |-1|, 0) = 1$.
\end{itemize}
این مثال به وضوح رابطه $\Del_\infty f \le \Del_2 f \le \Del_1 f$ را نشان می‌دهد.
\end{مثال}

انتخاب نوع حساسیت در طراحی مکانیزم، بستگی مستقیم به نوع نویز افزوده شده و ابعاد داده‌ها دارد. به طور خلاصه، حساسیت $\ell_1$ برای کالیبره کردن مکانیزم لاپلاس و حساسیت $\ell_2$ برای مکانیزم گوسی استفاده می‌شود.

\subsection{مکانیزم‌های بنیادی محرمانگی تفاضلی}
\label{sec:mechanisms}

در این بخش، سه مکانیزم اصلی را که بلوک‌های سازنده‌ی بسیاری از الگوریتم‌های پیچیده‌تر هستند، معرفی می‌کنیم.

\subsubsection{مکانیزم لاپلاس}
\label{sec:mech:laplace}

ساده‌ترین و پرکاربردترین روش برای توابع عددی، افزودن نویز از توزیع لاپلاس است. توزیع لاپلاس با پارامتر مقیاس $b$ و میانگین $\mu$ دارای تابع چگالی احتمال $h(z) = \frac{1}{2b} \exp\left(-\frac{|z-\mu|}{b}\right)$ است.

\begin{قضیه}[محرمانگی مکانیزم لاپلاس]
\label{thm:laplace-privacy}
فرض کنید $f: \Xset^n \to \Rset^k$ یک تابع پرس‌وجو با حساسیت سراسری $\Del_1 f$ باشد. مکانیزم لاپلاس که خروجی آن به صورت زیر تعریف می‌شود:
\begin{equation}
\label{eq:laplace-mech}
\mech_{\mathrm{Lap}}(\Dset) = f(\Dset) + (Y_1, \dots, Y_k)
\end{equation}
که در آن $Y_i\overset{i.i.d}{\sim} \mathrm{Lap}\left(\frac{\Del_1 f}{\eps}\right)$، شرط \DP\ را برآورده می‌کند.
\end{قضیه}

\begin{اثبات}
فرض کنید $\Dset[1] \sim \Dset[2]$ دو پایگاه‌داده‌ی همسایه باشند و خروجی تابع $f$ یک بردار $k$-بعدی باشد. نویز لاپلاس به هر مؤلفه به صورت مستقل اضافه می‌شود، بنابراین تابع چگالی احتمال توأم برابر با حاصل‌ضرب چگالی‌های مؤلفه‌هاست.
با فرض $b = \frac{\Del_1 f}{\eps}$، نسبت چگالی احتمال را برای یک بردار خروجی دلخواه $z = (z_1, \dots, z_k)$ بررسی می‌کنیم:
\begin{align*}
\frac{p(z | \Dset[1])}{p(z | \Dset[2])} &= \frac{\prod_{i=1}^k \frac{1}{2b} \exp\left(-\frac{|z_i - f(\Dset[1])_i|}{b}\right)}{\prod_{i=1}^k \frac{1}{2b} \exp\left(-\frac{|z_i - f(\Dset[2])_i|}{b}\right)} \\
&= \prod_{i=1}^k \exp\left( \frac{|z_i - f(\Dset[2])_i| - |z_i - f(\Dset[1])_i|}{b} \right) \\
&= \exp\left( \frac{1}{b} \sum_{i=1}^k \left( |z_i - f(\Dset[2])_i| - |z_i - f(\Dset[1])_i| \right) \right)
\end{align*}
طبق نامساوی مثلثی ($|a| - |b| \le |a-b|$) برای هر مؤلفه $i$ داریم:
\[
|z_i - f(\Dset[2])_i| - |z_i - f(\Dset[1])_i| \le |f(\Dset[1])_i - f(\Dset[2])_i|
\]
با اعمال این نامساوی در مجموع توان نمایی:
\[
\sum_{i=1}^k \left( |z_i - f(\Dset[2])_i| - |z_i - f(\Dset[1])_i| \right) \le \sum_{i=1}^k |f(\Dset[1])_i - f(\Dset[2])_i|
\]
عبارت سمت راست دقیقاً برابر با نرم $\ell_1$ تفاضل خروجی‌هاست:
\[
\sum_{i=1}^k |f(\Dset[1])_i - f(\Dset[2])_i| = \left\| f(\Dset[1]) - f(\Dset[2]) \right\|_1
\]
طبق تعریف حساسیت سراسری، می‌دانیم $\left\| f(\Dset[1]) - f(\Dset[2]) \right\|_1 \le \Del_1 f$. بنابراین:
\[
\frac{p(z | \Dset[1])}{p(z | \Dset[2])} \le \exp\left( \frac{\Del_1 f}{b} \right) = \exp\left( \frac{\Del_1 f}{\Del_1 f / \eps} \right) = e^\eps
\]
و حکم ثابت می‌شود.
\end{اثبات}

\begin{مثال}[پرس‌وجوهای شمارشی]
\label{ex:counting-queries}
پرس‌وجوهای شمارشی\LTRfootnote{Counting Queries}، پرس‌وجوهایی به فرم «چه تعداد از اعضای پایگاه‌داده ویژگی $P$ را دارند؟» هستند. این نوع توابع بلوک‌های سازنده‌ی بسیاری از تحلیل‌های آماری و داده‌کاوی (مانند هیستوگرام‌ها) هستند \cite{dwork2014roth}.

\textbf{حالت تک پرس‌وجو:}
حساسیت یک پرس‌وجوی شمارشی دقیقاً ۱ است ($\Del_1 f = 1$)؛ زیرا افزودن یا حذف یک فرد، نتیجه‌ی شمارش را حداکثر ۱ واحد تغییر می‌دهد. بنابراین طبق قضیه \ref{thm:laplace-privacy}، با افزودن نویز با مقیاس $1/\eps$ (یعنی $\mathrm{Lap}(1/\eps)$) به پاسخ واقعی، محرمانگی \DP[(\eps, 0)] تضمین می‌شود.

\textbf{حالت برداری (چند پرس‌وجو):}
فرض کنید لیستی از $k$ پرس‌وجوی شمارشی $f = (f_1, \dots, f_k)$ داریم (یک پرس‌وجوی برداری). بدون داشتن اطلاعات اضافی درباره‌ی ارتباط پرس‌وجوها، در بدترین حالت یک فرد مشخص می‌تواند در تمام $k$ شمارش تأثیر بگذارد. بنابراین حساسیت $\ell_1$ کل بردار برابر با مجموع تغییرات، یعنی $k$ خواهد بود ($\Del_1 f = k$).
در این حالت برای دست‌یابی به \DP، باید به هر پاسخ نویزی با مقیاس $k/\eps$ اضافه کنیم.
\end{مثال}

% ---------------------------------------------------------

\subsubsection{مکانیزم گوسی}
\label{sec:mech:gaussian}

زمانی که حساسیت $\ell_2$ تابع بسیار کم‌تر از حساسیت $\ell_1$ باشد (مثلاً در پرس‌وجوهای برداری)، مکانیزم گوسی ترجیح داده می‌شود. این مکانیزم به جای نویز لاپلاس، نویز گوسی (نرمال) به خروجی اضافه می‌کند.

\begin{قضیه}[محرمانگی مکانیزم گوسی]
\label{thm:gaussian-privacy}
فرض کنید $f: \Xset^n \to \Rset^k$ تابعی با حساسیت $\ell_2$ برابر با $\Del_2 f$ باشد. مکانیزم گوسی با افزودن نویز $Y \sim \mathcal{N}(0, \sigma^2 \mathbf{I}_k)$ به خروجی تعریف می‌شود:
\begin{equation}
\label{eq:gaussian-mech}
\mech_{\mathrm{Gauss}}(\Dset) = f(\Dset) + \mathcal{N}(0, \sigma^2 \mathbf{I}_k)
\end{equation}
اگر $\eps \in (0, 1)$ باشد، با انتخاب انحراف معیار $\sigma$ به صورت زیر، این مکانیزم شرط $(\eps, \del)$-\DP را برآورده می‌کند:
\begin{equation}
\sigma \ge \sqrt{2 \ln\left(1.25/\del\right)} \cdot \frac{\Del_2 f}{\eps}
\end{equation}
\end{قضیه}

\begin{اثبات}[طرح کلی]
اثبات دقیق این قضیه نیازمند تحلیل «متغیر تصادفی زیان محرمانگی»\LTRfootnote{Privacy Loss Random Variable} است که جزئیات کامل آن در \lr{\cite[Appendix A]{dwork2014roth}} موجود است. ایده اصلی این است که برخلاف توزیع لاپلاس، در توزیع گوسی نسبت $\frac{p(z)}{q(z)}$ کران‌دار نیست، اما احتمال رخداد مقادیری که این نسبت را بزرگ می‌کنند (نواحی دمی توزیع)، با \del\ محدود می‌شود.
\end{اثبات}

\begin{مثال}[انتشار آماره‌های چندگانه و اثر ابعاد]
\label{ex:high-dim-gaussian}
فرض کنید یک بیمارستان می‌خواهد میانگین $d$ ویژگی حیاتی مختلف را منتشر کند. داده‌های هر بیمار را می‌توان به صورت یک بردار $v \in [0, 1]^d$ در نظر گرفت.
اگر یک بیمار پرونده‌اش را تغییر دهد، در بدترین حالت تمام $d$ ویژگی او می‌توانند از ۰ به ۱ تغییر کنند.

\textbf{تحلیل حساسیت:}
\begin{itemize}
    \item \textbf{حساسیت $\ell_1$:} مجموع قدرمطلق تغییرات برابر است با $\sum_{i=1}^d |1-0| = d$.
    \item \textbf{حساسیت $\ell_2$:} جذر مجموع مربعات تغییرات برابر است با $\sqrt{\sum_{i=1}^d (1-0)^2} = \sqrt{d}$.
\end{itemize}

\textbf{مقایسه نویز:}
اگر تعداد ویژگی‌ها زیاد باشد (مثلاً $d=100$):
\begin{itemize}
    \item مکانیزم لاپلاس باید نویزی متناسب با $d=100$ اضافه کند.
    \item مکانیزم گوسی نویزی متناسب با $\sqrt{100}=10$ اضافه می‌کند.
\end{itemize}
این کاهش چشم‌گیر نویز (با ضریب $\sqrt{d}$) دلیل اصلی استفاده از مکانیزم گوسی در الگوریتم‌هایی نظیر یادگیری عمیق با محرمانگی تفاضلی \lr{(DP-SGD)} است.
\end{مثال}

% ---------------------------------------------------------

\subsubsection{مکانیزم نمایی}
\label{sec:mech:exponential}

مکانیزم‌های قبلی برای خروجی‌های عددی بودند. اگر خروجی یک «عضو» از یک مجموعه باشد، از مکانیزم نمایی استفاده می‌شود.
این مکانیزم بر اساس یک \textbf{تابع امتیاز}\LTRfootnote{Score Function} $q(\Dset, r)$ کار می‌کند که میزان «خوبی» خروجی $r$ را می‌سنجد. حساسیت این تابع به صورت $\Del q = \max_{r} \max_{\Dset \sim \Dset'} |q(\Dset, r) - q(\Dset', r)|$ تعریف می‌شود.

\begin{قضیه}[محرمانگی مکانیزم نمایی]
مکانیزم نمایی، یک خروجی $r$ از مجموعه ممکن $\mathcal{R}$ را با احتمالی متناسب با امتیاز آن انتخاب می‌کند:
\begin{equation}
\label{eq:exp-mech}
\mathbb{P}\left[\mech_{\mathrm{Exp}}(\Dset) = r\right] = \frac{\exp \left( \frac{\eps \cdot q(\Dset, r)}{2 \Del q} \right)}{\sum_{r' \in \mathcal{R}} \exp \left( \frac{\eps \cdot q(\Dset, r')}{2 \Del q} \right)}
\end{equation}
این مکانیزم شرط \DP را برآورده می‌کند.
\end{قضیه}

\begin{اثبات}
فرض کنید $\Dset[1] \sim \Dset[2]$. نسبت احتمالات برای یک خروجی ثابت $r$ عبارت است از:
\begin{align*}
\frac{\mathbb{P}\left[\mech(\Dset[1]) = r\right]}{\mathbb{P}\left[\mech(\Dset[2]) = r\right]} &= \frac{\exp\left(\frac{\eps q(\Dset[1], r)}{2\Del q}\right)}{\exp\left(\frac{\eps q(\Dset[2], r)}{2\Del q}\right)} \cdot \frac{\sum_{r'} \exp\left(\frac{\eps q(\Dset[2], r')}{2\Del q}\right)}{\sum_{r'} \exp\left(\frac{\eps q(\Dset[1], r')}{2\Del q}\right)}
\end{align*}
ترم اول (صورت کسرها) با استفاده از خاصیت حساسیت $q$ حداکثر $e^{\eps/2}$ است. ترم دوم (نسبت مخرج‌ها) نیز با استدلالی مشابه حداکثر $e^{\eps/2}$ خواهد بود. حاصل‌ضرب این دو مقدار حداکثر $e^\eps$ می‌شود.
\end{اثبات}

\subsection{ترکیب‌پذیری}
در کاربردهای واقعی، معمولاً چندین پرس‌وجو روی یک پایگاه‌داده اجرا می‌شود. قضایای ترکیب‌پذیری\LTRfootnote{Composition} نشان می‌دهند که چگونه بودجه‌ی محرمانگی انباشته می‌شود.

\begin{قضیه}[ترکیب‌پذیری ساده]
\label{thm:basic-comp}
اگر $k$ مکانیزم $\mech_1, \dots, \mech_k$ به ترتیب دارای بودجه‌های $\eps_1, \dots, \eps_k$ باشند، اجرای متوالی آن‌ها روی یک پایگاه‌داده‌ی واحد، تضمین \DP[(\sum \eps_i)] را فراهم می‌کند \cite{dwork2014roth}.
\end{قضیه}

این کران خطی در بسیاری موارد بدبینانه است. «قضیه ترکیب پیشرفته» نشان می‌دهد که با پذیرش اندکی احتمال شکست ($\del'$ اضافه)، انباشت بودجه بسیار کندتر (با نرخ $\sqrt{k}$) رشد می‌کند \cite{dwork2014roth}.

\begin{قضیه}[ترکیب‌پذیری پیشرفته]
\label{thm:advanced-comp}
برای هر $\del' > 0$، اجرای $k$ مکانیزم که هرکدام \DP\ هستند، دارای تضمین \DP[(\eps', k\del + \del')] است که در آن:
\begin{equation}
\eps' \approx \eps \sqrt{2k \ln(1/\del')} + k\eps(e^\eps - 1)
\end{equation}
برای مقادیر کوچک \eps، جمله دوم ناچیز است و بودجه کل تقریباً با $\eps\sqrt{k}$ رشد می‌کند.
\end{قضیه}

\subsection{محرمانگی گروهی}
محرمانگی تفاضلی نه تنها از یک فرد، بلکه به صورت خودکار از گروه‌های کوچک نیز محافظت می‌کند \cite{dwork2014roth}.

\begin{قضیه}[محرمانگی گروهی\LTRfootnote{Group Privacy}]
\label{thm:group-privacy}
اگر دو پایگاه‌داده \Dset[1] و \Dset[2] در $k$ رکورد متفاوت باشند (فاصله‌ی همسایگی $k$)، آنگاه هر مکانیزم \DP\ برای آن‌ها تضمین \DP[(k\eps)] را ارائه می‌دهد:
\begin{equation}
\mathbb{P}\left[\mech(\Dset[1]) \in S\right] \le e^{k\eps} \mathbb{P}\left[\mech(\Dset[2]) \in S\right]
\end{equation}
\end{قضیه}
این خاصیت نشان می‌دهد که با بزرگ شدن گروه ($k$)، تضمین محرمانگی به صورت نمایی تضعیف می‌شود ($e^{k\eps}$).

\subsection{محدودیت مدل متمرکز}
\label{sec:bg:cdp-limitations}

با وجود تمام مزایا، مدل \lr{CDP} یک نقطه‌ی ضعف اساسی دارد: نیاز به یک متصدی کاملاً مورد اعتماد. در بسیاری از سناریوهای دنیای واقعی (مانند جمع‌آوری داده از گوشی‌های هوشمند)، کاربران به سرور مرکزی اعتماد ندارند. این عدم اعتماد، ما را به سمت مدل جایگزین، یعنی «محرمانگی تفاضلی موضعی» سوق می‌دهد. در فصل بعد با محرمانگی تفاضلی موضعی و تعاریف و قضایای اساسی آن آشنا خواهیم شد.