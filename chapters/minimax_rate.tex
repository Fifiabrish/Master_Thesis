\chapter{تحلیل‌های مبتنی بر انقباض و نرخ‌های مینیماکس}
\label{ch:literature}

\section{مقدمه}
\label{sec:lit:intro}

در فصل پیشین، تعاریف پایه محرمانگی تفاضلی موضعی (\lr{LDP}) و مکانیزم‌های ابتدایی آن را بررسی کردیم. همان‌طور که دیدیم، چالش اصلی در مدل موضعی، کاهش شدید نسبت سیگنال به نویز است. برای تحلیل دقیق این پدیده و یافتن حدود نهایی دقت آماری، نیازمند ابزارهای قوی‌تری هستیم.

در این فصل، به بررسی چارچوب نظری استانداردی می‌پردازیم که توسط دوچی و همکاران \cite{Duchi2013} توسعه داده شده است. ایده مرکزی این چارچوب، نگاه به مکانیزم‌های محرمانگی به عنوان «عملگرهای انقباضی»\LTRfootnote{Contraction Operators} است. به بیان شهودی، اعمال شرط \LDP باعث می‌شود که توزیع‌های خروجی $\mech(\cdot|x)$ و $\mech(\cdot|x')$ بسیار به یکدیگر شبیه شوند، حتی اگر ورودی‌های $x$ و $x'$ کاملاً متفاوت باشند.

ما نشان خواهیم داد که چگونه می‌توان این شباهت اجباری را با استفاده از نامساوی‌های پردازش داده و \f-واگرایی‌ها (به‌ویژه واگرایی کولبک-لایبلر) مدل‌سازی کرد و از آن برای اثبات نرخ‌های مینیماکس در مسائل تخمین آماری استفاده نمود \cite{Duchi2013}.

\section{محرمانگی به عنوان انقباض اطلاعاتی}
\label{sec:lit:contraction}

یکی از ویژگی‌های بنیادین نظریه اطلاعات، «نامساوی پردازش داده»\LTRfootnote{Data Processing Inequality} است که بیان می‌کند پردازش روی داده‌ها (بدون دسترسی به منبع اصلی) نمی‌تواند اطلاعات متقابل را افزایش دهد. در زمینه محرمانگی، ما با نسخه قوی‌تری از این مفهوم سروکار داریم که به آن «نامساوی قوی پردازش داده»\LTRfootnote{Strong Data Processing Inequality (SDPI)} می‌گویند \cite{Asoodeh2024}.

فرض کنید $\mech$ یک مکانیزم \LDP باشد. هدف ما یافتن کرانی برای واگرایی بین توزیع‌های خروجی بر حسب واگرایی ورودی‌هاست. دوچی و همکاران نشان دادند که مکانیزم‌های موضعی باعث انقباض شدید در واگرایی \lr{KL} می‌شوند.

\begin{قضیه}[انقباض \lr{KL} در مکانیزم‌های موضعی]
\label{thm:kl-contraction}
فرض کنید $\mech$ یک مکانیزم \LDP باشد. برای هر دو ورودی $x, x' \in \Xset$، واگرایی کولبک-لایبلر بین توزیع‌های خروجی متناظر $\mech(\cdot|x)$ و $\mech(\cdot|x')$ با رابطه زیر محدود می‌شود:
\begin{equation}
\label{eq:kl-bound-duchi}
D_{KL}(\mech(\cdot|x) || \mech(\cdot|x')) \le 4(e^\alpha - 1)^2
\end{equation}
به طور دقیق‌تر، اگر $\alpha \le 1$ باشد، این کران به صورت $O(\alpha^2)$ رفتار می‌کند \cite{Duchi2013}.
\end{قضیه}

\begin{اثبات}
برای اثبات دقیق این قضیه، از تعریف واگرایی \lr{KL} شروع می‌کنیم. فرض کنید $q(z|x)$ و $q(z|x')$ چگالی‌های احتمال خروجی باشند. طبق تعریف \LDP می‌دانیم که برای هر $z \in \Zset$:
\begin{equation}
e^{-\alpha} \le \frac{q(z|x)}{q(z|x')} \le e^\alpha
\end{equation}
این شرط تضمین می‌کند که نسبت درست‌نمایی‌ها حول عدد ۱ محدود است. با بسط تیلور تابع $\log t$ حول $t=1$ و استفاده از خواص تحدب، می‌توان نشان داد که:
\begin{align}
D_{KL}(P || Q) &= \int p(z) \log \frac{p(z)}{q(z)} dz \\
&\le \int p(z) \left( \frac{p(z)}{q(z)} - 1 + \frac{1}{2} \left( \frac{p(z)}{q(z)} - 1 \right)^2 \right) dz
\end{align}
با اعمال کران‌های \LDP بر روی نسبت $p/q$، جمله درجه اول صفر می‌شود و جمله درجه دوم ضریب $(e^\alpha - 1)^2$ را تولید می‌کند. جزئیات کامل این محاسبات در لم ۱ مقاله \cite{Duchi2013} آمده است. نکته کلیدی این است که برای $\alpha$ کوچک، فاصله \lr{KL} به صورت مربعی با $\alpha$ کاهش می‌یابد.
\end{اثبات}

این قضیه ابزار بسیار قدرتمندی است. به جای اینکه مستقیماً با تعریف دشوار \LDP کار کنیم، می‌توانیم از این کران ساده در نامساوی‌هایی مثل فانو استفاده کنیم. همچنین مطالعات جدیدتر نشان داده‌اند که این انقباض را می‌توان با استفاده از معیارهای دیگری نظیر اطلاعات متقابل \cite{Cuff2016} یا واگرایی $E_\gamma$ \cite{Asoodeh2021} نیز بیان کرد که در فصل بعد به آن می‌پردازیم.

\subsection{انقباض در فاصله واریانس کل}
علاوه بر \lr{KL}، کران مشابهی برای فاصله واریانس کل (\lr{TV}) نیز ارائه شده است که در استفاده از روش «لم لو کم\LTRfootnote{Le Cam}» کاربرد دارد \cite{Duchi2013}:

\begin{قضیه}[انقباض \lr{TV}]
\label{thm:tv-contraction}
تحت شرایط مشابه، برای هر مکانیزم \LDP:
\begin{equation}
\norm{\mech(\cdot|x) - \mech(\cdot|x')}_{TV} \le \min \{1, e^\alpha - 1\} \cdot \norm{x - x'}_0
\end{equation}
(در اینجا $\norm{x-x'}_0$ نشان‌دهنده فاصله همینگ یا متریک مجزا روی ورودی است).
برای $\alpha$ کوچک، 
این رابطه بیان می‌کند که فاصله آماری خروجی‌ها نمی‌تواند بیشتر از
$O(\alpha)$ باشد.
\end{قضیه}

\section{تحلیل نرخ‌های مینیماکس با استفاده از انقباض}
\label{sec:lit:minimax}

حال که ابزار انقباض را در اختیار داریم، می‌توانیم استراتژی کلی اثبات حدود پایین\LTRfootnote{Lower Bounds} در مدل موضعی را صورت‌بندی کنیم. این استراتژی که توسط دوچی \cite{Duchi2013} و بعدها با جزئیات بیشتر در \cite{duchi2018} بسط داده شد، شامل سه گام است:

\begin{enumerate}
    \item \textbf{تقلیل به آزمون فرض:} تبدیل مسئله تخمین پارامتر $\theta$ به مسئله تشخیص اندیس $V$ در یک مجموعه متناهی (استفاده از لم فانو یا اسود).
    \item \textbf{کران‌دار کردن اطلاعات متقابل:} استفاده از خاصیت انقباض \LDP برای محدود کردن اطلاعاتی که نمونه‌های مشاهده شده $Z_1, \dots, Z_n$ درباره اندیس $V$ می‌دهند \cite{Barnes2020}.
    \item \textbf{محاسبه ریسک نهایی:} ترکیب نتایج برای رسیدن به کران پایین خطای تخمین.
\end{enumerate}

مهم‌ترین گام، گام دوم است. طبق نامساوی قوی پردازش داده برای مدل موضعی، داریم:
\begin{equation}
I(V; Z^n) \le \sum_{i=1}^n I(V; Z_i) \le n \cdot \alpha^2 \cdot C
\end{equation}
که در آن $C$ ثابتی است که به هندسه مسئله بستگی دارد. این رابطه نشان می‌دهد که اطلاعات موثر با نرخ $n\alpha^2$ رشد می‌کند، نه $n$. این همان دلیلی است که «اندازه نمونه موثر» در مدل موضعی برابر با $n\alpha^2$ در نظر گرفته می‌شود.

\section{مطالعه موردی: تخمین میانگین}
\label{sec:lit:mean-estimation}

برای نمایش قدرت این چارچوب، مسئله کلاسیک تخمین میانگین را در نظر می‌گیریم.
فرض کنید هر کاربر $i$ برداری $X_i \in [-1, 1]^d$ دارد و هدف تخمین میانگین جامعه $\mu = \Eset[X]$ است. معیار خطا را «میانگین مربعات خطا» (MSE) در نظر می‌گیریم.

\begin{قضیه}[کران پایین تخمین میانگین]
برای هر مکانیزم \LDP و هر تخمین‌گر $\hat{\mu}$، ماکسیمم خطای مورد انتظار با رابطه زیر محدود می‌شود \cite{Duchi2013}:
\begin{equation}
\inf_{\hat{\mu}, \mech} \sup_{P} \Eset [\norm{\hat{\mu} - \mu}^2] \ge \Omega \left( \frac{d}{n \min\{\alpha, \alpha^2\}} \right)
\end{equation}
\end{قضیه}

\textbf{تحلیل اثبات:}
برای اثبات این کران، از لم اسود استفاده می‌کنیم. فضای پارامتر را به صورت یک ابرمکعب $\{-1, 1\}^d$ گسسته‌سازی می‌کنیم.
طبق لم اسود، خطا با مجموع فاصله‌های \lr{TV} بین توزیع‌های شرطی مرتبط است.
با استفاده از قضیه انقباض \ref{thm:kl-contraction} و نامساوی پینسکر، می‌دانیم که:
\begin{equation}
\norm{\mech(\cdot|x) - \mech(\cdot|x')}_{TV}^2 \le \frac{1}{2} D_{KL}(\mech(\cdot|x) || \mech(\cdot|x')) \le O(\alpha^2)
\end{equation}
بنابراین فاصله \lr{TV} حداکثر از مرتبه $\alpha$ است. با جایگذاری این مقدار در لم اسود، کران پایین $\frac{1}{n\alpha^2}$ حاصل می‌شود.

این نتیجه نشان می‌دهد که برای رسیدن به خطای کم در مدل موضعی، تعداد داده‌ها باید متناسب با $1/\alpha^2$ افزایش یابد، که هزینه‌ی بسیار سنگین‌تری نسبت به مدل متمرکز (که متناسب با $1/\eps$ است) دارد.

\section{محدودیت‌های تحلیل کلاسیک}
\label{sec:lit:limitations}

با وجود موفقیت چارچوب دوچی در اثبات نرخ‌های مینیماکس بهینه برای $\alpha$های کوچک (رژیم محرمانگی بالا)، این روش در رژیم $\alpha$های بزرگ (محرمانگی پایین) دچار ضعف است.

همان‌طور که در رابطه \eqref{eq:kl-bound-duchi} دیدیم، کران انقباض \lr{KL} با ضریب $(e^\alpha - 1)^2$ رشد می‌کند. زمانی که $\alpha$ بزرگ باشد، این کران به سرعت به بی‌نهایت میل می‌کند و اطلاعاتی فراتر از کران بدیهی به ما نمی‌دهد. این در حالی است که به طور شهودی، حتی با $\alpha$ بزرگ، مکانیزم همچنان باید مقداری انقباض ایجاد کند.

این محدودیت ناشی از ذات واگرایی \lr{KL} است که رفتار دنباله‌های توزیع را با حساسیت زیادی وزن‌دهی می‌کند. برای رفع این مشکل و به دست آوردن تحلیل‌های دقیق‌تر\LTRfootnote{Tight} که در تمام بازه‌های $\alpha$ معتبر باشند، نیازمند معیار هندسی متفاوتی هستیم. این نیاز، انگیزه اصلی معرفی واگرایی‌های جدید مانند \f-واگرایی‌های خاص (نظیر $E_\gamma$) است \cite{Asoodeh2021, Asoodeh2024} که در فصل آینده به تفصیل به آن خواهیم پرداخت.